{"speakers": [{"speaker": "SPEAKER_01", "timestamp": [0.0, 15.6], "text": " Hello, everyone."}, {"speaker": "SPEAKER_01", "timestamp": [15.88, 20.4], "text": " Welcome to Cloud Native Live, where we dive into the code behind Cloud Native."}, {"speaker": "SPEAKER_01", "timestamp": [21.06, 25.04], "text": " I'm Taylor Dolezal, a senior developer advocate at HashiCorp, where I focus"}, {"speaker": "SPEAKER_01", "timestamp": [25.04, 30.4], "text": " on all things infrastructure, application delivery, and developer experience. Every week,"}, {"speaker": "SPEAKER_01", "timestamp": [30.46, 35.08], "text": " we bring a new set of presenters to showcase how to work with cloud native technologies."}, {"speaker": "SPEAKER_01", "timestamp": [35.6, 39.34], "text": " They will build things, they will break things, and they will answer your questions."}, {"speaker": "SPEAKER_01", "timestamp": [40.04, 48.0], "text": " In today's session, Alok and Cesar have joined us to talk about leveraging the CNCF observability tools for Kubernetes troubleshooting."}, {"speaker": "SPEAKER_01", "timestamp": [48.44, 55.02], "text": " This is an official live stream of the CNCF and as such is subject to the CNCF Code of Conduct."}, {"speaker": "SPEAKER_01", "timestamp": [55.44, 60.22], "text": " Please don't add anything to the chat or questions that would be in violation of the Code of Conduct."}, {"speaker": "SPEAKER_01", "timestamp": [60.52, 64.8], "text": " Basically, please be respectful to all of your fellow participants and presenters."}, {"speaker": "SPEAKER_01", "timestamp": [65.0, 68.0], "text": " In short, please be respectful to all of your fellow participants and presenters. In short, please be excellent to one another."}, {"speaker": "SPEAKER_01", "timestamp": [68.0, 73.0], "text": " With that, I'd love to hand it over to Alok and Cesar to kick off today's presentation."}, {"speaker": "SPEAKER_01", "timestamp": [73.0, 75.0], "text": " With that, I'll turn it over to you."}, {"speaker": "SPEAKER_03", "timestamp": [75.0, 89.44], "text": " Thank you, Taylor. So I'm Alok. I'm the Founder and CTO of Upscruise, an observability company built on open source and CNCF telemetry and I will also introduce Cesar Quintana"}, {"speaker": "SPEAKER_03", "timestamp": [89.44, 97.12], "text": " my colleague who was the principal solutions architect at Opscores thank you so the way we"}, {"speaker": "SPEAKER_03", "timestamp": [97.12, 104.24], "text": " thought we would do this before we set up the demo itself and go through that the fun part I want to"}, {"speaker": "SPEAKER_03", "timestamp": [104.24, 105.52], "text": " do a little bit of setup so you know"}, {"speaker": "SPEAKER_03", "timestamp": [105.52, 111.12], "text": " have the context of what we do so with that in mind let me share my screen and bring up"}, {"speaker": "SPEAKER_03", "timestamp": [111.12, 115.76], "text": " uh you know kind of set the stage if you will let me find the right one"}, {"speaker": "SPEAKER_03", "timestamp": [122.0, 123.92], "text": " and let me know if this is coming up"}, {"speaker": "SPEAKER_03", "timestamp": [125.0, 126.0], "text": " And let me know if this is coming up. Cool."}, {"speaker": "SPEAKER_01", "timestamp": [126.0, 128.5], "text": " That looks good here."}, {"speaker": "SPEAKER_03", "timestamp": [128.5, 130.4], "text": " Okay, great."}, {"speaker": "SPEAKER_03", "timestamp": [130.4, 136.6], "text": " So as mentioned by Taylor, we are talking about how to add intelligence and observability"}, {"speaker": "SPEAKER_04", "timestamp": [136.6, 140.4], "text": " now that we have open source monitoring."}, {"speaker": "SPEAKER_03", "timestamp": [140.4, 145.52], "text": " Going through the standard confidentiality and legal notice. We'll skip over that."}, {"speaker": "SPEAKER_02", "timestamp": [147.18, 147.4], "text": " I'll let the stage by, you know,"}, {"speaker": "SPEAKER_03", "timestamp": [153.08, 153.96], "text": " revisiting what really has become the challenges of cloud native application observability, right?"}, {"speaker": "SPEAKER_03", "timestamp": [153.96, 158.32], "text": " And fundamentally this has been happening now for a few years as applications"}, {"speaker": "SPEAKER_03", "timestamp": [158.32, 161.02], "text": " move to microservice architectures."}, {"speaker": "SPEAKER_03", "timestamp": [161.8, 162.06], "text": " You know,"}, {"speaker": "SPEAKER_00", "timestamp": [162.18, 168.34], "text": " there are three things that we know have started and really added a lot more challenges to the ops"}, {"speaker": "SPEAKER_00", "timestamp": [168.34, 170.18], "text": " teams that are managing them."}, {"speaker": "SPEAKER_00", "timestamp": [170.18, 174.44], "text": " Number one was just complexity, scale, the tiering."}, {"speaker": "SPEAKER_00", "timestamp": [174.44, 177.04], "text": " There's so many dependencies."}, {"speaker": "SPEAKER_03", "timestamp": [177.04, 179.0], "text": " And the dependencies, as we've shown on red,"}, {"speaker": "SPEAKER_00", "timestamp": [179.0, 181.12], "text": " you've got, of course, the application pieces."}, {"speaker": "SPEAKER_00", "timestamp": [181.12, 183.6], "text": " Could be PaaS services, could be SaaS services,"}, {"speaker": "SPEAKER_00", "timestamp": [183.6, 185.88], "text": " could be Kubernetes container could be SAS services could be a kubernetes kubernetes"}, {"speaker": "SPEAKER_00", "timestamp": [185.88, 191.58], "text": " uh container running an application code it could be serverless all of them and then of course even"}, {"speaker": "SPEAKER_03", "timestamp": [191.58, 195.96], "text": " dependents on kubernetes itself orchestrating all of these and then the underlying infrastructure"}, {"speaker": "SPEAKER_00", "timestamp": [195.96, 202.26], "text": " wherever it might be so these create tier dependencies you know if you will from top"}, {"speaker": "SPEAKER_03", "timestamp": [202.26, 205.08], "text": " down kind of what we call vertical, as well as across."}, {"speaker": "SPEAKER_03", "timestamp": [209.96, 215.58], "text": " And this is happening all the time. The third complexity is dynamism. Great. We want to be agile, right? We want to add services, change any one component, scale up, scale in, you know,"}, {"speaker": "SPEAKER_03", "timestamp": [215.76, 221.6], "text": " some things drop, some things brought up. That, together with all of this, is like a highly"}, {"speaker": "SPEAKER_03", "timestamp": [221.6, 225.78], "text": " complex distributed system. And just looking at a couple of metrics"}, {"speaker": "SPEAKER_03", "timestamp": [225.78, 228.12], "text": " is no longer sufficient."}, {"speaker": "SPEAKER_03", "timestamp": [228.46, 230.72], "text": " You know, things are changing, things are coming up."}, {"speaker": "SPEAKER_03", "timestamp": [230.98, 232.54], "text": " You have a zillion dashboards."}, {"speaker": "SPEAKER_03", "timestamp": [233.46, 237.66], "text": " So the good news and the sort of not so good news"}, {"speaker": "SPEAKER_03", "timestamp": [237.66, 238.26], "text": " is the following."}, {"speaker": "SPEAKER_00", "timestamp": [238.38, 241.9], "text": " The good news is, thanks to CNCF and open source,"}, {"speaker": "SPEAKER_03", "timestamp": [241.9, 244.24], "text": " pretty much every possible telemetry"}, {"speaker": "SPEAKER_03", "timestamp": [244.24, 248.88], "text": " from real-time metrics logs events"}, {"speaker": "SPEAKER_03", "timestamp": [249.68, 257.36], "text": " configuration information traces flows are all available now directly from an open source"}, {"speaker": "SPEAKER_03", "timestamp": [257.36, 262.64], "text": " environment meaning open telemetry is an example open source that has existed now for a while"}, {"speaker": "SPEAKER_03", "timestamp": [263.2, 268.48], "text": " so that means we can move to the the key pieces to solve that problem."}, {"speaker": "SPEAKER_03", "timestamp": [268.62, 269.54], "text": " What's the not so good news?"}, {"speaker": "SPEAKER_03", "timestamp": [269.62, 270.22], "text": " That complexity."}, {"speaker": "SPEAKER_00", "timestamp": [271.18, 272.9], "text": " The role of observability is changing."}, {"speaker": "SPEAKER_00", "timestamp": [273.04, 275.3], "text": " It's no longer about dashboards and just alerting."}, {"speaker": "SPEAKER_03", "timestamp": [275.62, 282.66], "text": " It's how can we help the ops teams get to understand what's happening in real time"}, {"speaker": "SPEAKER_03", "timestamp": [282.66, 289.6], "text": " so they can detect quickly, find the real issues, and get up and running you know it's the same things that you've heard"}, {"speaker": "SPEAKER_03", "timestamp": [289.6, 295.76], "text": " time to mean time to test should be fast don't waste time with false alerts and get to the root"}, {"speaker": "SPEAKER_03", "timestamp": [295.76, 308.1], "text": " cost mean time to resolution right so what we what we've learned having been born cloud native ourselves, is what does Ops really do, if you think about it?"}, {"speaker": "SPEAKER_00", "timestamp": [308.1, 311.38], "text": " What Ops does, beyond getting all the telemetry,"}, {"speaker": "SPEAKER_04", "timestamp": [311.38, 313.56], "text": " is they understand the dependency."}, {"speaker": "SPEAKER_00", "timestamp": [313.56, 315.9], "text": " And they look at an application with multiple services"}, {"speaker": "SPEAKER_03", "timestamp": [315.9, 317.02], "text": " talking to each other."}, {"speaker": "SPEAKER_03", "timestamp": [317.02, 319.28], "text": " They understand what the interactions are."}, {"speaker": "SPEAKER_03", "timestamp": [319.28, 322.38], "text": " They know how applications, services"}, {"speaker": "SPEAKER_03", "timestamp": [322.38, 324.0], "text": " are being monitored by the orchestration."}, {"speaker": "SPEAKER_03", "timestamp": [324.0, 326.24], "text": " They know the dependence of infrastructure."}, {"speaker": "SPEAKER_00", "timestamp": [326.24, 330.84], "text": " So really savvy ops teams, SRE teams know that."}, {"speaker": "SPEAKER_03", "timestamp": [330.84, 332.88], "text": " They also know what's changing"}, {"speaker": "SPEAKER_03", "timestamp": [332.88, 334.5], "text": " and they apply curated knowledge."}, {"speaker": "SPEAKER_03", "timestamp": [334.5, 337.78], "text": " They know a published, subscribing,"}, {"speaker": "SPEAKER_03", "timestamp": [337.78, 340.74], "text": " or producer consumer model versus a database,"}, {"speaker": "SPEAKER_03", "timestamp": [340.74, 342.04], "text": " what they're supposed to do."}, {"speaker": "SPEAKER_03", "timestamp": [342.04, 344.82], "text": " They know what to look for, what metrics to look for."}, {"speaker": "SPEAKER_03", "timestamp": [344.82, 349.76], "text": " They are aware of the app and the dependencies. That knowledge is what they're supposed to do they know what to look for what metrics to look for they are aware of the app and the dependencies that knowledge is what they use when they look through"}, {"speaker": "SPEAKER_03", "timestamp": [349.76, 354.8], "text": " the data and sift through whether it's the metrics the laws of the traces right so fundamentally when"}, {"speaker": "SPEAKER_03", "timestamp": [354.8, 359.2], "text": " they look at something that's happening they're looking at the every component the inbound and"}, {"speaker": "SPEAKER_03", "timestamp": [359.2, 365.3], "text": " outbound who's talking to whom what resources and services depending on? All of this."}, {"speaker": "SPEAKER_03", "timestamp": [365.3, 368.62], "text": " And so they essentially built context"}, {"speaker": "SPEAKER_03", "timestamp": [368.62, 369.82], "text": " to understand what is happening"}, {"speaker": "SPEAKER_03", "timestamp": [369.82, 373.1], "text": " so they can actually detect the problem,"}, {"speaker": "SPEAKER_03", "timestamp": [373.1, 374.92], "text": " isolate it and analyze"}, {"speaker": "SPEAKER_03", "timestamp": [374.92, 377.98], "text": " and figure out what the resolution should be."}, {"speaker": "SPEAKER_03", "timestamp": [377.98, 379.04], "text": " So if you think about,"}, {"speaker": "SPEAKER_03", "timestamp": [379.04, 382.2], "text": " if observability has to be really intelligent,"}, {"speaker": "SPEAKER_03", "timestamp": [382.2, 386.54], "text": " they have to establish this context, this understanding,"}, {"speaker": "SPEAKER_03", "timestamp": [386.54, 389.66], "text": " and surface that from all that effectively"}, {"speaker": "SPEAKER_03", "timestamp": [389.66, 392.72], "text": " called noise that's coming in, all the data that's sitting in."}, {"speaker": "SPEAKER_03", "timestamp": [392.72, 394.82], "text": " If you can't do that, then we've actually"}, {"speaker": "SPEAKER_03", "timestamp": [394.82, 399.14], "text": " made the life of a typical DevOps and SRE very difficult."}, {"speaker": "SPEAKER_03", "timestamp": [399.14, 400.66], "text": " So that's what we want to do."}, {"speaker": "SPEAKER_03", "timestamp": [400.66, 405.0], "text": " So our thesis is help leverage this data, right?"}, {"speaker": "SPEAKER_03", "timestamp": [405.0, 410.0], "text": " If you look at open source and open telemetry, clearly we know that things like Prometheus,"}, {"speaker": "SPEAKER_03", "timestamp": [410.0, 415.0], "text": " getting information from Fluidly for the logs and pulling into something like Grafana Loki,"}, {"speaker": "SPEAKER_03", "timestamp": [415.0, 421.0], "text": " using Jaeger or open tracing standards even to get the traces,"}, {"speaker": "SPEAKER_00", "timestamp": [421.0, 428.96], "text": " looking at flows from things like eBPF and Istio, using configuration information from Kubernetes, changes,"}, {"speaker": "SPEAKER_03", "timestamp": [429.22, 433.5], "text": " even the cloud infrastructure data as well as pass information,"}, {"speaker": "SPEAKER_03", "timestamp": [433.66, 436.52], "text": " all that that's available, pull that together"}, {"speaker": "SPEAKER_03", "timestamp": [436.52, 439.4], "text": " to build that context across all of them"}, {"speaker": "SPEAKER_03", "timestamp": [439.4, 443.84], "text": " and then help reduce the amount of information"}, {"speaker": "SPEAKER_03", "timestamp": [443.84, 446.96], "text": " and focus on the right information that ops needs, right?"}, {"speaker": "SPEAKER_03", "timestamp": [447.72, 450.66], "text": " So this is probably the one key slide"}, {"speaker": "SPEAKER_03", "timestamp": [450.66, 453.7], "text": " as we get into the demo to tell you."}, {"speaker": "SPEAKER_02", "timestamp": [454.24, 456.16], "text": " On the left-hand side, what you're seeing"}, {"speaker": "SPEAKER_01", "timestamp": [456.16, 458.3], "text": " is all the open telemetry,"}, {"speaker": "SPEAKER_01", "timestamp": [458.46, 460.82], "text": " all the open source that is available today."}, {"speaker": "SPEAKER_01", "timestamp": [461.2, 463.06], "text": " You don't have to put proprietary agents"}, {"speaker": "SPEAKER_01", "timestamp": [463.06, 466.0], "text": " and do proprietary information code thanks"}, {"speaker": "SPEAKER_01", "timestamp": [466.0, 470.88], "text": " to cncf thanks to open source monitoring that's available so the first thing as we said for"}, {"speaker": "SPEAKER_01", "timestamp": [470.88, 475.44], "text": " context is understand the structure of the application what we call the application graph"}, {"speaker": "SPEAKER_01", "timestamp": [475.44, 479.84], "text": " right so imagine able to automatically build out that structure in real time"}, {"speaker": "SPEAKER_03", "timestamp": [480.56, 489.0], "text": " so the ops guy doesn't know even the app developer doesn't have to go around trying to figure out what's talking to or do something exotic to get there."}, {"speaker": "SPEAKER_03", "timestamp": [489.0, 499.0], "text": " And that graph has to change dynamically. That graph tells you who's talking to whom, what are they dependent on, how is Kubernetes, you know, managing it, allocating resources appropriately or not,"}, {"speaker": "SPEAKER_03", "timestamp": [499.0, 506.4], "text": " and how does Kubernetes have access to the type of cloud resources it needs to allocate to the services"}, {"speaker": "SPEAKER_03", "timestamp": [507.44, 514.32], "text": " and then to really understand what's going on that context pull the data and what we do is something"}, {"speaker": "SPEAKER_03", "timestamp": [514.32, 519.76], "text": " called a behavior model profile every component that comprises the application to see what is"}, {"speaker": "SPEAKER_03", "timestamp": [519.76, 524.96], "text": " expected profiling is kind of like building a very simple emulator model and you can look at it"}, {"speaker": "SPEAKER_03", "timestamp": [524.24, 528.96], "text": " Profiling is kind of like building a very simple emulator model. And you can look at it, collect the data to figure out, for example, across all the metrics"}, {"speaker": "SPEAKER_03", "timestamp": [528.96, 533.88], "text": " that you collect, all the flows and events, and say, hey, this one, for example, is IO"}, {"speaker": "SPEAKER_00", "timestamp": [533.88, 537.72], "text": " dependent, this one is CPU dependent, or mix, or this one does a lot of calls."}, {"speaker": "SPEAKER_02", "timestamp": [537.72, 542.86], "text": " So that as data is coming in, as you learn that, this is where ML comes in, you know"}, {"speaker": "SPEAKER_00", "timestamp": [542.86, 550.54], "text": " what to expect, because you have the realtime metrics, you have the application graph. So once you have started learning that, over time,"}, {"speaker": "SPEAKER_03", "timestamp": [551.68, 555.88], "text": " you know, within, actually in our case, within 24 hours, you start getting a baseline behavior,"}, {"speaker": "SPEAKER_03", "timestamp": [555.88, 560.14], "text": " which gets better over time, you can start looking at deviations. So you don't have to"}, {"speaker": "SPEAKER_04", "timestamp": [560.14, 563.66], "text": " worry about setting thresholds. You don't have to try and guess what the thresholds are and start"}, {"speaker": "SPEAKER_04", "timestamp": [563.66, 570.16], "text": " tuning them. First of all, you don't know which metric and what the level is let the ml model learn expose that so"}, {"speaker": "SPEAKER_03", "timestamp": [570.16, 575.84], "text": " that we can analyze it again in context we know what what are the what are the drivers for every"}, {"speaker": "SPEAKER_03", "timestamp": [575.84, 581.04], "text": " service uh because we know the application route so once we have these deviations whether it's"}, {"speaker": "SPEAKER_03", "timestamp": [581.04, 589.58], "text": " coming from explicit alerts like a failure in infrastructure, a failure that Kubernetes detects, or application starts slowing down or having a degradation,"}, {"speaker": "SPEAKER_03", "timestamp": [590.16, 596.28], "text": " you put it all in context and then analyze that. We essentially do what we call local detection"}, {"speaker": "SPEAKER_03", "timestamp": [596.28, 601.58], "text": " across the application, and then we call an analysis of what we call dynamic decision,"}, {"speaker": "SPEAKER_03", "timestamp": [601.72, 605.04], "text": " which looks at everything in context, why would this happen given what I"}, {"speaker": "SPEAKER_03", "timestamp": [605.04, 610.74], "text": " have seen so essentially think of it it's almost like an anthropomorphic what an ops would do and"}, {"speaker": "SPEAKER_03", "timestamp": [610.74, 618.66], "text": " they understand if we can put all of this in place and automate this pipeline we have reduced amount"}, {"speaker": "SPEAKER_03", "timestamp": [618.66, 623.64], "text": " of work that off spends today trying to understand what is it what does the application look like"}, {"speaker": "SPEAKER_03", "timestamp": [623.64, 630.18], "text": " who's talking to whom when is there a problem there instead of setting thresholds, and if I do, how do I analyze"}, {"speaker": "SPEAKER_03", "timestamp": [630.18, 637.82], "text": " it? If we can collapse that and reduce that, we have really done the right service to get the"}, {"speaker": "SPEAKER_03", "timestamp": [637.82, 642.42], "text": " right level of intelligence and observability. So this flow, if you think about it, is what we will"}, {"speaker": "SPEAKER_03", "timestamp": [642.42, 645.28], "text": " demo today using what you're seeing on the left"}, {"speaker": "SPEAKER_03", "timestamp": [645.28, 651.2], "text": " that essentially build context understand the application graph understand the behavior to"}, {"speaker": "SPEAKER_04", "timestamp": [651.2, 656.48], "text": " surface problems detected analyze it in context using all the telemetry we have"}, {"speaker": "SPEAKER_03", "timestamp": [657.52, 664.08], "text": " you know including changes logs events and help isolate the cause and so that's our purpose of"}, {"speaker": "SPEAKER_03", "timestamp": [664.08, 665.22], "text": " our demo today."}, {"speaker": "SPEAKER_03", "timestamp": [667.32, 669.16], "text": " And I'm going to hand this over to Cesar because we want to get to the demo"}, {"speaker": "SPEAKER_03", "timestamp": [669.16, 671.78], "text": " and he'll tell you exactly how we leverage"}, {"speaker": "SPEAKER_03", "timestamp": [671.78, 676.92], "text": " open source monitoring and use CNCF,"}, {"speaker": "SPEAKER_03", "timestamp": [677.02, 678.08], "text": " OpenTelemetry to do this."}, {"speaker": "SPEAKER_00", "timestamp": [678.14, 679.06], "text": " So I'm going to stop sharing."}, {"speaker": "SPEAKER_00", "timestamp": [679.2, 680.8], "text": " Cesar, please take it away."}, {"speaker": "SPEAKER_00", "timestamp": [681.72, 683.88], "text": " Actually, look, if you could go back to that,"}, {"speaker": "SPEAKER_00", "timestamp": [683.94, 686.04], "text": " we'll talk really briefly about those"}, {"speaker": "SPEAKER_00", "timestamp": [686.04, 687.26], "text": " open source platforms"}, {"speaker": "SPEAKER_00", "timestamp": [687.26, 689.7], "text": " that we're leveraging. If you could share that."}, {"speaker": "SPEAKER_03", "timestamp": [690.16, 692.28], "text": " I'll go back. I shouldn't have done"}, {"speaker": "SPEAKER_00", "timestamp": [692.28, 693.46], "text": " that. I was a little too hasty there."}, {"speaker": "SPEAKER_00", "timestamp": [697.68, 699.14], "text": " All right. Is that coming up?"}, {"speaker": "SPEAKER_03", "timestamp": [700.08, 701.94], "text": " There it is."}, {"speaker": "SPEAKER_00", "timestamp": [702.26, 702.58], "text": " All right."}, {"speaker": "SPEAKER_03", "timestamp": [703.76, 706.0], "text": " Yeah. So again, everybody, my name is Cesar Quintana."}, {"speaker": "SPEAKER_00", "timestamp": [706.0, 709.0], "text": " I'm a principal solutions architect at OpsCrews here."}, {"speaker": "SPEAKER_00", "timestamp": [709.0, 713.0], "text": " And, yeah, so to add on to what Alok was mentioning, right,"}, {"speaker": "SPEAKER_00", "timestamp": [713.0, 718.0], "text": " the whole premise of leveraging these open source platforms that,"}, {"speaker": "SPEAKER_00", "timestamp": [718.0, 723.0], "text": " you know, essentially the whole data collection layer has been"}, {"speaker": "SPEAKER_00", "timestamp": [723.0, 724.0], "text": " commoditized, right?"}, {"speaker": "SPEAKER_00", "timestamp": [724.0, 734.54], "text": " Observing data is now easier than ever to access thanks to these, you know, powerful open source, particularly around the CNCF platforms, right?"}, {"speaker": "SPEAKER_00", "timestamp": [734.6, 749.3], "text": " So what we've set out in mind, right, is to build something and leverage these amazing tools to make everybody's life easier, right? So things like this is an example of our architecture"}, {"speaker": "SPEAKER_00", "timestamp": [749.72, 753.84], "text": " of how we're leveraging all this open source data"}, {"speaker": "SPEAKER_00", "timestamp": [753.84, 755.34], "text": " and all these open source platforms."}, {"speaker": "SPEAKER_00", "timestamp": [755.34, 758.22], "text": " So as you'll notice here,"}, {"speaker": "SPEAKER_00", "timestamp": [758.22, 761.2], "text": " if you focus on that Kubernetes cluster square"}, {"speaker": "SPEAKER_00", "timestamp": [761.2, 763.24], "text": " on the right side, right?"}, {"speaker": "SPEAKER_00", "timestamp": [763.24, 765.58], "text": " What you'll see is across the top in the"}, {"speaker": "SPEAKER_00", "timestamp": [765.58, 769.18], "text": " green you'll see your workloads right you know pod one two three four these"}, {"speaker": "SPEAKER_00", "timestamp": [769.18, 772.9], "text": " are eventually your own applications running whatever you're doing whether"}, {"speaker": "SPEAKER_00", "timestamp": [772.9, 777.32], "text": " you're running an e-commerce site a financial trading platform etc this is"}, {"speaker": "SPEAKER_00", "timestamp": [777.32, 781.0], "text": " what you're running inside your actual workloads but underneath in the in that"}, {"speaker": "SPEAKER_00", "timestamp": [781.0, 788.8], "text": " light and dark blue are the open source tools that are now so common throughout the"}, {"speaker": "SPEAKER_00", "timestamp": [788.8, 794.76], "text": " IT landscape and in the modern application environments. So towards the bottom with the"}, {"speaker": "SPEAKER_00", "timestamp": [794.76, 800.28], "text": " dark blue, you'll see here in this reference architecture, we're showing Jaeger, Prometheus,"}, {"speaker": "SPEAKER_00", "timestamp": [800.6, 826.48], "text": " Loki. It could be something, this is just an example. We can leverage logs from other sources like Fluent. I think somebody asked about Fluent. It could be Loki. It could be Fluent. And then we take metrics in from Prometheus. And then Traces. We're leveraging Jaeger as a backend for our particular architecture. But we are supporting open telemetry libraries for the client side."}, {"speaker": "SPEAKER_00", "timestamp": [826.48, 832.44], "text": " So the important that's one of the really, really cool things about the new standards"}, {"speaker": "SPEAKER_00", "timestamp": [832.44, 837.76], "text": " is that they're now well defined, which means that you could be using a mixture in your"}, {"speaker": "SPEAKER_01", "timestamp": [837.76, 844.76], "text": " environment of open zipkin and Jaeger, the open telemetry libraries themselves and still"}, {"speaker": "SPEAKER_00", "timestamp": [844.76, 851.0], "text": " have a unified back end where you're able to collect all that data and leverage it and use it,"}, {"speaker": "SPEAKER_00", "timestamp": [851.0, 855.0], "text": " even though you're technically using disparate libraries throughout your enterprise."}, {"speaker": "SPEAKER_00", "timestamp": [855.0, 872.32], "text": " Right. So so what you'll see here, you know how we've architected ourselves to be built is again around these open source platforms again whether it's fluency whether it's loki and prometheus and jaeger etc they serve as you know now your your your data collection and data data"}, {"speaker": "SPEAKER_00", "timestamp": [872.32, 879.36], "text": " store you don't have to go out and pay another vendor you know uh 10 15 x for storing just"}, {"speaker": "SPEAKER_00", "timestamp": [879.36, 883.36], "text": " metrics right when you can store them in your own infrastructure we're all doing the same thing"}, {"speaker": "SPEAKER_00", "timestamp": [883.36, 890.4], "text": " right just putting them inside of uh uh inside of a long-term, you know, bucket, right? And so now that's under"}, {"speaker": "SPEAKER_00", "timestamp": [890.4, 897.28], "text": " your control. And so we, for example, Promptail, right? If you start looking upward towards the"}, {"speaker": "SPEAKER_00", "timestamp": [897.28, 902.3], "text": " stack in the light blue, Promptail will run as a daemon set, collect logs from all your nodes and"}, {"speaker": "SPEAKER_01", "timestamp": [902.3, 905.04], "text": " from all your containers, right?"}, {"speaker": "SPEAKER_01", "timestamp": [907.84, 908.0], "text": " And then you have on top of that node exporter, right,"}, {"speaker": "SPEAKER_01", "timestamp": [912.78, 913.08], "text": " functioning as an exporter for Prometheus to grab the metrics from the nodes themselves."}, {"speaker": "SPEAKER_00", "timestamp": [921.1, 921.38], "text": " And going above that, you'll see C-Advisor collecting data from the containers themselves running on each node."}, {"speaker": "SPEAKER_00", "timestamp": [930.44, 937.02], "text": " And then we also leverage KSM exporter, pretty awesome, grabbing Kubernetes object status data. And all those are going to be fed out into Prometheus or to if you're using traces again to Jaeger. And really, you know, now, even just with that,"}, {"speaker": "SPEAKER_00", "timestamp": [937.16, 943.6], "text": " you've got a pretty darn functional observability layer, right? Now you have metrics and they have"}, {"speaker": "SPEAKER_00", "timestamp": [943.6, 946.82], "text": " traces. Now you can go into different places and look at your logs."}, {"speaker": "SPEAKER_00", "timestamp": [947.46, 952.58], "text": " But what we're, you know, what Alok was mentioning earlier is that smart layer, right?"}, {"speaker": "SPEAKER_00", "timestamp": [953.24, 958.18], "text": " Now you want to leverage all those pieces of data, bring them in together and do something"}, {"speaker": "SPEAKER_00", "timestamp": [958.18, 962.26], "text": " really, really powerful with having all that context, all that configuration data that"}, {"speaker": "SPEAKER_00", "timestamp": [962.26, 965.78], "text": " we can grab from the Kubernetes API, and then just bring it all together."}, {"speaker": "SPEAKER_00", "timestamp": [966.24, 973.12], "text": " On top of that, you have metric data, configuration data, performance data,"}, {"speaker": "SPEAKER_00", "timestamp": [973.28, 977.06], "text": " and event data from your cloud environments, right?"}, {"speaker": "SPEAKER_00", "timestamp": [977.06, 981.36], "text": " So bringing in things like, you know, more and more applications are hybrid, right?"}, {"speaker": "SPEAKER_00", "timestamp": [981.42, 990.08], "text": " They're using, you know, whether it's VM and kubernetes or serverless and uh and pass you know you have all these really really hybrid environments that"}, {"speaker": "SPEAKER_00", "timestamp": [990.08, 997.04], "text": " again it's the whole um uh extreme uh production of data and having one place and easy ways to"}, {"speaker": "SPEAKER_00", "timestamp": [997.04, 1001.2], "text": " collect them and that's really what these open source platforms have allowed us to do right but"}, {"speaker": "SPEAKER_00", "timestamp": [1001.2, 1009.98], "text": " going back to what i was mentioning about cloud you also want a place where you can grab your data and bring it in. Talking about, again, serverless or function as"}, {"speaker": "SPEAKER_00", "timestamp": [1009.98, 1016.96], "text": " a service, the PaaS layers, which are only constantly growing, right? You have these cloud"}, {"speaker": "SPEAKER_00", "timestamp": [1016.96, 1027.32], "text": " caches and messaging services, cloud databases, et cetera. So what OpsCrew sets out to do is not only grab that open source data"}, {"speaker": "SPEAKER_00", "timestamp": [1027.32, 1028.78], "text": " and leverage those collection platforms,"}, {"speaker": "SPEAKER_00", "timestamp": [1028.78, 1034.44], "text": " but also bring in the cloud data and mesh it all together"}, {"speaker": "SPEAKER_00", "timestamp": [1034.44, 1035.72], "text": " and build something really, really rich"}, {"speaker": "SPEAKER_00", "timestamp": [1035.72, 1038.6], "text": " and then provide actionable data based on that."}, {"speaker": "SPEAKER_00", "timestamp": [1039.18, 1044.22], "text": " So what I'm going to do is I'm going to show you a demo of OpsCrew's."}, {"speaker": "SPEAKER_00", "timestamp": [1044.4, 1046.02], "text": " Oh, sorry, Alok, did you want to add?"}, {"speaker": "SPEAKER_03", "timestamp": [1046.18, 1048.42], "text": " Since I had the opportunity to look at the message,"}, {"speaker": "SPEAKER_00", "timestamp": [1048.58, 1051.72], "text": " someone asked, what about FluentD or FluentBit?"}, {"speaker": "SPEAKER_00", "timestamp": [1051.92, 1053.16], "text": " And, you know, everything for that."}, {"speaker": "SPEAKER_00", "timestamp": [1053.26, 1054.6], "text": " So let's address that."}, {"speaker": "SPEAKER_00", "timestamp": [1056.48, 1060.7], "text": " Yeah, no, so as mentioned, right, we can take logs, basically,"}, {"speaker": "SPEAKER_00", "timestamp": [1061.1, 1063.98], "text": " from whether it's Loki, FluentBit is usually the thing,"}, {"speaker": "SPEAKER_00", "timestamp": [1064.18, 1066.02], "text": " or FluentD, those are usually the pieces we"}, {"speaker": "SPEAKER_00", "timestamp": [1066.02, 1068.3], "text": " run into, right? And absolutely"}, {"speaker": "SPEAKER_00", "timestamp": [1068.3, 1070.18], "text": " you know, the whole point is to build a modular"}, {"speaker": "SPEAKER_00", "timestamp": [1070.18, 1070.84], "text": " flexible"}, {"speaker": "SPEAKER_00", "timestamp": [1070.84, 1074.28], "text": " platform where you can grab"}, {"speaker": "SPEAKER_00", "timestamp": [1074.28, 1076.06], "text": " data from, you know,"}, {"speaker": "SPEAKER_00", "timestamp": [1076.12, 1078.44], "text": " whatever your preferred variant"}, {"speaker": "SPEAKER_00", "timestamp": [1078.44, 1080.18], "text": " of that is,"}, {"speaker": "SPEAKER_00", "timestamp": [1080.38, 1082.1], "text": " right? So yeah, absolutely. OpsCrews particularly"}, {"speaker": "SPEAKER_00", "timestamp": [1082.1, 1083.4], "text": " provides support for Fluent D,"}, {"speaker": "SPEAKER_00", "timestamp": [1084.0, 1086.72], "text": " Loki, and a few others as well"}, {"speaker": "SPEAKER_03", "timestamp": [1086.72, 1091.16], "text": " yeah so the takeaway message I want to do before we go to the demo so we can you know address them"}, {"speaker": "SPEAKER_00", "timestamp": [1091.16, 1098.6], "text": " all across is as long as we have the source of getting the logs in the metrics in this approach"}, {"speaker": "SPEAKER_00", "timestamp": [1098.6, 1103.58], "text": " will still work of course with OpenCNC we don't have to do proprietary agents proprietary"}, {"speaker": "SPEAKER_00", "timestamp": [1103.58, 1107.32], "text": " instrumentation we can be sitting outside without being intrusive."}, {"speaker": "SPEAKER_03", "timestamp": [1107.52, 1108.88], "text": " So think of it that way."}, {"speaker": "SPEAKER_03", "timestamp": [1109.08, 1112.76], "text": " The real intelligence or observability is not how the metrics got to us"}, {"speaker": "SPEAKER_03", "timestamp": [1112.96, 1115.32], "text": " and what it is, as long as we have coverage."}, {"speaker": "SPEAKER_03", "timestamp": [1115.32, 1118.08], "text": " That's the key. The coverage is all of these is needed."}, {"speaker": "SPEAKER_00", "timestamp": [1118.08, 1121.32], "text": " You can't just go on metrics and logs and traces independently."}, {"speaker": "SPEAKER_00", "timestamp": [1121.52, 1122.72], "text": " It doesn't give you the whole picture."}, {"speaker": "SPEAKER_04", "timestamp": [1122.92, 1125.3], "text": " Otherwise we are one of the six blind men looking"}, {"speaker": "SPEAKER_04", "timestamp": [1125.3, 1128.14], "text": " at the elephant, which is in the room."}, {"speaker": "SPEAKER_03", "timestamp": [1128.14, 1129.3], "text": " All right, go ahead, Cesar."}, {"speaker": "SPEAKER_00", "timestamp": [1129.3, 1131.26], "text": " I had to say that because I think it's good."}, {"speaker": "SPEAKER_03", "timestamp": [1131.26, 1133.02], "text": " CESAR GAVIDIA MARTÍNEZ- Thanks for that."}, {"speaker": "SPEAKER_00", "timestamp": [1133.02, 1134.94], "text": " All right, so now I'll share."}, {"speaker": "SPEAKER_00", "timestamp": [1134.94, 1137.1], "text": " So look, I think you might have to stop."}, {"speaker": "SPEAKER_03", "timestamp": [1137.1, 1138.52], "text": " CESAR GAVIDIA MARTÍNEZ- I can stop sharing, right?"}, {"speaker": "SPEAKER_00", "timestamp": [1138.52, 1140.76], "text": " Sure."}, {"speaker": "SPEAKER_00", "timestamp": [1140.76, 1143.56], "text": " CESAR GAVIDIA MARTÍNEZ- All right."}, {"speaker": "SPEAKER_00", "timestamp": [1143.56, 1155.36], "text": " So let me share hopefully you guys will be able to see my screen here"}, {"speaker": "SPEAKER_03", "timestamp": [1156.82, 1162.32], "text": " so I have one proof pointed but there you go it's coming up excellent okay"}, {"speaker": "SPEAKER_00", "timestamp": [1162.32, 1168.32], "text": " awesome so yeah so this is this is a uh this is our landing page for"}, {"speaker": "SPEAKER_00", "timestamp": [1168.32, 1173.52], "text": " obscures and you can see there's there's uh quite a few pieces of data here um you might you know"}, {"speaker": "SPEAKER_00", "timestamp": [1173.52, 1180.16], "text": " this screen might look familiar uh for any of you who have used apm tools uh before so this is a"}, {"speaker": "SPEAKER_00", "timestamp": [1180.16, 1187.34], "text": " real-time service apology um map essentially you know you know, we're leveraging, as Alok mentioned,"}, {"speaker": "SPEAKER_00", "timestamp": [1187.44, 1196.44], "text": " eBPF as well, right? So eBPF allows us to grab this network data and bring it in alongside not"}, {"speaker": "SPEAKER_00", "timestamp": [1196.44, 1202.9], "text": " only the tracing, which in this case happens to be optional because we have eBPF, but it is the"}, {"speaker": "SPEAKER_01", "timestamp": [1202.9, 1206.02], "text": " eBPF network data alongside the tracing data,"}, {"speaker": "SPEAKER_04", "timestamp": [1206.14, 1208.66], "text": " alongside the metric data, alongside the logs,"}, {"speaker": "SPEAKER_01", "timestamp": [1208.8, 1211.78], "text": " alongside the configuration data discovered from whether it's cloud"}, {"speaker": "SPEAKER_01", "timestamp": [1211.78, 1214.04], "text": " or Kubernetes or the virtual machines themselves"}, {"speaker": "SPEAKER_01", "timestamp": [1214.04, 1216.1], "text": " or the serverless or other PaaS components."}, {"speaker": "SPEAKER_01", "timestamp": [1216.18, 1218.1], "text": " It's all brought together in a single place."}, {"speaker": "SPEAKER_01", "timestamp": [1218.5, 1221.2], "text": " But we're giving you this real-time, excuse me,"}, {"speaker": "SPEAKER_01", "timestamp": [1221.36, 1225.2], "text": " a flow of how your services are interacting with each other."}, {"speaker": "SPEAKER_01", "timestamp": [1225.2, 1227.04], "text": " And I'm zooming in more."}, {"speaker": "SPEAKER_01", "timestamp": [1227.04, 1230.72], "text": " Of course, now this is not even, this is nowhere close to some of the busiest environments,"}, {"speaker": "SPEAKER_01", "timestamp": [1230.72, 1233.44], "text": " but you can see that it does get busy really, really quick."}, {"speaker": "SPEAKER_01", "timestamp": [1233.6, 1237.96], "text": " And that's one of the cool things about, you know, having all the configuration data"}, {"speaker": "SPEAKER_01", "timestamp": [1237.96, 1241.96], "text": " and the really rich data that the underlying tools like C-Advisor collect"}, {"speaker": "SPEAKER_01", "timestamp": [1241.96, 1247.92], "text": " is that we get a lot of really rich object data along with the metric"}, {"speaker": "SPEAKER_00", "timestamp": [1247.92, 1256.66], "text": " blocks. So things like being able to understand the configuration data of these pieces allows us"}, {"speaker": "SPEAKER_00", "timestamp": [1256.66, 1263.48], "text": " to also extract things like labels and tags. So when you have a busy environment, you might only"}, {"speaker": "SPEAKER_00", "timestamp": [1263.48, 1266.38], "text": " want to filter, for example, on a particular namespace, right?"}, {"speaker": "SPEAKER_00", "timestamp": [1266.38, 1269.78], "text": " I might only want to look at maybe the Obscures namespace."}, {"speaker": "SPEAKER_00", "timestamp": [1269.78, 1273.62], "text": " And so that really helps you cut down on some of that noise"}, {"speaker": "SPEAKER_00", "timestamp": [1273.62, 1276.74], "text": " when you're trying to isolate an issue."}, {"speaker": "SPEAKER_00", "timestamp": [1276.74, 1278.36], "text": " But going back to kind of our premise,"}, {"speaker": "SPEAKER_00", "timestamp": [1278.36, 1280.46], "text": " what we're showing here is a mixture"}, {"speaker": "SPEAKER_00", "timestamp": [1280.46, 1282.08], "text": " of quite a few different pieces of data."}, {"speaker": "SPEAKER_00", "timestamp": [1282.08, 1284.36], "text": " You're showing the EVPF pieces."}, {"speaker": "SPEAKER_00", "timestamp": [1284.36, 1290.46], "text": " Again, we talked about Cloud. So this demo happens to be running inside of AWS. But, you know, whatever cloud you're"}, {"speaker": "SPEAKER_00", "timestamp": [1290.46, 1295.26], "text": " running on, you're going to have that PaaS layer very likely. So being able to collect that data"}, {"speaker": "SPEAKER_00", "timestamp": [1295.26, 1299.94], "text": " and bring it all together to your Kubernetes environments, you know, and all monitor it in"}, {"speaker": "SPEAKER_00", "timestamp": [1299.94, 1305.88], "text": " a single place is absolutely powerful. So if I click on, for example, that AWS RDS instance,"}, {"speaker": "SPEAKER_00", "timestamp": [1306.3, 1309.04], "text": " you know, again, we're talking about the metrics. So if you look at this right side,"}, {"speaker": "SPEAKER_00", "timestamp": [1309.36, 1314.22], "text": " we're collecting all those individual metrics, the read IOPS and the throughput, et cetera."}, {"speaker": "SPEAKER_00", "timestamp": [1315.18, 1321.0], "text": " And this is a high level summary, but important is metrics, right? So I can go in here and look"}, {"speaker": "SPEAKER_00", "timestamp": [1321.0, 1325.28], "text": " at all the individual metrics. That one of the pillars right over topability"}, {"speaker": "SPEAKER_00", "timestamp": [1325.28, 1330.88], "text": " um and that's just that's just for one entity same thing for uh same thing for a pod right"}, {"speaker": "SPEAKER_00", "timestamp": [1330.88, 1335.44], "text": " this is a pod in the container so if i click on a pod same thing i'm bringing back all this"}, {"speaker": "SPEAKER_00", "timestamp": [1335.44, 1340.8], "text": " configuration data all these labels you know what time this was created uh what's what host it's"}, {"speaker": "SPEAKER_00", "timestamp": [1340.8, 1344.0], "text": " running on it's important to understand all these things because when you're troubleshooting you"}, {"speaker": "SPEAKER_00", "timestamp": [1344.0, 1345.02], "text": " know well when was i well you know, well, when was I,"}, {"speaker": "SPEAKER_00", "timestamp": [1345.14, 1347.0], "text": " you know, what time was this pod running?"}, {"speaker": "SPEAKER_00", "timestamp": [1347.08, 1348.92], "text": " It was supposed to have been restarted five minutes ago."}, {"speaker": "SPEAKER_00", "timestamp": [1349.0, 1350.34], "text": " Did we actually perform the restart"}, {"speaker": "SPEAKER_00", "timestamp": [1350.34, 1351.8], "text": " or was there an issue, you know,"}, {"speaker": "SPEAKER_00", "timestamp": [1351.8, 1354.4], "text": " doing that rollout of the application?"}, {"speaker": "SPEAKER_00", "timestamp": [1354.58, 1356.12], "text": " Well, look, it's been running for, you know,"}, {"speaker": "SPEAKER_00", "timestamp": [1356.54, 1357.82], "text": " since a couple months ago."}, {"speaker": "SPEAKER_00", "timestamp": [1358.18, 1361.46], "text": " So, hey, that rollout wasn't successful, right?"}, {"speaker": "SPEAKER_00", "timestamp": [1362.18, 1365.06], "text": " Again, we've got metrics as well,"}, {"speaker": "SPEAKER_00", "timestamp": [1365.28, 1368.5], "text": " and each entity has its own pieces of data,"}, {"speaker": "SPEAKER_00", "timestamp": [1368.58, 1371.38], "text": " and it's important to be able to look at that data, again,"}, {"speaker": "SPEAKER_00", "timestamp": [1371.48, 1375.54], "text": " in context for, you know, whatever problem you're troubleshooting."}, {"speaker": "SPEAKER_00", "timestamp": [1376.0, 1378.06], "text": " In this scenario, I clicked on this container."}, {"speaker": "SPEAKER_00", "timestamp": [1378.3, 1380.68], "text": " It happens to be the Jaeger agent, but I click on this container,"}, {"speaker": "SPEAKER_00", "timestamp": [1380.82, 1382.88], "text": " and now I'm getting, you know, additional data that's contextual"}, {"speaker": "SPEAKER_00", "timestamp": [1382.88, 1385.12], "text": " for that particular"}, {"speaker": "SPEAKER_00", "timestamp": [1385.12, 1391.44], "text": " container, the ports that are being exposed. But on top of that, being able to see how the"}, {"speaker": "SPEAKER_00", "timestamp": [1391.44, 1397.36], "text": " infrastructure is working, what things are related to what. So for example, we have these contextual"}, {"speaker": "SPEAKER_00", "timestamp": [1397.36, 1402.4], "text": " access to these different pieces, right? So if I click on this three layer view, right, what it"}, {"speaker": "SPEAKER_00", "timestamp": [1402.4, 1408.16], "text": " does is it shows me this particular container and this pod"}, {"speaker": "SPEAKER_00", "timestamp": [1408.16, 1413.04], "text": " is running some details about it, the IP address, the image name that it's using, as well as some"}, {"speaker": "SPEAKER_00", "timestamp": [1413.04, 1418.88], "text": " high-level metrics such as CPU and memory. But also, it shows me what Kubernetes node this"}, {"speaker": "SPEAKER_00", "timestamp": [1418.88, 1423.36], "text": " particular container is running on, as well as some of the neighbors and those CPU and memory"}, {"speaker": "SPEAKER_00", "timestamp": [1423.36, 1429.04], "text": " metrics for those neighbors. And then this Kubernetes node is running on top of what cloud instance, right? So when you're"}, {"speaker": "SPEAKER_00", "timestamp": [1429.04, 1434.0], "text": " troubleshooting, I know I have some instances in, let's say you're running EKS and you have"}, {"speaker": "SPEAKER_00", "timestamp": [1434.64, 1439.52], "text": " some nodes in one particular subnet or one particular availability zone that are having"}, {"speaker": "SPEAKER_00", "timestamp": [1439.52, 1444.96], "text": " connectivity issues and you're trying to diagnose, right, in a single click, you can understand"}, {"speaker": "SPEAKER_00", "timestamp": [1444.96, 1449.12], "text": " if your container happens to be running on one of those nodes and things"}, {"speaker": "SPEAKER_00", "timestamp": [1449.12, 1452.26], "text": " like the region and how much storage is attached to it."}, {"speaker": "SPEAKER_00", "timestamp": [1452.26, 1460.52], "text": " But not only that, again, as we mentioned, the ubiquitousness of all this data and the"}, {"speaker": "SPEAKER_00", "timestamp": [1460.52, 1464.62], "text": " ease of collecting makes it really, really simple to bring it all together."}, {"speaker": "SPEAKER_00", "timestamp": [1464.84, 1467.1], "text": " And now we can look at the infrastructure map that we call,"}, {"speaker": "SPEAKER_00", "timestamp": [1467.1, 1468.72], "text": " which is essentially a cloud map."}, {"speaker": "SPEAKER_00", "timestamp": [1468.72, 1470.12], "text": " And now we're looking in the context"}, {"speaker": "SPEAKER_00", "timestamp": [1470.12, 1473.04], "text": " of this particular cloud instance,"}, {"speaker": "SPEAKER_00", "timestamp": [1473.04, 1475.5], "text": " and we're looking at this EC2 virtual machine"}, {"speaker": "SPEAKER_00", "timestamp": [1475.5, 1478.5], "text": " and looking at the configuration of that and the tags, right?"}, {"speaker": "SPEAKER_00", "timestamp": [1478.5, 1481.76], "text": " And I'm just kind of showing behind the scenes,"}, {"speaker": "SPEAKER_00", "timestamp": [1483.1, 1488.88], "text": " all the open source data that we're actually collecting and how even that open source"}, {"speaker": "SPEAKER_00", "timestamp": [1488.88, 1490.84], "text": " data by itself makes it really powerful."}, {"speaker": "SPEAKER_00", "timestamp": [1490.84, 1491.98], "text": " But once you combine the intelligence,"}, {"speaker": "SPEAKER_00", "timestamp": [1491.98, 1493.38], "text": " which I'll talk about in a second,"}, {"speaker": "SPEAKER_00", "timestamp": [1493.38, 1495.78], "text": " that's where things really start to take off."}, {"speaker": "SPEAKER_00", "timestamp": [1495.78, 1497.62], "text": " But as we mentioned, we're collecting data"}, {"speaker": "SPEAKER_00", "timestamp": [1497.62, 1501.64], "text": " from the Kubernetes API and from the container."}, {"speaker": "SPEAKER_00", "timestamp": [1501.64, 1504.3], "text": " So that's where we're grabbing the individual container"}, {"speaker": "SPEAKER_01", "timestamp": [1504.3, 1506.0], "text": " metrics and the node metrics."}, {"speaker": "SPEAKER_00", "timestamp": [1507.52, 1510.56], "text": " We also have an understanding, for example, at a per node view, right?"}, {"speaker": "SPEAKER_00", "timestamp": [1510.64, 1513.8], "text": " So instead of looking at it from a kind of application centered view,"}, {"speaker": "SPEAKER_00", "timestamp": [1513.86, 1516.08], "text": " I can look at the node level."}, {"speaker": "SPEAKER_00", "timestamp": [1516.5, 1517.72], "text": " Let's clear out some of these filters."}, {"speaker": "SPEAKER_00", "timestamp": [1518.5, 1520.6], "text": " Now, so you see we have five nodes running,"}, {"speaker": "SPEAKER_00", "timestamp": [1520.92, 1522.82], "text": " and now I'm looking at each individual node,"}, {"speaker": "SPEAKER_00", "timestamp": [1523.1, 1528.4], "text": " and I can see the workloads that are running on top of that node I can click on metrics and get"}, {"speaker": "SPEAKER_00", "timestamp": [1528.4, 1534.34], "text": " the metrics for that particular node so load in just a second but I'll go back"}, {"speaker": "SPEAKER_00", "timestamp": [1534.34, 1547.04], "text": " and then we can actually look at the configuration for the particular node Yep. I have some filters on here by default. Real time internet issues."}, {"speaker": "SPEAKER_03", "timestamp": [1548.64, 1554.48], "text": " Always fun. There we go. All right. So yeah, so again, we're collecting all the configuration"}, {"speaker": "SPEAKER_00", "timestamp": [1554.48, 1559.28], "text": " and metadata, not only of the containers themselves, but even the nodes that you're"}, {"speaker": "SPEAKER_03", "timestamp": [1559.28, 1565.36], "text": " running on. So things like the memory utilized, sorry, the memory capacity, whether the node is ready."}, {"speaker": "SPEAKER_01", "timestamp": [1565.44, 1571.34], "text": " So you'll see here like max memory, max storage, what version of Kubernetes are they running?"}, {"speaker": "SPEAKER_03", "timestamp": [1571.82, 1578.76], "text": " And so, you know, here we see that we're running version 117 of the Kubernetes node, which is actually probably a little bit outdated."}, {"speaker": "SPEAKER_00", "timestamp": [1579.52, 1585.94], "text": " And the kernel version of the operating system that it's running on, et cetera. So we're bringing, again, all this data together,"}, {"speaker": "SPEAKER_03", "timestamp": [1585.94, 1589.56], "text": " which is really, really empowered by all these open source layer tools."}, {"speaker": "SPEAKER_00", "timestamp": [1589.72, 1590.78], "text": " We're not using custom agents."}, {"speaker": "SPEAKER_03", "timestamp": [1590.9, 1592.66], "text": " We're not doing anything special."}, {"speaker": "SPEAKER_00", "timestamp": [1592.8, 1594.48], "text": " It's just leveraging all this data,"}, {"speaker": "SPEAKER_00", "timestamp": [1594.56, 1596.5], "text": " but bringing it all together in a single place."}, {"speaker": "SPEAKER_00", "timestamp": [1597.94, 1600.62], "text": " On top of that, I mentioned it's important to cover things"}, {"speaker": "SPEAKER_00", "timestamp": [1600.62, 1602.66], "text": " like PaaS services and serverless."}, {"speaker": "SPEAKER_00", "timestamp": [1602.66, 1606.3], "text": " So, again, we also collect that kind of data."}, {"speaker": "SPEAKER_00", "timestamp": [1606.3, 1609.66], "text": " So you'll notice here you saw an RDS instance."}, {"speaker": "SPEAKER_00", "timestamp": [1609.66, 1611.84], "text": " I think I'm going to show a load balancer as well."}, {"speaker": "SPEAKER_00", "timestamp": [1611.84, 1616.2], "text": " In this case, in this environment, I have an API gateway running with"}, {"speaker": "SPEAKER_00", "timestamp": [1616.66, 1620.34], "text": " with an S3 call out actually via serverless."}, {"speaker": "SPEAKER_00", "timestamp": [1620.8, 1622.92], "text": " So you'll see this API gateway."}, {"speaker": "SPEAKER_00", "timestamp": [1622.92, 1626.0], "text": " And again, I'm grabbing the data from that particular API gateway,"}, {"speaker": "SPEAKER_00", "timestamp": [1626.0, 1630.0], "text": " just like for the containers we saw that particular entities made it in for the nodes."}, {"speaker": "SPEAKER_01", "timestamp": [1630.0, 1634.0], "text": " Now here's for the API gateway and some of the metrics as well."}, {"speaker": "SPEAKER_01", "timestamp": [1634.0, 1636.0], "text": " And same thing for for the serverless functions. Right."}, {"speaker": "SPEAKER_01", "timestamp": [1636.0, 1640.0], "text": " I can see the ARN of that particular serverless function, the region."}, {"speaker": "SPEAKER_01", "timestamp": [1640.0, 1649.28], "text": " And I think we've got metrics down to that. So the whole point is to bring something that's all together. And finally, you know, actually, before I show that, I also did mention"}, {"speaker": "SPEAKER_01", "timestamp": [1649.28, 1654.88], "text": " traces. And let me actually share this screen because I think I'm not sharing that."}, {"speaker": "SPEAKER_00", "timestamp": [1657.44, 1663.52], "text": " I do want to show the traces before jumping on to something else. Here we go."}, {"speaker": "SPEAKER_00", "timestamp": [1661.6, 1662.92], "text": " to something else."}, {"speaker": "SPEAKER_00", "timestamp": [1663.64, 1665.42], "text": " There we go."}, {"speaker": "SPEAKER_01", "timestamp": [1668.32, 1669.78], "text": " So again, we also have our trace map view that we just recently announced."}, {"speaker": "SPEAKER_01", "timestamp": [1669.94, 1671.72], "text": " And so when you're leveraging, as we mentioned,"}, {"speaker": "SPEAKER_01", "timestamp": [1671.84, 1672.66], "text": " distributed tracing,"}, {"speaker": "SPEAKER_01", "timestamp": [1673.42, 1675.12], "text": " we can collect all that data, again,"}, {"speaker": "SPEAKER_01", "timestamp": [1675.12, 1676.32], "text": " on a single space."}, {"speaker": "SPEAKER_01", "timestamp": [1677.36, 1679.74], "text": " And now what we're doing is we're collecting"}, {"speaker": "SPEAKER_01", "timestamp": [1679.74, 1681.24], "text": " the individual traces."}, {"speaker": "SPEAKER_01", "timestamp": [1681.44, 1683.28], "text": " And actually, we're doing something pretty cool,"}, {"speaker": "SPEAKER_01", "timestamp": [1683.36, 1685.56], "text": " which is what we call the trace map"}, {"speaker": "SPEAKER_01", "timestamp": [1685.56, 1688.44], "text": " and identification of these trace paths."}, {"speaker": "SPEAKER_01", "timestamp": [1689.42, 1689.98], "text": " Sorry, Alok."}, {"speaker": "SPEAKER_00", "timestamp": [1690.42, 1691.76], "text": " I don't see your screen."}, {"speaker": "SPEAKER_03", "timestamp": [1692.28, 1693.1], "text": " Oh, you don't see my screen?"}, {"speaker": "SPEAKER_00", "timestamp": [1693.3, 1693.66], "text": " Sorry about that."}, {"speaker": "SPEAKER_03", "timestamp": [1693.66, 1694.28], "text": " Can you share?"}, {"speaker": "SPEAKER_00", "timestamp": [1694.42, 1696.62], "text": " Because I don't think they know what you mean by the trace."}, {"speaker": "SPEAKER_00", "timestamp": [1696.9, 1697.84], "text": " Okay, great."}, {"speaker": "SPEAKER_00", "timestamp": [1698.62, 1698.84], "text": " Okay."}, {"speaker": "SPEAKER_01", "timestamp": [1699.46, 1700.84], "text": " And I did see one thing."}, {"speaker": "SPEAKER_01", "timestamp": [1700.9, 1703.1], "text": " If you are able to bump up the text just a little bit,"}, {"speaker": "SPEAKER_01", "timestamp": [1703.34, 1706.4], "text": " I saw a couple of comments about that as well."}, {"speaker": "SPEAKER_00", "timestamp": [1707.4, 1711.12], "text": " Sure. Is this, hopefully this is a little bit better."}, {"speaker": "SPEAKER_01", "timestamp": [1713.22, 1714.92], "text": " Yeah, I think, I think that should be good."}, {"speaker": "SPEAKER_01", "timestamp": [1715.32, 1715.58], "text": " Okay."}, {"speaker": "SPEAKER_00", "timestamp": [1715.58, 1716.42], "text": " Thank you for calling that up."}, {"speaker": "SPEAKER_00", "timestamp": [1717.2, 1719.68], "text": " Gotcha. Let me know if there's still visibility issues,"}, {"speaker": "SPEAKER_00", "timestamp": [1719.76, 1721.14], "text": " but I've bumped it up just a little bit."}, {"speaker": "SPEAKER_00", "timestamp": [1722.26, 1723.4], "text": " Yeah. So again, we're just,"}, {"speaker": "SPEAKER_00", "timestamp": [1723.52, 1725.08], "text": " I'm just showing off the tracing capabilities."}, {"speaker": "SPEAKER_00", "timestamp": [1725.2, 1727.26], "text": " Again, just bringing everything all together in a single place."}, {"speaker": "SPEAKER_00", "timestamp": [1727.58, 1730.72], "text": " You can see here this trace map showing the different interactions"}, {"speaker": "SPEAKER_00", "timestamp": [1730.72, 1733.66], "text": " from the front end to the ad service to the product catalog service."}, {"speaker": "SPEAKER_00", "timestamp": [1734.96, 1738.12], "text": " But one of the really cool things that is kind of unique"}, {"speaker": "SPEAKER_00", "timestamp": [1738.12, 1742.9], "text": " that we've been able to develop is identifying, you know,"}, {"speaker": "SPEAKER_00", "timestamp": [1743.0, 1746.04], "text": " a lot of times in tracing, you'll get transaction"}, {"speaker": "SPEAKER_00", "timestamp": [1746.04, 1755.02], "text": " identifications. Oh, I'm seeing that. Hopefully, hopefully, this is a little bit better. I think"}, {"speaker": "SPEAKER_00", "timestamp": [1755.02, 1763.12], "text": " I've hit the limit of my of my zooming in capabilities. Sorry, guys, I always was a"}, {"speaker": "SPEAKER_00", "timestamp": [1763.12, 1766.4], "text": " little bit bigger. Hopefully, this is some sort of a mobile for you."}, {"speaker": "SPEAKER_00", "timestamp": [1767.28, 1768.7], "text": " Okay, so we've got the traces."}, {"speaker": "SPEAKER_00", "timestamp": [1768.86, 1770.34], "text": " We've got the trace maps."}, {"speaker": "SPEAKER_00", "timestamp": [1772.62, 1777.94], "text": " And, oh, it looks like I'm getting some too much noise on my machine."}, {"speaker": "SPEAKER_00", "timestamp": [1778.08, 1778.98], "text": " So, sorry about that."}, {"speaker": "SPEAKER_00", "timestamp": [1779.0, 1779.94], "text": " I'm seeing that in the chat."}, {"speaker": "SPEAKER_00", "timestamp": [1781.24, 1783.74], "text": " Hopefully, it turned off the notification sounds here."}, {"speaker": "SPEAKER_00", "timestamp": [1783.84, 1785.68], "text": " Hopefully, that will stop interrupting. Okay, so we've got the notification sounds here. Hopefully that'll stop interrupting."}, {"speaker": "SPEAKER_00", "timestamp": [1785.68, 1791.76], "text": " Okay, so we've got the trace map view, but we're also discovering what we call the trace"}, {"speaker": "SPEAKER_00", "timestamp": [1791.76, 1799.38], "text": " paths. So these trace paths are not just sorry, guys, give me just one second. I'm trying"}, {"speaker": "SPEAKER_03", "timestamp": [1799.38, 1800.38], "text": " to..."}, {"speaker": "SPEAKER_00", "timestamp": [1800.38, 1802.38], "text": " We're still on Slack, guys. That's why."}, {"speaker": "SPEAKER_03", "timestamp": [1802.38, 1803.38], "text": " Yeah, there's..."}, {"speaker": "SPEAKER_00", "timestamp": [1803.38, 1805.28], "text": " Taylor, you know how this goes. on slack guys that's why yeah there's uh"}, {"speaker": "SPEAKER_03", "timestamp": [1808.8, 1812.08], "text": " taylor you know how this goes oh absolutely absolutely"}, {"speaker": "SPEAKER_01", "timestamp": [1816.56, 1820.48], "text": " i feel like as soon as anyone goes live that's there must be like a hidden button or something like that somewhere because that's that's what i started to get a lot of money we are being pinged"}, {"speaker": "SPEAKER_03", "timestamp": [1820.48, 1825.04], "text": " on that so anyway i believe i've turned off not do not disturb successfully,"}, {"speaker": "SPEAKER_02", "timestamp": [1825.04, 1831.0], "text": " which I thought I did before the call. I just shut down Slack. My apologies to everyone. Okay."}, {"speaker": "SPEAKER_03", "timestamp": [1831.48, 1838.64], "text": " So let me head back here. Okay. You know, we have auto discovery, essentially, of not only the"}, {"speaker": "SPEAKER_00", "timestamp": [1838.64, 1847.44], "text": " transactions themselves, which are used to seeing distributed tracing uh platforms but um we are also grabbing uh"}, {"speaker": "SPEAKER_01", "timestamp": [1847.44, 1854.24], "text": " that identification of the paths themselves you might have a transaction you know for one of these"}, {"speaker": "SPEAKER_02", "timestamp": [1854.24, 1859.68], "text": " uh products um that you know might be a slash checkout but you might have a different types"}, {"speaker": "SPEAKER_00", "timestamp": [1859.68, 1866.3], "text": " of checkouts for maybe um uh a uh class right maybe you're selling a class on your e-commerce site"}, {"speaker": "SPEAKER_02", "timestamp": [1866.3, 1867.38], "text": " versus a product, right?"}, {"speaker": "SPEAKER_04", "timestamp": [1867.5, 1869.82], "text": " So even though, you know, they're both called checkout,"}, {"speaker": "SPEAKER_00", "timestamp": [1869.94, 1871.02], "text": " one might go to AD service"}, {"speaker": "SPEAKER_04", "timestamp": [1871.02, 1873.38], "text": " and another one might go to this checkout service"}, {"speaker": "SPEAKER_00", "timestamp": [1873.38, 1874.58], "text": " and then product catalog service."}, {"speaker": "SPEAKER_00", "timestamp": [1874.66, 1875.8], "text": " So even though they're both named the same,"}, {"speaker": "SPEAKER_00", "timestamp": [1876.1, 1878.16], "text": " we identify those differences between them"}, {"speaker": "SPEAKER_00", "timestamp": [1878.16, 1881.72], "text": " and then also perform automated anomaly detection"}, {"speaker": "SPEAKER_00", "timestamp": [1881.72, 1884.6], "text": " and profile those transactions separately"}, {"speaker": "SPEAKER_00", "timestamp": [1884.6, 1885.84], "text": " from each other"}, {"speaker": "SPEAKER_00", "timestamp": [1885.84, 1891.36], "text": " right um so that is that you know that's some of the tracing we won't delve too too far into this"}, {"speaker": "SPEAKER_00", "timestamp": [1891.36, 1896.8], "text": " because i want to show really some of the some of the magic behind um what we can do now that"}, {"speaker": "SPEAKER_00", "timestamp": [1896.8, 1905.12], "text": " we have all that really rich open source data right so um let me stop sharing and reshare my other screen. Just give me a second here."}, {"speaker": "SPEAKER_00", "timestamp": [1912.24, 1912.72], "text": " So,"}, {"speaker": "SPEAKER_00", "timestamp": [1913.92, 1920.86], "text": " there we go. You guys should be seeing my screen pop up here in a second."}, {"speaker": "SPEAKER_00", "timestamp": [1929.96, 1937.24], "text": " up here in a second. All right. All right. So, you know, some of the things that we can do now that we have all this open source data is that we can now start doing anomaly detection, detecting"}, {"speaker": "SPEAKER_00", "timestamp": [1937.24, 1943.02], "text": " of misconfigurations, misbehaviors. You know, one of the things I actually did not show, if I go"}, {"speaker": "SPEAKER_00", "timestamp": [1943.02, 1954.54], "text": " back here really quickly, is that we're also collecting configuration data, not only at this kind of high-level metadata kind of view, but we're also showing the entire manifest."}, {"speaker": "SPEAKER_00", "timestamp": [1954.7, 1965.84], "text": " So if I click, and I'll just show what I did there, if I click on detailed view for this particular pod, right, now I'm looking at the actual manifest for this particular pod so I can look at the details of what exactly is going"}, {"speaker": "SPEAKER_01", "timestamp": [1965.84, 1970.32], "text": " on throughout without having to go inside the command line and figure out you know get a cube"}, {"speaker": "SPEAKER_01", "timestamp": [1970.32, 1976.56], "text": " ctl get pod dash oh yaml and this is uh this is way simpler and it also helps keep everything in"}, {"speaker": "SPEAKER_00", "timestamp": [1976.56, 1982.8], "text": " context and keep you inside of a single place um but now with all this really rich data and on and"}, {"speaker": "SPEAKER_00", "timestamp": [1982.8, 1995.36], "text": " knowing you know the other thing we do is we have what we call curated knowledge, because on top of all this, you do need to understand how these systems interoperate with each other and what kind of dependencies they have on each other."}, {"speaker": "SPEAKER_00", "timestamp": [1995.46, 1999.58], "text": " That's why we do build that relationship view, leveraging all the data."}, {"speaker": "SPEAKER_00", "timestamp": [2000.26, 2002.8], "text": " That's why we want to know what container is running on what pod."}, {"speaker": "SPEAKER_00", "timestamp": [2003.22, 2006.94], "text": " I'm sorry, on what node and what node is running on top of what piece of infrastructure."}, {"speaker": "SPEAKER_00", "timestamp": [2007.1, 2008.76], "text": " So we know when a piece of infrastructure is down,"}, {"speaker": "SPEAKER_00", "timestamp": [2009.1, 2012.7], "text": " we know that it's affecting the container that's hosted on it."}, {"speaker": "SPEAKER_00", "timestamp": [2013.72, 2017.88], "text": " And there's a lot of nuance and variance to the kind of problems that can arise."}, {"speaker": "SPEAKER_00", "timestamp": [2018.3, 2023.08], "text": " But having, again, this richness of this open source data, it makes it all possible."}, {"speaker": "SPEAKER_00", "timestamp": [2023.54, 2025.92], "text": " So I'll show a couple of things that we do."}, {"speaker": "SPEAKER_00", "timestamp": [2026.38, 2031.32], "text": " Here, let me find an alert. I think I was looking at this alert a little bit earlier. So I'll explain"}, {"speaker": "SPEAKER_00", "timestamp": [2031.32, 2035.26], "text": " a little bit what this is, right? So in this case, we have a deployment problem, right? On our"}, {"speaker": "SPEAKER_00", "timestamp": [2035.26, 2040.02], "text": " particular web server deployment, we're supposed to have a total of three replicas. And in this"}, {"speaker": "SPEAKER_00", "timestamp": [2040.02, 2043.14], "text": " case, and you know what, I'll bump up the text a little bit because I know that was asked before."}, {"speaker": "SPEAKER_00", "timestamp": [2043.72, 2047.72], "text": " So we're supposed to have a total of three replicas. In this case, we've only got two"}, {"speaker": "SPEAKER_00", "timestamp": [2047.72, 2052.34], "text": " available replicas, and this has been going on for a little bit. So down here, you know,"}, {"speaker": "SPEAKER_00", "timestamp": [2052.34, 2056.2], "text": " we provide some details. It's part of the shopping cart namespace. It's the web server deployment."}, {"speaker": "SPEAKER_00", "timestamp": [2056.58, 2061.4], "text": " And here's some, you know, additional kind of feel key value pair details, but we'll go to"}, {"speaker": "SPEAKER_00", "timestamp": [2061.4, 2065.28], "text": " the fun view. I know some of you guys love reading JSON, but I kind of like"}, {"speaker": "SPEAKER_00", "timestamp": [2066.16, 2073.12], "text": " the UI just a little bit more. So when I click on this analyze view, what it shows us is what we"}, {"speaker": "SPEAKER_00", "timestamp": [2073.12, 2080.24], "text": " call the contextual RCA, which is our fishbone RCA, right? So in this case, what we're showing"}, {"speaker": "SPEAKER_00", "timestamp": [2080.24, 2085.04], "text": " is we're showing failure categories across the top and bottom that are affecting"}, {"speaker": "SPEAKER_00", "timestamp": [2085.04, 2089.28], "text": " this particular deployment so again all this is being collected just through the you know"}, {"speaker": "SPEAKER_00", "timestamp": [2089.28, 2094.8], "text": " acquiring the kubernetes api and then the the relationship of the of collecting the events and"}, {"speaker": "SPEAKER_00", "timestamp": [2094.8, 2099.92], "text": " the containers and linking those all those pieces together um so we have a replica set scaling issue"}, {"speaker": "SPEAKER_00", "timestamp": [2099.92, 2106.34], "text": " right we're having a an issue scaling up an additional replica of that particular image."}, {"speaker": "SPEAKER_00", "timestamp": [2106.82, 2109.7], "text": " And now we're getting actually a back off restart as well."}, {"speaker": "SPEAKER_00", "timestamp": [2109.88, 2112.8], "text": " But this is all really associated to the startup failure."}, {"speaker": "SPEAKER_00", "timestamp": [2113.06, 2116.6], "text": " And if I click on that, what it's going to tell me is that I have an invalid image name."}, {"speaker": "SPEAKER_00", "timestamp": [2116.86, 2122.14], "text": " So obscures is spelled with one I, and it looks here like somebody spelled obscure with two I's."}, {"speaker": "SPEAKER_00", "timestamp": [2122.54, 2123.98], "text": " And so that's a bad image name."}, {"speaker": "SPEAKER_00", "timestamp": [2122.14, 2122.68], "text": " like somebody spelled obscrute with two I's."}, {"speaker": "SPEAKER_00", "timestamp": [2123.98, 2125.24], "text": " And so that's a bad image name."}, {"speaker": "SPEAKER_00", "timestamp": [2129.44, 2134.14], "text": " You know, it took us all of, what, you know, three, four seconds to figure out that one of our replicas isn't coming up because of a bad image name."}, {"speaker": "SPEAKER_00", "timestamp": [2134.2, 2136.82], "text": " So it's those kinds of things, the richness of the data that allows us"}, {"speaker": "SPEAKER_00", "timestamp": [2136.82, 2142.18], "text": " to build these really, really quick root cause analysis pieces"}, {"speaker": "SPEAKER_00", "timestamp": [2142.18, 2144.66], "text": " into something like obscripts, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2144.74, 2146.82], "text": " So, yes, you can do this from the command line."}, {"speaker": "SPEAKER_00", "timestamp": [2148.38, 2149.9], "text": " It's, you know, it's a little bit more work."}, {"speaker": "SPEAKER_00", "timestamp": [2149.98, 2153.32], "text": " It'll probably take anywhere from, I don't know, 30 seconds to a couple of minutes."}, {"speaker": "SPEAKER_00", "timestamp": [2153.32, 2156.8], "text": " But, you know, multiply this times 1,000 times 5,000."}, {"speaker": "SPEAKER_00", "timestamp": [2156.98, 2157.92], "text": " That can happen in a month."}, {"speaker": "SPEAKER_00", "timestamp": [2158.76, 2161.88], "text": " You know, that's a lot of time saved for operations teams, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2162.1, 2164.78], "text": " And you'll also notice other ones that some of these are more complex."}, {"speaker": "SPEAKER_00", "timestamp": [2168.1, 2175.88], "text": " And, you know, these are just building blocks to what I'm going to show you in a sec of these individual kind of problem detections and anomaly detections. But you'll notice other"}, {"speaker": "SPEAKER_01", "timestamp": [2175.88, 2180.76], "text": " categories. So things like a missing config map, right? If you reference a config map in your"}, {"speaker": "SPEAKER_04", "timestamp": [2180.76, 2185.44], "text": " manifest that does not exist, you're going to have a failure of your pod."}, {"speaker": "SPEAKER_01", "timestamp": [2185.54, 2190.02], "text": " So we'll highlight those things or failed volume mounts or even bad image tags. I think I might"}, {"speaker": "SPEAKER_00", "timestamp": [2190.02, 2196.64], "text": " actually have a bad image tag in here that I was looking at. It's a very, very similar scenario,"}, {"speaker": "SPEAKER_00", "timestamp": [2196.64, 2200.64], "text": " but for the cart server, if I click on analyze, yep, same kind of symptoms,"}, {"speaker": "SPEAKER_00", "timestamp": [2201.1, 2205.12], "text": " replica set scaling issues, we're having back off restarts going to a crash loop."}, {"speaker": "SPEAKER_00", "timestamp": [2205.12, 2210.48], "text": " But in this case, we have an invalid image tag. This particular image tag does not exist."}, {"speaker": "SPEAKER_00", "timestamp": [2212.16, 2217.44], "text": " Now, the other thing that I didn't go too far into, but it really is absolutely key,"}, {"speaker": "SPEAKER_00", "timestamp": [2218.08, 2227.92], "text": " is machine learning. So from all the individual services that you deploy onto your clusters,"}, {"speaker": "SPEAKER_00", "timestamp": [2228.24, 2231.64], "text": " what happens is that with that data being collected from C-Advisor"}, {"speaker": "SPEAKER_00", "timestamp": [2231.64, 2234.96], "text": " and from Nodex program, from the discovery pieces,"}, {"speaker": "SPEAKER_00", "timestamp": [2234.96, 2242.34], "text": " what we do is we create a really rich behavior model, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2242.4, 2248.9], "text": " We detect what is normal behavior for your individual services, right? So"}, {"speaker": "SPEAKER_00", "timestamp": [2248.9, 2255.5], "text": " if you are, you know, we don't just look at one or two metrics like error rates and response time,"}, {"speaker": "SPEAKER_00", "timestamp": [2255.52, 2259.74], "text": " but we look actually each one of the entities that I've shown you have their own behavior"}, {"speaker": "SPEAKER_00", "timestamp": [2259.74, 2262.28], "text": " models. And there's a bunch of others that I didn't show you as part of this demo, but things"}, {"speaker": "SPEAKER_00", "timestamp": [2262.28, 2268.84], "text": " like if you're using database like MongoDB or a JVM or an Nginx container, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2269.1, 2273.74], "text": " And then the generic containers themselves, the nodes themselves, they all have their own behavior models."}, {"speaker": "SPEAKER_00", "timestamp": [2274.4, 2278.76], "text": " And we pick up a mixture of a lot of different metrics to understand what is normal behavior."}, {"speaker": "SPEAKER_00", "timestamp": [2279.3, 2285.12], "text": " And then when we find what is abnormal, we have these types of alerts that are prefixed by ML telling us"}, {"speaker": "SPEAKER_00", "timestamp": [2285.12, 2290.56], "text": " that there is some sort of ML detected performance violation, right? So if I click on this,"}, {"speaker": "SPEAKER_00", "timestamp": [2291.52, 2295.6], "text": " in this scenario, you know, again, I'm going to get some details as to what happened, right? I"}, {"speaker": "SPEAKER_00", "timestamp": [2295.6, 2300.32], "text": " get, you know, I know when we zoom in a bit, network transmittal bytes increased by 540%"}, {"speaker": "SPEAKER_00", "timestamp": [2300.96, 2307.4], "text": " and level four bytes for the outbound traffic increase and inbound transmittal bytes decreased,"}, {"speaker": "SPEAKER_00", "timestamp": [2307.4, 2311.96], "text": " actually. So we don't only detect increases, but also abnormal decreases as well."}, {"speaker": "SPEAKER_00", "timestamp": [2311.96, 2318.6], "text": " But just like in the other scenarios, if I click on analyze, I can get a fishbone representation"}, {"speaker": "SPEAKER_00", "timestamp": [2318.6, 2326.62], "text": " of what exactly is going on with the metrics and why the ML in the first place triggered an anomaly. And so I'm going to"}, {"speaker": "SPEAKER_00", "timestamp": [2326.62, 2333.24], "text": " zoom out just one piece. Just like you saw for the Kubernetes specific deployment scenarios,"}, {"speaker": "SPEAKER_00", "timestamp": [2333.64, 2339.52], "text": " now in this phase one RCA, we're looking at a container view. This particularly the card cache"}, {"speaker": "SPEAKER_00", "timestamp": [2339.52, 2346.62], "text": " had some deviation in its metrics. And actually before looking at this,"}, {"speaker": "SPEAKER_00", "timestamp": [2346.62, 2349.26], "text": " I'm gonna go back just to the summary screen"}, {"speaker": "SPEAKER_00", "timestamp": [2349.26, 2352.48], "text": " and show you down here, if I click more details,"}, {"speaker": "SPEAKER_00", "timestamp": [2352.48, 2353.92], "text": " you know, speaking about the ML"}, {"speaker": "SPEAKER_00", "timestamp": [2353.92, 2355.08], "text": " and all the metrics we take,"}, {"speaker": "SPEAKER_00", "timestamp": [2355.08, 2356.32], "text": " these are all the different metrics"}, {"speaker": "SPEAKER_00", "timestamp": [2356.32, 2358.38], "text": " for just the generic container model"}, {"speaker": "SPEAKER_00", "timestamp": [2358.38, 2359.94], "text": " that we're looking at, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2359.94, 2362.16], "text": " So again, it's not just one or two or three metrics."}, {"speaker": "SPEAKER_00", "timestamp": [2362.16, 2363.78], "text": " We're looking at transmittal bytes,"}, {"speaker": "SPEAKER_00", "timestamp": [2363.78, 2366.98], "text": " packets in, packets out, memory failures, CPU utilization."}, {"speaker": "SPEAKER_00", "timestamp": [2367.34, 2373.1], "text": " All this data, particularly for the container, is, again, provided by C-Advisor, an open source tool, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2373.48, 2382.04], "text": " So, again, going back to the analyze button, now we're seeing the actual pieces that actually triggered the ML."}, {"speaker": "SPEAKER_00", "timestamp": [2382.46, 2385.64], "text": " And so now you'll notice that the fishbone has changed from our startup failure."}, {"speaker": "SPEAKER_00", "timestamp": [2386.12, 2389.8], "text": " Now we're showing memory and file system and CPU."}, {"speaker": "SPEAKER_00", "timestamp": [2390.32, 2392.08], "text": " And so right away, we'll show you in red."}, {"speaker": "SPEAKER_00", "timestamp": [2392.12, 2394.16], "text": " You don't have to go and look at a chart for this specific thing."}, {"speaker": "SPEAKER_00", "timestamp": [2394.28, 2395.54], "text": " It's here, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2395.58, 2399.04], "text": " So I'm seeing CPU utilization has increased by close to 50%."}, {"speaker": "SPEAKER_00", "timestamp": [2399.04, 2403.12], "text": " I'm looking at demand, which is incoming requests."}, {"speaker": "SPEAKER_00", "timestamp": [2403.12, 2406.08], "text": " The response time has increased by over 1,700%."}, {"speaker": "SPEAKER_00", "timestamp": [2406.08, 2409.08], "text": " I'm looking at outbound supply side."}, {"speaker": "SPEAKER_00", "timestamp": [2409.5, 2413.86], "text": " Response time has increased by 2,200% for outbound requests from PartCash."}, {"speaker": "SPEAKER_00", "timestamp": [2414.4, 2420.24], "text": " And then not only that, our response size has increased from one to close to eight megs, right?"}, {"speaker": "SPEAKER_01", "timestamp": [2420.3, 2424.5], "text": " And then bringing in the Kubernetes layer, it's this whole image change, right?"}, {"speaker": "SPEAKER_01", "timestamp": [2424.5, 2431.22], "text": " So again, bringing in the data from the Kubernetes API, I can see that I've got a recent image change that's likely contributing to this failure."}, {"speaker": "SPEAKER_00", "timestamp": [2432.12, 2440.32], "text": " Now, again, I'm glossing over a few details because in the interest of time, I want to show you guys how we bring, you know, a couple of these things together."}, {"speaker": "SPEAKER_00", "timestamp": [2440.44, 2448.66], "text": " This is, you know, an ML alert. And by the way, you can see this automatically chart those important metrics down here. So you can see their behavior during the time of the anomaly."}, {"speaker": "SPEAKER_00", "timestamp": [2449.04, 2455.14], "text": " And as mentioned, you know, you can drill down into any logs that might be coming in. Actually,"}, {"speaker": "SPEAKER_00", "timestamp": [2455.22, 2460.38], "text": " I should probably show that. I don't think I did. So here's another example of an anomaly database"}, {"speaker": "SPEAKER_00", "timestamp": [2460.38, 2470.04], "text": " server. And you'll notice here that you have different contextual access, right? Application state, I'll show that. We have a time travel capability where with all this data,"}, {"speaker": "SPEAKER_00", "timestamp": [2470.22, 2475.44], "text": " the metric data from Prometheus, the log data from Fluent, from Loki, the trace data, all this,"}, {"speaker": "SPEAKER_00", "timestamp": [2475.56, 2480.76], "text": " you know, we build that real-time map that you guys saw and all the configuration data."}, {"speaker": "SPEAKER_00", "timestamp": [2481.16, 2484.26], "text": " We take snapshots every five minutes, and I'll show you guys that in a second, but you can go"}, {"speaker": "SPEAKER_00", "timestamp": [2484.26, 2488.9], "text": " back in time at the time how your system was configured during the time of this particular"}, {"speaker": "SPEAKER_00", "timestamp": [2488.9, 2494.18], "text": " anomaly. In this case, this goes back a day. So if I click that, it'll take me back into one day"}, {"speaker": "SPEAKER_00", "timestamp": [2494.18, 2498.94], "text": " before and show me the entire config of my entire state at that time. But we'll go into that in a"}, {"speaker": "SPEAKER_00", "timestamp": [2498.94, 2503.34], "text": " second. I can click on metrics to understand the metrics for that database server or any events"}, {"speaker": "SPEAKER_00", "timestamp": [2503.34, 2506.34], "text": " that are related to that. In this case, I want to show logs. So if I just click on metrics to understand the metrics for that database server or any events that are related to that. In this case, I wanna show logs."}, {"speaker": "SPEAKER_00", "timestamp": [2506.34, 2508.94], "text": " So if I just click on the pod or the container logs,"}, {"speaker": "SPEAKER_00", "timestamp": [2509.9, 2511.54], "text": " oh, maybe it wasn't logging there,"}, {"speaker": "SPEAKER_00", "timestamp": [2511.54, 2514.62], "text": " but I do wanna show that we have contextual access"}, {"speaker": "SPEAKER_00", "timestamp": [2514.62, 2515.62], "text": " to the logs actually."}, {"speaker": "SPEAKER_00", "timestamp": [2515.62, 2518.42], "text": " Let me find, I just wanna show"}, {"speaker": "SPEAKER_00", "timestamp": [2518.42, 2520.46], "text": " because I think we did not actually show logs."}, {"speaker": "SPEAKER_00", "timestamp": [2520.46, 2524.38], "text": " Let me see, I think maybe note exporter will be logged."}, {"speaker": "SPEAKER_00", "timestamp": [2524.38, 2525.24], "text": " Sorry."}, {"speaker": "SPEAKER_01", "timestamp": [2525.24, 2528.32], "text": " And Cesar, I saw a couple questions come in on that front."}, {"speaker": "SPEAKER_01", "timestamp": [2528.32, 2532.7], "text": " One question was, does OpsCruise enable custom metrics?"}, {"speaker": "SPEAKER_00", "timestamp": [2532.7, 2534.24], "text": " Does OpsCruise enable custom metrics?"}, {"speaker": "SPEAKER_00", "timestamp": [2534.24, 2535.24], "text": " Yes."}, {"speaker": "SPEAKER_00", "timestamp": [2535.24, 2540.8], "text": " Basically, it's any data that's being exposed to Prometheus."}, {"speaker": "SPEAKER_00", "timestamp": [2540.8, 2545.76], "text": " So as long as data is being exposed to Prometheus, that data can be brought into OpsCrews."}, {"speaker": "SPEAKER_00", "timestamp": [2545.76, 2549.24], "text": " We're just leveraging Prometheus as a metric ingestion point."}, {"speaker": "SPEAKER_01", "timestamp": [2549.24, 2550.24], "text": " Awesome."}, {"speaker": "SPEAKER_01", "timestamp": [2550.24, 2554.96], "text": " And then one other question was, do the ML alerts wait for a particular size of training"}, {"speaker": "SPEAKER_01", "timestamp": [2554.96, 2557.62], "text": " data before alerting on that front?"}, {"speaker": "SPEAKER_03", "timestamp": [2557.62, 2558.9], "text": " So I can answer that."}, {"speaker": "SPEAKER_01", "timestamp": [2558.9, 2564.26], "text": " So typically it depends on, our default is about one day, but we can speed it up so we"}, {"speaker": "SPEAKER_01", "timestamp": [2564.26, 2566.94], "text": " can learn. And the only reason I'll say that is"}, {"speaker": "SPEAKER_01", "timestamp": [2566.94, 2569.46], "text": " because let's say there was hardly any activity"}, {"speaker": "SPEAKER_03", "timestamp": [2569.46, 2570.76], "text": " on a weekend, he deployed it."}, {"speaker": "SPEAKER_03", "timestamp": [2570.76, 2572.56], "text": " You're not gonna see a lot of activity to profile it,"}, {"speaker": "SPEAKER_03", "timestamp": [2572.56, 2574.36], "text": " but the next day it starts increasing."}, {"speaker": "SPEAKER_03", "timestamp": [2574.36, 2576.42], "text": " So over time, essentially we update that,"}, {"speaker": "SPEAKER_03", "timestamp": [2576.42, 2579.98], "text": " but our default is 24 hours can be even less."}, {"speaker": "SPEAKER_03", "timestamp": [2579.98, 2581.16], "text": " Gonna make it even a few hours."}, {"speaker": "SPEAKER_03", "timestamp": [2581.16, 2583.44], "text": " Just have enough data to get an initial profile"}, {"speaker": "SPEAKER_01", "timestamp": [2583.44, 2592.32], "text": " and then we continuously update that. cool thank you no the users don't have to do anything i wish i could learn that fast"}, {"speaker": "SPEAKER_03", "timestamp": [2592.32, 2596.72], "text": " yeah accelerate like one day is good but yeah if i could learn like this you saw that generic"}, {"speaker": "SPEAKER_01", "timestamp": [2596.72, 2601.92], "text": " container example that says there's about 30 metrics and you have no way of knowing because"}, {"speaker": "SPEAKER_03", "timestamp": [2601.92, 2609.92], "text": " there's some calls being made as a problem versus the memory failure suddenly increases so there is no way a person can do that that's what"}, {"speaker": "SPEAKER_00", "timestamp": [2609.92, 2616.96], "text": " the beauty of the using the ml to get unknown numbers keep going yeah absolutely um so the"}, {"speaker": "SPEAKER_03", "timestamp": [2616.96, 2620.48], "text": " the thing i wanted to show is logs because i absolutely did not not show that even though"}, {"speaker": "SPEAKER_00", "timestamp": [2620.48, 2625.06], "text": " it's super important but um any anything that logging, right, we pick that up from your"}, {"speaker": "SPEAKER_03", "timestamp": [2625.06, 2631.84], "text": " standard out. But if you click on any, whether it's an anomaly, and this is showing the pod,"}, {"speaker": "SPEAKER_00", "timestamp": [2631.96, 2636.44], "text": " right, I have a pod open. And from its, this is what we call the quick view, right? From its quick"}, {"speaker": "SPEAKER_01", "timestamp": [2636.44, 2641.6], "text": " view, one of the links you have is for logs, right? So I can just click on logs. And that"}, {"speaker": "SPEAKER_01", "timestamp": [2641.6, 2644.78], "text": " takes me straight into the logs for that particular service. Now, this is pretty,"}, {"speaker": "SPEAKER_01", "timestamp": [2645.04, 2651.12], "text": " click on logs and that takes me straight into the logs for that particular service now this is pretty um you know uh static logs here but i can you know we it is searchable right so i can look for"}, {"speaker": "SPEAKER_01", "timestamp": [2651.12, 2658.4], "text": " requests for example or conversion yeah the idea is to contextually link it to the problem correct"}, {"speaker": "SPEAKER_03", "timestamp": [2658.4, 2664.16], "text": " yes uh so you know depending where we get it from yeah correct yeah i i think i don't have a problem"}, {"speaker": "SPEAKER_00", "timestamp": [2664.16, 2669.44], "text": " here that has logs right now but um but we surface that as well. So if you're having an anomaly, you can go straight"}, {"speaker": "SPEAKER_00", "timestamp": [2669.44, 2674.08], "text": " into the logs for when they're active and that'll show. Now, what I do want to show with all this"}, {"speaker": "SPEAKER_00", "timestamp": [2674.08, 2681.36], "text": " really put together is I'm going to show you an alert, right? So we have, I showed you guys,"}, {"speaker": "SPEAKER_01", "timestamp": [2681.36, 2690.4], "text": " you know, how we collect all the different data, the architecture, right, again, we're leveraging just purely open source tools here to collect the data from, you know,"}, {"speaker": "SPEAKER_04", "timestamp": [2690.4, 2694.88], "text": " whether it's VMs, or whether it's Kubernetes, et cetera, from the application level, MongoDB"}, {"speaker": "SPEAKER_04", "timestamp": [2694.88, 2699.52], "text": " exporters, or Nginx exporters, as well as the traces, whatever, open telemetry, compatible"}, {"speaker": "SPEAKER_00", "timestamp": [2699.52, 2707.8], "text": " libraries, all basically built on open source. But now, right, what we have here is, again, I also showed"}, {"speaker": "SPEAKER_00", "timestamp": [2707.8, 2711.88], "text": " you the anomalies on how we like specific Kubernetes detection, and then I showed you the ML,"}, {"speaker": "SPEAKER_00", "timestamp": [2712.28, 2716.54], "text": " how we automatically detect performance deviation, and again, lots of different metrics."}, {"speaker": "SPEAKER_00", "timestamp": [2716.98, 2721.68], "text": " So in this scenario, we're kind of tying everything together, right? So I have a response time SLO"}, {"speaker": "SPEAKER_00", "timestamp": [2721.68, 2725.3], "text": " breach on my Nginx service. So I'm going to click on that."}, {"speaker": "SPEAKER_00", "timestamp": [2726.4, 2728.02], "text": " And again, here's some details, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2728.06, 2729.5], "text": " I have an SLO of five seconds."}, {"speaker": "SPEAKER_00", "timestamp": [2729.86, 2731.68], "text": " My response time is at over 15 seconds."}, {"speaker": "SPEAKER_00", "timestamp": [2732.0, 2734.0], "text": " So I want to see what's going on, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2734.02, 2737.92], "text": " If I click on Analyze, what this is going to do, and I'm going to close this."}, {"speaker": "SPEAKER_00", "timestamp": [2737.92, 2739.0], "text": " We'll come back to this summary in a second."}, {"speaker": "SPEAKER_00", "timestamp": [2739.04, 2739.82], "text": " I'm going to close that piece."}, {"speaker": "SPEAKER_00", "timestamp": [2740.54, 2748.14], "text": " What this is doing is now we're showing a slice of the actual app map that we were looking at earlier."}, {"speaker": "SPEAKER_00", "timestamp": [2748.28, 2754.72], "text": " But now it's focused on the timeframe and in the context of this particular anomaly."}, {"speaker": "SPEAKER_00", "timestamp": [2755.06, 2757.3], "text": " So your route is essentially here."}, {"speaker": "SPEAKER_00", "timestamp": [2757.48, 2758.96], "text": " At NGINX, you're seeing a slowdown."}, {"speaker": "SPEAKER_00", "timestamp": [2759.46, 2762.5], "text": " But we've also identified what downstream services are involved."}, {"speaker": "SPEAKER_00", "timestamp": [2762.82, 2769.94], "text": " So we have NGINX itself, right? So this is the Kubernetes service, the pod, the container, and same thing, service, pod, container. For web"}, {"speaker": "SPEAKER_00", "timestamp": [2769.94, 2775.1], "text": " server, Redis service, Redis pod, Redis container. They've got a cart server, service, pod, and"}, {"speaker": "SPEAKER_00", "timestamp": [2775.1, 2778.8], "text": " container. You'll notice immediately in the red, we've highlighted. So we're doing fault domain"}, {"speaker": "SPEAKER_00", "timestamp": [2778.8, 2783.3], "text": " isolation as well. You don't have to call the NGINX microservice team, whoever's managing that."}, {"speaker": "SPEAKER_00", "timestamp": [2783.36, 2786.42], "text": " You don't have to call the web server microservice team, whoever's managing that."}, {"speaker": "SPEAKER_00", "timestamp": [2786.58, 2788.52], "text": " Could be the same team, could be a couple of different teams."}, {"speaker": "SPEAKER_00", "timestamp": [2788.76, 2789.7], "text": " You don't have to reach out to them."}, {"speaker": "SPEAKER_00", "timestamp": [2789.74, 2792.64], "text": " You don't have to go inside your tools and look at the metrics for these parts."}, {"speaker": "SPEAKER_00", "timestamp": [2793.02, 2795.34], "text": " We're showing you they're healthy, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2795.44, 2805.12], "text": " So what the data has shown us from the data we've collected from these containers, as well as from the network data and the configuration data and combined with RML, that intelligent layer of"}, {"speaker": "SPEAKER_00", "timestamp": [2805.12, 2811.76], "text": " the operations is we've highlighted the red pieces, right? So our container for Redis is red,"}, {"speaker": "SPEAKER_00", "timestamp": [2812.08, 2816.12], "text": " our card server service is red, and so are our podding containers. So we'll kind of take this"}, {"speaker": "SPEAKER_00", "timestamp": [2816.12, 2820.14], "text": " in the chain and see what's going on. So we've identified we have an SLO failure up here."}, {"speaker": "SPEAKER_00", "timestamp": [2820.42, 2824.66], "text": " We're responding really, really slow. Now, if I click on the next piece in the chain, I'm showing"}, {"speaker": "SPEAKER_00", "timestamp": [2824.66, 2826.8], "text": " that Redis container is problematic."}, {"speaker": "SPEAKER_00", "timestamp": [2826.8, 2828.92], "text": " If I click on that, what it's going to do,"}, {"speaker": "SPEAKER_00", "timestamp": [2828.92, 2830.62], "text": " it's going to show us this is a separate,"}, {"speaker": "SPEAKER_00", "timestamp": [2830.62, 2832.92], "text": " technically a separate anomaly from the Nginx one,"}, {"speaker": "SPEAKER_00", "timestamp": [2832.92, 2836.16], "text": " but the ML has detected that this is very much related."}, {"speaker": "SPEAKER_00", "timestamp": [2836.16, 2837.92], "text": " You'll see a few different failures."}, {"speaker": "SPEAKER_00", "timestamp": [2837.92, 2840.0], "text": " You'll see that we're getting"}, {"speaker": "SPEAKER_00", "timestamp": [2840.0, 2842.78], "text": " an increase in throttling on the CPU."}, {"speaker": "SPEAKER_00", "timestamp": [2842.78, 2846.1], "text": " The user second solders spent on the CPU has increased by"}, {"speaker": "SPEAKER_00", "timestamp": [2846.1, 2851.5], "text": " about 10%. But really interesting actually here is you'll notice the response time normally is at"}, {"speaker": "SPEAKER_00", "timestamp": [2851.5, 2857.34], "text": " 2.94 milliseconds. Right now we're at over two seconds. This was automatically detected. And"}, {"speaker": "SPEAKER_00", "timestamp": [2857.34, 2862.14], "text": " then also super important is our error rate, right? Basically you usually have zero errors."}, {"speaker": "SPEAKER_00", "timestamp": [2862.34, 2865.34], "text": " Right now our error rate has jumped up sorry to 36"}, {"speaker": "SPEAKER_00", "timestamp": [2865.34, 2869.12], "text": " out of every single basically every single request has essentially got into"}, {"speaker": "SPEAKER_00", "timestamp": [2869.12, 2873.5], "text": " an error mode so something is wrong so we're gonna we're gonna go back here and"}, {"speaker": "SPEAKER_00", "timestamp": [2873.5, 2878.0], "text": " and just see what our RCA is pointing at so Redis is calling card server now if I"}, {"speaker": "SPEAKER_00", "timestamp": [2878.0, 2882.32], "text": " click on card server I'm gonna see the alert very very clear this service the"}, {"speaker": "SPEAKER_00", "timestamp": [2882.32, 2890.4], "text": " card server doesn't have any pod to serve requests. That's a very, very clear indicator that obviously Redis is experiencing a bunch of"}, {"speaker": "SPEAKER_00", "timestamp": [2890.4, 2894.74], "text": " response time failures and now error rate failures because there are no requests behind"}, {"speaker": "SPEAKER_00", "timestamp": [2894.74, 2900.26], "text": " this Kubernetes pod, sorry, no pods behind this service to serve any requests. And to look into"}, {"speaker": "SPEAKER_00", "timestamp": [2900.26, 2904.86], "text": " just a little bit more detail, if I click on this particular pod now on the cart server to see if I"}, {"speaker": "SPEAKER_00", "timestamp": [2904.86, 2907.18], "text": " can glean what's going on, it looks like I'm having a back off restart. detail if I click on this particular pod now on the cart server to see if I can glean what's going on looks like I'm having a back"}, {"speaker": "SPEAKER_00", "timestamp": [2907.18, 2911.68], "text": " off restart and if I look at the details of that alert it'll actually show me a"}, {"speaker": "SPEAKER_00", "timestamp": [2911.68, 2914.52], "text": " little bit more detail again these are all separate but linked together"}, {"speaker": "SPEAKER_00", "timestamp": [2914.52, 2919.28], "text": " problems if I click on the analyze tab now it's going to show me the real root"}, {"speaker": "SPEAKER_00", "timestamp": [2919.28, 2923.28], "text": " cons right we've got an invalid image name as we talked about earlier so this"}, {"speaker": "SPEAKER_00", "timestamp": [2923.28, 2931.12], "text": " broken image name with two eyes in India India, this is, I'll zoom in a little bit so you can see that."}, {"speaker": "SPEAKER_00", "timestamp": [2932.08, 2933.02], "text": " It doesn't look much."}, {"speaker": "SPEAKER_00", "timestamp": [2934.6, 2938.4], "text": " But you can see that Obscures India here showing two I's, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2938.4, 2943.96], "text": " So we have this invalid image name, and that's really, you know, the root cause of the issue."}, {"speaker": "SPEAKER_00", "timestamp": [2944.12, 2946.72], "text": " And it's all shown right here in a"}, {"speaker": "SPEAKER_00", "timestamp": [2946.72, 2953.22], "text": " matter of seconds. Our engine is experiencing a response time slowdown. Redis is saying that"}, {"speaker": "SPEAKER_00", "timestamp": [2953.22, 2958.44], "text": " we've got an increase in response time and errors. Card server is saying, well, I don't have any pods"}, {"speaker": "SPEAKER_00", "timestamp": [2958.44, 2963.1], "text": " to serve. And the pod itself is saying, well, I can't start because somebody gave me a bad image"}, {"speaker": "SPEAKER_00", "timestamp": [2963.1, 2967.62], "text": " name. All this in a matter of, you know, about 20 seconds, right?"}, {"speaker": "SPEAKER_00", "timestamp": [2967.66, 2970.46], "text": " It took me obviously close to a minute, minute and a half to explain."}, {"speaker": "SPEAKER_00", "timestamp": [2970.92, 2979.42], "text": " But all this understanding the Kubernetes level up to the application level and how they are affecting each other is all powered by these open source tools."}, {"speaker": "SPEAKER_00", "timestamp": [2980.0, 2983.96], "text": " Plus, you know, the intelligent layer on top, which is, in my opinion, pretty darn cool."}, {"speaker": "SPEAKER_00", "timestamp": [2984.96, 2985.0], "text": " In the interest of time, there are things I wanted to show. I did want to show time travel, which is, in my opinion, pretty darn cool."}, {"speaker": "SPEAKER_00", "timestamp": [2986.86, 2986.88], "text": " In the interest of time, there are things I wanted to show."}, {"speaker": "SPEAKER_00", "timestamp": [2989.48, 2989.52], "text": " I did want to show time travel, but I think we're pretty close out of time."}, {"speaker": "SPEAKER_00", "timestamp": [2990.66, 2991.26], "text": " I want to open it up for questions."}, {"speaker": "SPEAKER_00", "timestamp": [2993.52, 2994.2], "text": " So with that, I'll turn it back to you, Alok and Tyler."}, {"speaker": "SPEAKER_03", "timestamp": [2996.68, 3000.66], "text": " Yeah, so I think we should go back to Q&A. So if I were just to summarize, kind of back to the premise we started with, right?"}, {"speaker": "SPEAKER_00", "timestamp": [3001.34, 3007.36], "text": " In order to really help ops in this new cloud native microservice environment and"}, {"speaker": "SPEAKER_03", "timestamp": [3007.36, 3012.48], "text": " kubernetes you know we have no longer to worry about where the data is coming from all the"}, {"speaker": "SPEAKER_03", "timestamp": [3012.48, 3017.44], "text": " telemetry is there the idea is to build it but really what observer already has to do to have"}, {"speaker": "SPEAKER_03", "timestamp": [3017.44, 3022.72], "text": " this intelligence is able to understand the full context of the application of course everything"}, {"speaker": "SPEAKER_03", "timestamp": [3022.72, 3025.68], "text": " across all dependency track that users should"}, {"speaker": "SPEAKER_03", "timestamp": [3025.68, 3029.84], "text": " not have to do that so that's what we need to build in and then understand the application"}, {"speaker": "SPEAKER_03", "timestamp": [3029.84, 3034.24], "text": " profile the behavior so they don't have to worry about how to detect problem setting thresholds"}, {"speaker": "SPEAKER_03", "timestamp": [3034.24, 3039.28], "text": " we want to take that off the table and then contextually analyze everything because now"}, {"speaker": "SPEAKER_03", "timestamp": [3039.84, 3044.64], "text": " we have rich data in this whole distributed systems that whether it's an infrastructure"}, {"speaker": "SPEAKER_03", "timestamp": [3044.64, 3045.94], "text": " or kubernetes related or down to the application they all link together and don't you don't need We have rich data in this whole distributed systems, and whether it's in infrastructure or Kubernetes related"}, {"speaker": "SPEAKER_03", "timestamp": [3045.94, 3048.76], "text": " or down to the application, they all link together."}, {"speaker": "SPEAKER_03", "timestamp": [3048.76, 3051.76], "text": " And you don't need six different folks"}, {"speaker": "SPEAKER_03", "timestamp": [3051.76, 3055.12], "text": " looking at traces, logs, events, alerts to do that."}, {"speaker": "SPEAKER_03", "timestamp": [3055.12, 3058.78], "text": " That's the role of observability in this new world."}, {"speaker": "SPEAKER_03", "timestamp": [3058.78, 3062.26], "text": " And thanks to open telemetry and open source,"}, {"speaker": "SPEAKER_02", "timestamp": [3062.26, 3063.76], "text": " it is possible to do that."}, {"speaker": "SPEAKER_02", "timestamp": [3063.76, 3065.62], "text": " So think of us as the proof point."}, {"speaker": "SPEAKER_03", "timestamp": [3066.06, 3069.96], "text": " You don't need to worry about running multiple siloed tools"}, {"speaker": "SPEAKER_03", "timestamp": [3069.96, 3073.38], "text": " to really build that intelligence and reduce the amount of effort needed."}, {"speaker": "SPEAKER_03", "timestamp": [3073.88, 3074.62], "text": " I'll pause there."}, {"speaker": "SPEAKER_03", "timestamp": [3074.94, 3076.94], "text": " That was the whole point of, you know, guys,"}, {"speaker": "SPEAKER_03", "timestamp": [3076.94, 3081.92], "text": " take advantage of the open telemetry and the open source tooling"}, {"speaker": "SPEAKER_03", "timestamp": [3081.92, 3083.42], "text": " that CNC has been helping."}, {"speaker": "SPEAKER_03", "timestamp": [3083.76, 3085.28], "text": " We are firm believers in that,"}, {"speaker": "SPEAKER_03", "timestamp": [3085.4, 3087.18], "text": " and we hope you can leverage it too."}, {"speaker": "SPEAKER_01", "timestamp": [3088.44, 3090.44], "text": " Awesome. Awesome. Thank you so much."}, {"speaker": "SPEAKER_01", "timestamp": [3090.7, 3092.36], "text": " I did see quite a few questions come in."}, {"speaker": "SPEAKER_01", "timestamp": [3092.4, 3093.24], "text": " I see at least three."}, {"speaker": "SPEAKER_01", "timestamp": [3093.8, 3095.86], "text": " One question, the first one from Ishmael,"}, {"speaker": "SPEAKER_01", "timestamp": [3095.86, 3098.3], "text": " was how easy is the installation?"}, {"speaker": "SPEAKER_01", "timestamp": [3100.52, 3101.7], "text": " That's a great question."}, {"speaker": "SPEAKER_00", "timestamp": [3102.3, 3104.76], "text": " Let me see if I have it in this environment here."}, {"speaker": "SPEAKER_00", "timestamp": [3108.18, 3114.28], "text": " That's a great question. Let me see if I have it in this environment here. So for a typical deployment into maybe an on-prem cluster, we leverage Helm. Right? Again, another open"}, {"speaker": "SPEAKER_00", "timestamp": [3114.28, 3119.8], "text": " source tool. So we leverage Helm. It's these commands. It's about three or four, well,"}, {"speaker": "SPEAKER_00", "timestamp": [3119.8, 3120.8], "text": " it's actually five commands."}, {"speaker": "SPEAKER_01", "timestamp": [3120.8, 3123.24], "text": " And Sarah, could you bump that up just a little bit?"}, {"speaker": "SPEAKER_00", "timestamp": [3123.24, 3130.96], "text": " Yeah, I absolutely will. Thank you. So essentially, you know, if you don't have these, if you have these existing tools,"}, {"speaker": "SPEAKER_01", "timestamp": [3130.96, 3135.0], "text": " because a lot of people, as I mentioned, you know, these are the essentially the de facto"}, {"speaker": "SPEAKER_00", "timestamp": [3135.0, 3138.96], "text": " standard for open source monitoring and all these modern environments. Most of the people we run"}, {"speaker": "SPEAKER_00", "timestamp": [3138.96, 3143.2], "text": " into already have these tools. So it's actually a little bit simpler. But if you don't have these"}, {"speaker": "SPEAKER_00", "timestamp": [3143.2, 3153.0], "text": " tools, we absolutely, you know, this, I think it's this last command that will deploy, you know, all the open source tools if you don't have them underneath already."}, {"speaker": "SPEAKER_00", "timestamp": [3153.0, 3155.0], "text": " Essentially, it's through Helm, right?"}, {"speaker": "SPEAKER_00", "timestamp": [3155.0, 3161.0], "text": " These commands, you know, these five commands, and that gets you from a greenfield cluster to up and running."}, {"speaker": "SPEAKER_00", "timestamp": [3161.0, 3168.62], "text": " Literally, I mean, copy and paste, I mean, you're up and running in about, I don't know, three minutes, four minutes, and you have the entire environment that I showed."}, {"speaker": "SPEAKER_00", "timestamp": [3168.72, 3172.98], "text": " The only thing that's not available right off the bat is the ML because, you know, again,"}, {"speaker": "SPEAKER_00", "timestamp": [3173.02, 3177.16], "text": " it takes a couple of, anywhere from a, I've seen ML alerts come back in a couple of hours"}, {"speaker": "SPEAKER_00", "timestamp": [3177.16, 3183.06], "text": " to, you know, 24 hours. This is usually that sweet spot. But everything else that you saw"}, {"speaker": "SPEAKER_00", "timestamp": [3183.06, 3186.24], "text": " within, you know, three to five minutes of deploying, you're getting"}, {"speaker": "SPEAKER_00", "timestamp": [3186.24, 3187.24], "text": " all that data."}, {"speaker": "SPEAKER_00", "timestamp": [3187.24, 3189.24], "text": " So this is how simple it is."}, {"speaker": "SPEAKER_01", "timestamp": [3189.24, 3190.24], "text": " Awesome."}, {"speaker": "SPEAKER_01", "timestamp": [3190.24, 3191.24], "text": " Awesome."}, {"speaker": "SPEAKER_01", "timestamp": [3191.24, 3192.24], "text": " Thank you so much."}, {"speaker": "SPEAKER_01", "timestamp": [3192.24, 3196.8], "text": " The next question I saw was, is it free or what levels?"}, {"speaker": "SPEAKER_01", "timestamp": [3196.8, 3197.8], "text": " How does it work?"}, {"speaker": "SPEAKER_01", "timestamp": [3197.8, 3200.1], "text": " Is it a SaaS or is it something you can host on your own?"}, {"speaker": "SPEAKER_03", "timestamp": [3200.1, 3202.1], "text": " It is a SaaS service, yes."}, {"speaker": "SPEAKER_01", "timestamp": [3202.1, 3203.1], "text": " Cool."}, {"speaker": "SPEAKER_00", "timestamp": [3203.1, 3204.1], "text": " Cool."}, {"speaker": "SPEAKER_03", "timestamp": [3204.1, 3207.22], "text": " Absolutely. There is a freemium offering for people who want to try it out."}, {"speaker": "SPEAKER_00", "timestamp": [3208.18, 3211.54], "text": " So you can go to our website, opsfeeds.com, and check it out."}, {"speaker": "SPEAKER_03", "timestamp": [3212.94, 3214.66], "text": " Cool, cool. Thank you."}, {"speaker": "SPEAKER_01", "timestamp": [3215.24, 3217.2], "text": " Next question. We've got lots of questions coming in."}, {"speaker": "SPEAKER_01", "timestamp": [3217.28, 3218.94], "text": " Thank you, everybody, so much for submitting those."}, {"speaker": "SPEAKER_01", "timestamp": [3219.42, 3221.82], "text": " Please keep them coming. I think we have about seven minutes,"}, {"speaker": "SPEAKER_01", "timestamp": [3221.92, 3224.98], "text": " so I'm happy to field those as much as possible."}, {"speaker": "SPEAKER_01", "timestamp": [3225.76, 3226.84], "text": " The next question is,"}, {"speaker": "SPEAKER_01", "timestamp": [3226.84, 3229.22], "text": " is it a good idea to export this data"}, {"speaker": "SPEAKER_01", "timestamp": [3229.22, 3232.4], "text": " to do offline troubleshooting by importing data collected?"}, {"speaker": "SPEAKER_01", "timestamp": [3232.4, 3234.94], "text": " I was wondering about troubleshooting edge deployment cases"}, {"speaker": "SPEAKER_01", "timestamp": [3234.94, 3237.04], "text": " where we don't have access to the cluster."}, {"speaker": "SPEAKER_03", "timestamp": [3238.18, 3239.02], "text": " Interesting."}, {"speaker": "SPEAKER_03", "timestamp": [3239.92, 3243.12], "text": " So you're talking about when you don't have,"}, {"speaker": "SPEAKER_03", "timestamp": [3243.12, 3245.52], "text": " if you can collect those metrics, you're saying,"}, {"speaker": "SPEAKER_03", "timestamp": [3245.52, 3249.28], "text": " and push it to us, it's a little trickier with this,"}, {"speaker": "SPEAKER_03", "timestamp": [3249.28, 3250.52], "text": " and depending on the context,"}, {"speaker": "SPEAKER_03", "timestamp": [3250.52, 3252.64], "text": " maybe you have to dig into a little more specifics,"}, {"speaker": "SPEAKER_03", "timestamp": [3252.64, 3254.76], "text": " you know, what data, because remember,"}, {"speaker": "SPEAKER_03", "timestamp": [3254.76, 3256.42], "text": " not understand the application context,"}, {"speaker": "SPEAKER_03", "timestamp": [3256.42, 3259.42], "text": " we pull everything, we may be able to see the dependencies."}, {"speaker": "SPEAKER_03", "timestamp": [3259.42, 3262.0], "text": " So seeing it in isolation doesn't tell you that."}, {"speaker": "SPEAKER_03", "timestamp": [3262.0, 3264.4], "text": " So it would probably be specific,"}, {"speaker": "SPEAKER_03", "timestamp": [3264.4, 3265.04], "text": " so we can take this"}, {"speaker": "SPEAKER_03", "timestamp": [3265.04, 3270.72], "text": " offline and just if this uh you know attendee has something specific that we can follow up"}, {"speaker": "SPEAKER_00", "timestamp": [3270.72, 3276.16], "text": " and ping us yeah i will mention you know it you know it depends on how your edges it just"}, {"speaker": "SPEAKER_03", "timestamp": [3276.16, 3282.8], "text": " compared like if you flat out don't have like um access to to like export metrics right i mean"}, {"speaker": "SPEAKER_00", "timestamp": [3282.8, 3286.06], "text": " again you know obscures itself isn't really doing much"}, {"speaker": "SPEAKER_03", "timestamp": [3286.06, 3287.36], "text": " on the collection side."}, {"speaker": "SPEAKER_00", "timestamp": [3287.44, 3288.56], "text": " It's really around"}, {"speaker": "SPEAKER_01", "timestamp": [3288.56, 3291.66], "text": " having Prometheus on the cluster"}, {"speaker": "SPEAKER_01", "timestamp": [3291.66, 3293.8], "text": " and having Loki or FluentD"}, {"speaker": "SPEAKER_01", "timestamp": [3293.8, 3295.4], "text": " on the cluster to collect that data."}, {"speaker": "SPEAKER_01", "timestamp": [3296.08, 3298.02], "text": " Really, if you don't have access to there,"}, {"speaker": "SPEAKER_01", "timestamp": [3298.3, 3300.02], "text": " that's something to be explored."}, {"speaker": "SPEAKER_01", "timestamp": [3300.38, 3302.06], "text": " But speaking about Edge itself,"}, {"speaker": "SPEAKER_01", "timestamp": [3302.58, 3304.6], "text": " we have recently published"}, {"speaker": "SPEAKER_01", "timestamp": [3304.6, 3307.44], "text": " a joint blog with Verizon where we're talking about."}, {"speaker": "SPEAKER_01", "timestamp": [3307.58, 3309.52], "text": " And again, I don't know what's going on with my D&D button."}, {"speaker": "SPEAKER_01", "timestamp": [3309.98, 3310.76], "text": " I know somebody mentioned it."}, {"speaker": "SPEAKER_01", "timestamp": [3310.86, 3312.28], "text": " I don't know what's going on with my Do Not Disturb."}, {"speaker": "SPEAKER_01", "timestamp": [3312.32, 3313.46], "text": " I promise I turned it on."}, {"speaker": "SPEAKER_00", "timestamp": [3314.12, 3330.88], "text": " But anyway, you know, when you're running Kubernetes clusters, for example, at the Edge or running workloads at the Edge, it absolutely is a supported model right again that joint blog where i mentioned is is uh is launching a kubernetes cluster on aws wavelength and um"}, {"speaker": "SPEAKER_01", "timestamp": [3330.88, 3336.32], "text": " you know with kubernetes and uh observability built in with you know with obstacles and you"}, {"speaker": "SPEAKER_00", "timestamp": [3336.32, 3339.92], "text": " know it functions perfectly fine but again if you have like a really really locked down edge and"}, {"speaker": "SPEAKER_00", "timestamp": [3339.92, 3344.0], "text": " that might be something we can you know talk offline feel free to reach out we can talk about"}, {"speaker": "SPEAKER_03", "timestamp": [3344.0, 3345.18], "text": " that scenario."}, {"speaker": "SPEAKER_04", "timestamp": [3346.34, 3348.22], "text": " It's a definitely use case because a lot of the edge applications"}, {"speaker": "SPEAKER_03", "timestamp": [3348.22, 3349.44], "text": " are not deploying in Kubernetes,"}, {"speaker": "SPEAKER_04", "timestamp": [3349.64, 3351.14], "text": " so we are playing into that space."}, {"speaker": "SPEAKER_03", "timestamp": [3352.78, 3354.9], "text": " Awesome. Awesome. Thank you."}, {"speaker": "SPEAKER_04", "timestamp": [3355.48, 3357.16], "text": " Next question is,"}, {"speaker": "SPEAKER_03", "timestamp": [3357.26, 3359.14], "text": " is AWS BottleRocket supported?"}, {"speaker": "SPEAKER_01", "timestamp": [3361.48, 3362.5], "text": " Can you read the second part?"}, {"speaker": "SPEAKER_03", "timestamp": [3362.7, 3363.74], "text": " AWS what? Sorry."}, {"speaker": "SPEAKER_01", "timestamp": [3364.38, 3365.5], "text": " Yeah, Amazon's operating system, BottleRocket, Can you read the second part? AWS? What? Sorry."}, {"speaker": "SPEAKER_01", "timestamp": [3365.5, 3366.5], "text": " Yeah, so that's..."}, {"speaker": "SPEAKER_01", "timestamp": [3366.5, 3371.36], "text": " Yeah, Amazon's operating system, BottleRocket, I believe that it's kind of built for containers"}, {"speaker": "SPEAKER_01", "timestamp": [3371.36, 3373.6], "text": " and running things on that front."}, {"speaker": "SPEAKER_01", "timestamp": [3373.6, 3378.0], "text": " My initial thought would be yes, because of the interfaces that you've chosen to bind"}, {"speaker": "SPEAKER_01", "timestamp": [3378.0, 3380.62], "text": " to, you know, CNI, CSI, all of those things."}, {"speaker": "SPEAKER_01", "timestamp": [3380.62, 3383.04], "text": " But so long as those are supported, that should be good."}, {"speaker": "SPEAKER_01", "timestamp": [3383.04, 3390.88], "text": " But not sure if that might correlate to the system metrics might be the the specific question yeah correct so it should be"}, {"speaker": "SPEAKER_00", "timestamp": [3390.88, 3394.96], "text": " you know i don't i don't remember if there's actually somebody that is using aws bottle"}, {"speaker": "SPEAKER_00", "timestamp": [3394.96, 3399.52], "text": " rocket but again we actually don't go in necessarily too much into the into the os"}, {"speaker": "SPEAKER_00", "timestamp": [3399.52, 3411.2], "text": " because as long as they're running like the minimum required kernel like on the on the on the nodes right which is uh i believe kernel 415 uh of linux and up um yeah i mean we we shouldn't have any"}, {"speaker": "SPEAKER_00", "timestamp": [3411.2, 3415.52], "text": " any issues supporting that um if you want to explore i i highly suggest signing up for the"}, {"speaker": "SPEAKER_00", "timestamp": [3415.52, 3422.0], "text": " uh for the free version and it should work i don't see why it wouldn't um so yeah that that's primarily"}, {"speaker": "SPEAKER_03", "timestamp": [3422.0, 3426.26], "text": " we look at os's that will enable collecting flow data like eBPF."}, {"speaker": "SPEAKER_03", "timestamp": [3426.26, 3428.26], "text": " That's probably our primary requirement."}, {"speaker": "SPEAKER_03", "timestamp": [3428.26, 3429.64], "text": " That's it."}, {"speaker": "SPEAKER_01", "timestamp": [3429.64, 3430.48], "text": " Cool."}, {"speaker": "SPEAKER_03", "timestamp": [3430.48, 3433.54], "text": " And all those nodes run Node Explorer, of course."}, {"speaker": "SPEAKER_01", "timestamp": [3433.54, 3434.44], "text": " Cool, cool."}, {"speaker": "SPEAKER_01", "timestamp": [3434.44, 3435.46], "text": " Interesting, interesting."}, {"speaker": "SPEAKER_01", "timestamp": [3435.46, 3437.26], "text": " It's fun to kind of see all these different computes"}, {"speaker": "SPEAKER_01", "timestamp": [3437.26, 3439.38], "text": " and then being able to surface that information."}, {"speaker": "SPEAKER_03", "timestamp": [3439.38, 3440.9], "text": " And you know, it's not just AWS."}, {"speaker": "SPEAKER_03", "timestamp": [3440.9, 3442.86], "text": " We work with any cloud vendor."}, {"speaker": "SPEAKER_03", "timestamp": [3442.86, 3445.92], "text": " That's the advantage of working without looking at provider"}, {"speaker": "SPEAKER_01", "timestamp": [3445.92, 3454.56], "text": " agents. Awesome. Awesome. Awesome. Next question is, does the SaaS support SSO and SAML login?"}, {"speaker": "SPEAKER_00", "timestamp": [3456.08, 3462.24], "text": " Yes, absolutely. Yes. So I believe the free version doesn't have that quality of life."}, {"speaker": "SPEAKER_00", "timestamp": [3462.24, 3465.26], "text": " There's some of those things that are you know, are like more enterprise features."}, {"speaker": "SPEAKER_00", "timestamp": [3465.68, 3466.62], "text": " But yes, absolutely."}, {"speaker": "SPEAKER_00", "timestamp": [3467.04, 3470.72], "text": " Many of our customers are using like Azure AD"}, {"speaker": "SPEAKER_00", "timestamp": [3470.72, 3473.02], "text": " or they might be using Okta, et cetera."}, {"speaker": "SPEAKER_00", "timestamp": [3473.14, 3474.06], "text": " We absolutely support that."}, {"speaker": "SPEAKER_00", "timestamp": [3475.56, 3476.0], "text": " Excellent."}, {"speaker": "SPEAKER_01", "timestamp": [3477.04, 3478.88], "text": " Next question is, I promise,"}, {"speaker": "SPEAKER_01", "timestamp": [3479.0, 3480.8], "text": " keep peppering you until we're out of time."}, {"speaker": "SPEAKER_00", "timestamp": [3480.8, 3480.94], "text": " Keep going."}, {"speaker": "SPEAKER_00", "timestamp": [3482.88, 3486.44], "text": " What resources does OpsCruise require?"}, {"speaker": "SPEAKER_00", "timestamp": [3486.56, 3488.18], "text": " I'm guessing that might be pertinent"}, {"speaker": "SPEAKER_00", "timestamp": [3488.18, 3491.0], "text": " to like Kubernetes primitives,"}, {"speaker": "SPEAKER_00", "timestamp": [3491.2, 3493.56], "text": " like nodes, storage, deployments, config maps."}, {"speaker": "SPEAKER_03", "timestamp": [3493.58, 3494.84], "text": " You're talking about what is required"}, {"speaker": "SPEAKER_01", "timestamp": [3494.84, 3496.6], "text": " for just the collectors that we have, right?"}, {"speaker": "SPEAKER_01", "timestamp": [3496.8, 3497.72], "text": " That's an easy one."}, {"speaker": "SPEAKER_01", "timestamp": [3498.12, 3500.9], "text": " I think it might be more the platform"}, {"speaker": "SPEAKER_03", "timestamp": [3500.9, 3502.66], "text": " to like install that integration"}, {"speaker": "SPEAKER_03", "timestamp": [3502.66, 3504.8], "text": " to report the data to the SaaS."}, {"speaker": "SPEAKER_03", "timestamp": [3505.18, 3505.3], "text": " Yeah, so for, you know, when we showed the architecture, really, more the platform to install that integration to report the data to the SaaS. Yeah."}, {"speaker": "SPEAKER_00", "timestamp": [3505.66, 3514.06], "text": " So when we showed the architecture, really, it doesn't look like a cluster, right?"}, {"speaker": "SPEAKER_03", "timestamp": [3514.18, 3519.7], "text": " But typically, for the actual open source collection tools, I mean, each one has their"}, {"speaker": "SPEAKER_00", "timestamp": [3519.7, 3521.06], "text": " own requirements, but they're really small."}, {"speaker": "SPEAKER_00", "timestamp": [3521.06, 3526.48], "text": " I mean, we're talking about hundreds of millicores to run the open"}, {"speaker": "SPEAKER_00", "timestamp": [3526.48, 3531.84], "text": " source collectors like C-Advisor and Notixware. Those are all really, really lightweight."}, {"speaker": "SPEAKER_01", "timestamp": [3531.84, 3538.4], "text": " The only piece that utilizes more resources is really Prometheus, right? And that's just"}, {"speaker": "SPEAKER_01", "timestamp": [3538.4, 3545.2], "text": " depending on how many objects you have in the cluster. We typically for a small size cluster recommend maybe like in maybe like a"}, {"speaker": "SPEAKER_01", "timestamp": [3545.84, 3552.96], "text": " two CPU 8 or 12 gig machine. But you know, as you scale up in the amount of objects that you know,"}, {"speaker": "SPEAKER_00", "timestamp": [3552.96, 3558.56], "text": " I think I think I've seen just and I might be misremembering. So if you have if you want to"}, {"speaker": "SPEAKER_00", "timestamp": [3558.56, 3564.8], "text": " more details, like really hard numbers, please reach out. But I think we've seen like five,"}, {"speaker": "SPEAKER_00", "timestamp": [3568.6, 3577.9], "text": " hard numbers, please reach out. But I think we've seen like five, close to 5000 containers being monitored by like, at this point, maybe 64 gig for CPU machine node to power Prometheus. And it's"}, {"speaker": "SPEAKER_00", "timestamp": [3577.9, 3582.52], "text": " not fully used. But sometimes when it you know, when you scale out, and you get just all those"}, {"speaker": "SPEAKER_00", "timestamp": [3582.52, 3585.0], "text": " some tons of spike spike in objects."}, {"speaker": "SPEAKER_00", "timestamp": [3585.94, 3587.32], "text": " That's really when it uses that."}, {"speaker": "SPEAKER_00", "timestamp": [3587.32, 3589.36], "text": " But Prometheus is really the biggest one"}, {"speaker": "SPEAKER_00", "timestamp": [3589.36, 3591.32], "text": " and you'll find this, it's not an option,"}, {"speaker": "SPEAKER_00", "timestamp": [3591.32, 3593.58], "text": " it's a Prometheus piece."}, {"speaker": "SPEAKER_01", "timestamp": [3593.58, 3595.78], "text": " But other than that, all those components,"}, {"speaker": "SPEAKER_01", "timestamp": [3595.78, 3597.18], "text": " I mean, you're talking extremely,"}, {"speaker": "SPEAKER_01", "timestamp": [3597.18, 3599.12], "text": " extremely small resource requirements,"}, {"speaker": "SPEAKER_01", "timestamp": [3599.12, 3601.4], "text": " really inexorable on your clusters."}, {"speaker": "SPEAKER_01", "timestamp": [3602.48, 3604.34], "text": " Absolutely, absolutely."}, {"speaker": "SPEAKER_01", "timestamp": [3604.34, 3607.32], "text": " Well, with that, unfortunately, we are at time."}, {"speaker": "SPEAKER_01", "timestamp": [3607.42, 3609.12], "text": " I really do appreciate everybody reaching out"}, {"speaker": "SPEAKER_01", "timestamp": [3609.12, 3610.42], "text": " and asking those questions."}, {"speaker": "SPEAKER_01", "timestamp": [3610.94, 3614.3], "text": " Like all good things, the streams have to come to an end."}, {"speaker": "SPEAKER_01", "timestamp": [3614.56, 3616.26], "text": " So we are at that point."}, {"speaker": "SPEAKER_03", "timestamp": [3616.38, 3617.6], "text": " But thank you so much."}, {"speaker": "SPEAKER_03", "timestamp": [3617.6, 3620.56], "text": " If there is anyone looking to kind of reach out to either of you,"}, {"speaker": "SPEAKER_03", "timestamp": [3620.66, 3623.14], "text": " is there a good place to open up those questions?"}, {"speaker": "SPEAKER_03", "timestamp": [3623.7, 3624.04], "text": " Sure."}, {"speaker": "SPEAKER_01", "timestamp": [3624.12, 3629.24], "text": " You can reach us to info at obscurus.com to be generic enough."}, {"speaker": "SPEAKER_03", "timestamp": [3629.24, 3633.84], "text": " And you can also ping us on the website, obscurus.com itself."}, {"speaker": "SPEAKER_03", "timestamp": [3633.84, 3635.88], "text": " You know, it should be easy to find us."}, {"speaker": "SPEAKER_03", "timestamp": [3635.88, 3639.44], "text": " We're also on LinkedIn if you want to look us up on the Obscurus page."}, {"speaker": "SPEAKER_03", "timestamp": [3639.44, 3641.76], "text": " Love to chat with you guys, get your feedback."}, {"speaker": "SPEAKER_02", "timestamp": [3641.76, 3642.76], "text": " Excellent."}, {"speaker": "SPEAKER_01", "timestamp": [3642.76, 3643.76], "text": " Excellent."}, {"speaker": "SPEAKER_02", "timestamp": [3643.76, 3646.64], "text": " Hopefully this was interesting and exciting,"}, {"speaker": "SPEAKER_02", "timestamp": [3646.64, 3649.16], "text": " given where things are going with OpenTelemetry,"}, {"speaker": "SPEAKER_02", "timestamp": [3649.16, 3651.64], "text": " open source and observability."}, {"speaker": "SPEAKER_01", "timestamp": [3651.64, 3653.4], "text": " Well, thank you both so much."}, {"speaker": "SPEAKER_01", "timestamp": [3653.4, 3655.74], "text": " Thank you, everyone, for joining the latest episode"}, {"speaker": "SPEAKER_01", "timestamp": [3655.74, 3656.6], "text": " of Cloud Native Live."}, {"speaker": "SPEAKER_01", "timestamp": [3656.6, 3659.0], "text": " It was great to hear from Eloke and Cesar."}, {"speaker": "SPEAKER_01", "timestamp": [3659.0, 3661.78], "text": " We really, again, love the interaction and questions"}, {"speaker": "SPEAKER_01", "timestamp": [3661.78, 3663.54], "text": " from all of the audience."}, {"speaker": "SPEAKER_01", "timestamp": [3663.54, 3666.96], "text": " Join us next week to hear about how we're going to be building stability"}, {"speaker": "SPEAKER_01", "timestamp": [3666.96, 3670.3], "text": " in Kubernetes with Andy Suderman of Fairwinds."}, {"speaker": "SPEAKER_01", "timestamp": [3670.78, 3673.64], "text": " Thank you all for joining us today, and we will see you soon."}, {"speaker": "SPEAKER_01", "timestamp": [3674.1, 3674.42], "text": " Have a good one."}, {"speaker": "SPEAKER_03", "timestamp": [3674.44, 3674.94], "text": " Thanks, everyone."}], "chunks": [{"timestamp": [0.0, 15.6], "text": " Hello, everyone."}, {"timestamp": [15.88, 20.4], "text": " Welcome to Cloud Native Live, where we dive into the code behind Cloud Native."}, {"timestamp": [21.06, 25.04], "text": " I'm Taylor Dolezal, a senior developer advocate at HashiCorp, where I focus"}, {"timestamp": [25.04, 30.4], "text": " on all things infrastructure, application delivery, and developer experience. Every week,"}, {"timestamp": [30.46, 35.08], "text": " we bring a new set of presenters to showcase how to work with cloud native technologies."}, {"timestamp": [35.6, 39.34], "text": " They will build things, they will break things, and they will answer your questions."}, {"timestamp": [40.04, 48.0], "text": " In today's session, Alok and Cesar have joined us to talk about leveraging the CNCF observability tools for Kubernetes troubleshooting."}, {"timestamp": [48.44, 55.02], "text": " This is an official live stream of the CNCF and as such is subject to the CNCF Code of Conduct."}, {"timestamp": [55.44, 60.22], "text": " Please don't add anything to the chat or questions that would be in violation of the Code of Conduct."}, {"timestamp": [60.52, 64.8], "text": " Basically, please be respectful to all of your fellow participants and presenters."}, {"timestamp": [65.0, 68.0], "text": " In short, please be respectful to all of your fellow participants and presenters. In short, please be excellent to one another."}, {"timestamp": [68.0, 73.0], "text": " With that, I'd love to hand it over to Alok and Cesar to kick off today's presentation."}, {"timestamp": [73.0, 75.0], "text": " With that, I'll turn it over to you."}, {"timestamp": [75.0, 89.44], "text": " Thank you, Taylor. So I'm Alok. I'm the Founder and CTO of Upscruise, an observability company built on open source and CNCF telemetry and I will also introduce Cesar Quintana"}, {"timestamp": [89.44, 97.12], "text": " my colleague who was the principal solutions architect at Opscores thank you so the way we"}, {"timestamp": [97.12, 104.24], "text": " thought we would do this before we set up the demo itself and go through that the fun part I want to"}, {"timestamp": [104.24, 105.52], "text": " do a little bit of setup so you know"}, {"timestamp": [105.52, 111.12], "text": " have the context of what we do so with that in mind let me share my screen and bring up"}, {"timestamp": [111.12, 115.76], "text": " uh you know kind of set the stage if you will let me find the right one"}, {"timestamp": [122.0, 123.92], "text": " and let me know if this is coming up"}, {"timestamp": [125.0, 126.0], "text": " And let me know if this is coming up. Cool."}, {"timestamp": [126.0, 128.5], "text": " That looks good here."}, {"timestamp": [128.5, 130.4], "text": " Okay, great."}, {"timestamp": [130.4, 136.6], "text": " So as mentioned by Taylor, we are talking about how to add intelligence and observability"}, {"timestamp": [136.6, 140.4], "text": " now that we have open source monitoring."}, {"timestamp": [140.4, 145.52], "text": " Going through the standard confidentiality and legal notice. We'll skip over that."}, {"timestamp": [147.18, 147.4], "text": " I'll let the stage by, you know,"}, {"timestamp": [153.08, 153.96], "text": " revisiting what really has become the challenges of cloud native application observability, right?"}, {"timestamp": [153.96, 158.32], "text": " And fundamentally this has been happening now for a few years as applications"}, {"timestamp": [158.32, 161.02], "text": " move to microservice architectures."}, {"timestamp": [161.8, 162.06], "text": " You know,"}, {"timestamp": [162.18, 168.34], "text": " there are three things that we know have started and really added a lot more challenges to the ops"}, {"timestamp": [168.34, 170.18], "text": " teams that are managing them."}, {"timestamp": [170.18, 174.44], "text": " Number one was just complexity, scale, the tiering."}, {"timestamp": [174.44, 177.04], "text": " There's so many dependencies."}, {"timestamp": [177.04, 179.0], "text": " And the dependencies, as we've shown on red,"}, {"timestamp": [179.0, 181.12], "text": " you've got, of course, the application pieces."}, {"timestamp": [181.12, 183.6], "text": " Could be PaaS services, could be SaaS services,"}, {"timestamp": [183.6, 185.88], "text": " could be Kubernetes container could be SAS services could be a kubernetes kubernetes"}, {"timestamp": [185.88, 191.58], "text": " uh container running an application code it could be serverless all of them and then of course even"}, {"timestamp": [191.58, 195.96], "text": " dependents on kubernetes itself orchestrating all of these and then the underlying infrastructure"}, {"timestamp": [195.96, 202.26], "text": " wherever it might be so these create tier dependencies you know if you will from top"}, {"timestamp": [202.26, 205.08], "text": " down kind of what we call vertical, as well as across."}, {"timestamp": [209.96, 215.58], "text": " And this is happening all the time. The third complexity is dynamism. Great. We want to be agile, right? We want to add services, change any one component, scale up, scale in, you know,"}, {"timestamp": [215.76, 221.6], "text": " some things drop, some things brought up. That, together with all of this, is like a highly"}, {"timestamp": [221.6, 225.78], "text": " complex distributed system. And just looking at a couple of metrics"}, {"timestamp": [225.78, 228.12], "text": " is no longer sufficient."}, {"timestamp": [228.46, 230.72], "text": " You know, things are changing, things are coming up."}, {"timestamp": [230.98, 232.54], "text": " You have a zillion dashboards."}, {"timestamp": [233.46, 237.66], "text": " So the good news and the sort of not so good news"}, {"timestamp": [237.66, 238.26], "text": " is the following."}, {"timestamp": [238.38, 241.9], "text": " The good news is, thanks to CNCF and open source,"}, {"timestamp": [241.9, 244.24], "text": " pretty much every possible telemetry"}, {"timestamp": [244.24, 248.88], "text": " from real-time metrics logs events"}, {"timestamp": [249.68, 257.36], "text": " configuration information traces flows are all available now directly from an open source"}, {"timestamp": [257.36, 262.64], "text": " environment meaning open telemetry is an example open source that has existed now for a while"}, {"timestamp": [263.2, 268.48], "text": " so that means we can move to the the key pieces to solve that problem."}, {"timestamp": [268.62, 269.54], "text": " What's the not so good news?"}, {"timestamp": [269.62, 270.22], "text": " That complexity."}, {"timestamp": [271.18, 272.9], "text": " The role of observability is changing."}, {"timestamp": [273.04, 275.3], "text": " It's no longer about dashboards and just alerting."}, {"timestamp": [275.62, 282.66], "text": " It's how can we help the ops teams get to understand what's happening in real time"}, {"timestamp": [282.66, 289.6], "text": " so they can detect quickly, find the real issues, and get up and running you know it's the same things that you've heard"}, {"timestamp": [289.6, 295.76], "text": " time to mean time to test should be fast don't waste time with false alerts and get to the root"}, {"timestamp": [295.76, 308.1], "text": " cost mean time to resolution right so what we what we've learned having been born cloud native ourselves, is what does Ops really do, if you think about it?"}, {"timestamp": [308.1, 311.38], "text": " What Ops does, beyond getting all the telemetry,"}, {"timestamp": [311.38, 313.56], "text": " is they understand the dependency."}, {"timestamp": [313.56, 315.9], "text": " And they look at an application with multiple services"}, {"timestamp": [315.9, 317.02], "text": " talking to each other."}, {"timestamp": [317.02, 319.28], "text": " They understand what the interactions are."}, {"timestamp": [319.28, 322.38], "text": " They know how applications, services"}, {"timestamp": [322.38, 324.0], "text": " are being monitored by the orchestration."}, {"timestamp": [324.0, 326.24], "text": " They know the dependence of infrastructure."}, {"timestamp": [326.24, 330.84], "text": " So really savvy ops teams, SRE teams know that."}, {"timestamp": [330.84, 332.88], "text": " They also know what's changing"}, {"timestamp": [332.88, 334.5], "text": " and they apply curated knowledge."}, {"timestamp": [334.5, 337.78], "text": " They know a published, subscribing,"}, {"timestamp": [337.78, 340.74], "text": " or producer consumer model versus a database,"}, {"timestamp": [340.74, 342.04], "text": " what they're supposed to do."}, {"timestamp": [342.04, 344.82], "text": " They know what to look for, what metrics to look for."}, {"timestamp": [344.82, 349.76], "text": " They are aware of the app and the dependencies. That knowledge is what they're supposed to do they know what to look for what metrics to look for they are aware of the app and the dependencies that knowledge is what they use when they look through"}, {"timestamp": [349.76, 354.8], "text": " the data and sift through whether it's the metrics the laws of the traces right so fundamentally when"}, {"timestamp": [354.8, 359.2], "text": " they look at something that's happening they're looking at the every component the inbound and"}, {"timestamp": [359.2, 365.3], "text": " outbound who's talking to whom what resources and services depending on? All of this."}, {"timestamp": [365.3, 368.62], "text": " And so they essentially built context"}, {"timestamp": [368.62, 369.82], "text": " to understand what is happening"}, {"timestamp": [369.82, 373.1], "text": " so they can actually detect the problem,"}, {"timestamp": [373.1, 374.92], "text": " isolate it and analyze"}, {"timestamp": [374.92, 377.98], "text": " and figure out what the resolution should be."}, {"timestamp": [377.98, 379.04], "text": " So if you think about,"}, {"timestamp": [379.04, 382.2], "text": " if observability has to be really intelligent,"}, {"timestamp": [382.2, 386.54], "text": " they have to establish this context, this understanding,"}, {"timestamp": [386.54, 389.66], "text": " and surface that from all that effectively"}, {"timestamp": [389.66, 392.72], "text": " called noise that's coming in, all the data that's sitting in."}, {"timestamp": [392.72, 394.82], "text": " If you can't do that, then we've actually"}, {"timestamp": [394.82, 399.14], "text": " made the life of a typical DevOps and SRE very difficult."}, {"timestamp": [399.14, 400.66], "text": " So that's what we want to do."}, {"timestamp": [400.66, 405.0], "text": " So our thesis is help leverage this data, right?"}, {"timestamp": [405.0, 410.0], "text": " If you look at open source and open telemetry, clearly we know that things like Prometheus,"}, {"timestamp": [410.0, 415.0], "text": " getting information from Fluidly for the logs and pulling into something like Grafana Loki,"}, {"timestamp": [415.0, 421.0], "text": " using Jaeger or open tracing standards even to get the traces,"}, {"timestamp": [421.0, 428.96], "text": " looking at flows from things like eBPF and Istio, using configuration information from Kubernetes, changes,"}, {"timestamp": [429.22, 433.5], "text": " even the cloud infrastructure data as well as pass information,"}, {"timestamp": [433.66, 436.52], "text": " all that that's available, pull that together"}, {"timestamp": [436.52, 439.4], "text": " to build that context across all of them"}, {"timestamp": [439.4, 443.84], "text": " and then help reduce the amount of information"}, {"timestamp": [443.84, 446.96], "text": " and focus on the right information that ops needs, right?"}, {"timestamp": [447.72, 450.66], "text": " So this is probably the one key slide"}, {"timestamp": [450.66, 453.7], "text": " as we get into the demo to tell you."}, {"timestamp": [454.24, 456.16], "text": " On the left-hand side, what you're seeing"}, {"timestamp": [456.16, 458.3], "text": " is all the open telemetry,"}, {"timestamp": [458.46, 460.82], "text": " all the open source that is available today."}, {"timestamp": [461.2, 463.06], "text": " You don't have to put proprietary agents"}, {"timestamp": [463.06, 466.0], "text": " and do proprietary information code thanks"}, {"timestamp": [466.0, 470.88], "text": " to cncf thanks to open source monitoring that's available so the first thing as we said for"}, {"timestamp": [470.88, 475.44], "text": " context is understand the structure of the application what we call the application graph"}, {"timestamp": [475.44, 479.84], "text": " right so imagine able to automatically build out that structure in real time"}, {"timestamp": [480.56, 489.0], "text": " so the ops guy doesn't know even the app developer doesn't have to go around trying to figure out what's talking to or do something exotic to get there."}, {"timestamp": [489.0, 499.0], "text": " And that graph has to change dynamically. That graph tells you who's talking to whom, what are they dependent on, how is Kubernetes, you know, managing it, allocating resources appropriately or not,"}, {"timestamp": [499.0, 506.4], "text": " and how does Kubernetes have access to the type of cloud resources it needs to allocate to the services"}, {"timestamp": [507.44, 514.32], "text": " and then to really understand what's going on that context pull the data and what we do is something"}, {"timestamp": [514.32, 519.76], "text": " called a behavior model profile every component that comprises the application to see what is"}, {"timestamp": [519.76, 524.96], "text": " expected profiling is kind of like building a very simple emulator model and you can look at it"}, {"timestamp": [524.24, 528.96], "text": " Profiling is kind of like building a very simple emulator model. And you can look at it, collect the data to figure out, for example, across all the metrics"}, {"timestamp": [528.96, 533.88], "text": " that you collect, all the flows and events, and say, hey, this one, for example, is IO"}, {"timestamp": [533.88, 537.72], "text": " dependent, this one is CPU dependent, or mix, or this one does a lot of calls."}, {"timestamp": [537.72, 542.86], "text": " So that as data is coming in, as you learn that, this is where ML comes in, you know"}, {"timestamp": [542.86, 550.54], "text": " what to expect, because you have the realtime metrics, you have the application graph. So once you have started learning that, over time,"}, {"timestamp": [551.68, 555.88], "text": " you know, within, actually in our case, within 24 hours, you start getting a baseline behavior,"}, {"timestamp": [555.88, 560.14], "text": " which gets better over time, you can start looking at deviations. So you don't have to"}, {"timestamp": [560.14, 563.66], "text": " worry about setting thresholds. You don't have to try and guess what the thresholds are and start"}, {"timestamp": [563.66, 570.16], "text": " tuning them. First of all, you don't know which metric and what the level is let the ml model learn expose that so"}, {"timestamp": [570.16, 575.84], "text": " that we can analyze it again in context we know what what are the what are the drivers for every"}, {"timestamp": [575.84, 581.04], "text": " service uh because we know the application route so once we have these deviations whether it's"}, {"timestamp": [581.04, 589.58], "text": " coming from explicit alerts like a failure in infrastructure, a failure that Kubernetes detects, or application starts slowing down or having a degradation,"}, {"timestamp": [590.16, 596.28], "text": " you put it all in context and then analyze that. We essentially do what we call local detection"}, {"timestamp": [596.28, 601.58], "text": " across the application, and then we call an analysis of what we call dynamic decision,"}, {"timestamp": [601.72, 605.04], "text": " which looks at everything in context, why would this happen given what I"}, {"timestamp": [605.04, 610.74], "text": " have seen so essentially think of it it's almost like an anthropomorphic what an ops would do and"}, {"timestamp": [610.74, 618.66], "text": " they understand if we can put all of this in place and automate this pipeline we have reduced amount"}, {"timestamp": [618.66, 623.64], "text": " of work that off spends today trying to understand what is it what does the application look like"}, {"timestamp": [623.64, 630.18], "text": " who's talking to whom when is there a problem there instead of setting thresholds, and if I do, how do I analyze"}, {"timestamp": [630.18, 637.82], "text": " it? If we can collapse that and reduce that, we have really done the right service to get the"}, {"timestamp": [637.82, 642.42], "text": " right level of intelligence and observability. So this flow, if you think about it, is what we will"}, {"timestamp": [642.42, 645.28], "text": " demo today using what you're seeing on the left"}, {"timestamp": [645.28, 651.2], "text": " that essentially build context understand the application graph understand the behavior to"}, {"timestamp": [651.2, 656.48], "text": " surface problems detected analyze it in context using all the telemetry we have"}, {"timestamp": [657.52, 664.08], "text": " you know including changes logs events and help isolate the cause and so that's our purpose of"}, {"timestamp": [664.08, 665.22], "text": " our demo today."}, {"timestamp": [667.32, 669.16], "text": " And I'm going to hand this over to Cesar because we want to get to the demo"}, {"timestamp": [669.16, 671.78], "text": " and he'll tell you exactly how we leverage"}, {"timestamp": [671.78, 676.92], "text": " open source monitoring and use CNCF,"}, {"timestamp": [677.02, 678.08], "text": " OpenTelemetry to do this."}, {"timestamp": [678.14, 679.06], "text": " So I'm going to stop sharing."}, {"timestamp": [679.2, 680.8], "text": " Cesar, please take it away."}, {"timestamp": [681.72, 683.88], "text": " Actually, look, if you could go back to that,"}, {"timestamp": [683.94, 686.04], "text": " we'll talk really briefly about those"}, {"timestamp": [686.04, 687.26], "text": " open source platforms"}, {"timestamp": [687.26, 689.7], "text": " that we're leveraging. If you could share that."}, {"timestamp": [690.16, 692.28], "text": " I'll go back. I shouldn't have done"}, {"timestamp": [692.28, 693.46], "text": " that. I was a little too hasty there."}, {"timestamp": [697.68, 699.14], "text": " All right. Is that coming up?"}, {"timestamp": [700.08, 701.94], "text": " There it is."}, {"timestamp": [702.26, 702.58], "text": " All right."}, {"timestamp": [703.76, 706.0], "text": " Yeah. So again, everybody, my name is Cesar Quintana."}, {"timestamp": [706.0, 709.0], "text": " I'm a principal solutions architect at OpsCrews here."}, {"timestamp": [709.0, 713.0], "text": " And, yeah, so to add on to what Alok was mentioning, right,"}, {"timestamp": [713.0, 718.0], "text": " the whole premise of leveraging these open source platforms that,"}, {"timestamp": [718.0, 723.0], "text": " you know, essentially the whole data collection layer has been"}, {"timestamp": [723.0, 724.0], "text": " commoditized, right?"}, {"timestamp": [724.0, 734.54], "text": " Observing data is now easier than ever to access thanks to these, you know, powerful open source, particularly around the CNCF platforms, right?"}, {"timestamp": [734.6, 749.3], "text": " So what we've set out in mind, right, is to build something and leverage these amazing tools to make everybody's life easier, right? So things like this is an example of our architecture"}, {"timestamp": [749.72, 753.84], "text": " of how we're leveraging all this open source data"}, {"timestamp": [753.84, 755.34], "text": " and all these open source platforms."}, {"timestamp": [755.34, 758.22], "text": " So as you'll notice here,"}, {"timestamp": [758.22, 761.2], "text": " if you focus on that Kubernetes cluster square"}, {"timestamp": [761.2, 763.24], "text": " on the right side, right?"}, {"timestamp": [763.24, 765.58], "text": " What you'll see is across the top in the"}, {"timestamp": [765.58, 769.18], "text": " green you'll see your workloads right you know pod one two three four these"}, {"timestamp": [769.18, 772.9], "text": " are eventually your own applications running whatever you're doing whether"}, {"timestamp": [772.9, 777.32], "text": " you're running an e-commerce site a financial trading platform etc this is"}, {"timestamp": [777.32, 781.0], "text": " what you're running inside your actual workloads but underneath in the in that"}, {"timestamp": [781.0, 788.8], "text": " light and dark blue are the open source tools that are now so common throughout the"}, {"timestamp": [788.8, 794.76], "text": " IT landscape and in the modern application environments. So towards the bottom with the"}, {"timestamp": [794.76, 800.28], "text": " dark blue, you'll see here in this reference architecture, we're showing Jaeger, Prometheus,"}, {"timestamp": [800.6, 826.48], "text": " Loki. It could be something, this is just an example. We can leverage logs from other sources like Fluent. I think somebody asked about Fluent. It could be Loki. It could be Fluent. And then we take metrics in from Prometheus. And then Traces. We're leveraging Jaeger as a backend for our particular architecture. But we are supporting open telemetry libraries for the client side."}, {"timestamp": [826.48, 832.44], "text": " So the important that's one of the really, really cool things about the new standards"}, {"timestamp": [832.44, 837.76], "text": " is that they're now well defined, which means that you could be using a mixture in your"}, {"timestamp": [837.76, 844.76], "text": " environment of open zipkin and Jaeger, the open telemetry libraries themselves and still"}, {"timestamp": [844.76, 851.0], "text": " have a unified back end where you're able to collect all that data and leverage it and use it,"}, {"timestamp": [851.0, 855.0], "text": " even though you're technically using disparate libraries throughout your enterprise."}, {"timestamp": [855.0, 872.32], "text": " Right. So so what you'll see here, you know how we've architected ourselves to be built is again around these open source platforms again whether it's fluency whether it's loki and prometheus and jaeger etc they serve as you know now your your your data collection and data data"}, {"timestamp": [872.32, 879.36], "text": " store you don't have to go out and pay another vendor you know uh 10 15 x for storing just"}, {"timestamp": [879.36, 883.36], "text": " metrics right when you can store them in your own infrastructure we're all doing the same thing"}, {"timestamp": [883.36, 890.4], "text": " right just putting them inside of uh uh inside of a long-term, you know, bucket, right? And so now that's under"}, {"timestamp": [890.4, 897.28], "text": " your control. And so we, for example, Promptail, right? If you start looking upward towards the"}, {"timestamp": [897.28, 902.3], "text": " stack in the light blue, Promptail will run as a daemon set, collect logs from all your nodes and"}, {"timestamp": [902.3, 905.04], "text": " from all your containers, right?"}, {"timestamp": [907.84, 908.0], "text": " And then you have on top of that node exporter, right,"}, {"timestamp": [912.78, 913.08], "text": " functioning as an exporter for Prometheus to grab the metrics from the nodes themselves."}, {"timestamp": [921.1, 921.38], "text": " And going above that, you'll see C-Advisor collecting data from the containers themselves running on each node."}, {"timestamp": [930.44, 937.02], "text": " And then we also leverage KSM exporter, pretty awesome, grabbing Kubernetes object status data. And all those are going to be fed out into Prometheus or to if you're using traces again to Jaeger. And really, you know, now, even just with that,"}, {"timestamp": [937.16, 943.6], "text": " you've got a pretty darn functional observability layer, right? Now you have metrics and they have"}, {"timestamp": [943.6, 946.82], "text": " traces. Now you can go into different places and look at your logs."}, {"timestamp": [947.46, 952.58], "text": " But what we're, you know, what Alok was mentioning earlier is that smart layer, right?"}, {"timestamp": [953.24, 958.18], "text": " Now you want to leverage all those pieces of data, bring them in together and do something"}, {"timestamp": [958.18, 962.26], "text": " really, really powerful with having all that context, all that configuration data that"}, {"timestamp": [962.26, 965.78], "text": " we can grab from the Kubernetes API, and then just bring it all together."}, {"timestamp": [966.24, 973.12], "text": " On top of that, you have metric data, configuration data, performance data,"}, {"timestamp": [973.28, 977.06], "text": " and event data from your cloud environments, right?"}, {"timestamp": [977.06, 981.36], "text": " So bringing in things like, you know, more and more applications are hybrid, right?"}, {"timestamp": [981.42, 990.08], "text": " They're using, you know, whether it's VM and kubernetes or serverless and uh and pass you know you have all these really really hybrid environments that"}, {"timestamp": [990.08, 997.04], "text": " again it's the whole um uh extreme uh production of data and having one place and easy ways to"}, {"timestamp": [997.04, 1001.2], "text": " collect them and that's really what these open source platforms have allowed us to do right but"}, {"timestamp": [1001.2, 1009.98], "text": " going back to what i was mentioning about cloud you also want a place where you can grab your data and bring it in. Talking about, again, serverless or function as"}, {"timestamp": [1009.98, 1016.96], "text": " a service, the PaaS layers, which are only constantly growing, right? You have these cloud"}, {"timestamp": [1016.96, 1027.32], "text": " caches and messaging services, cloud databases, et cetera. So what OpsCrew sets out to do is not only grab that open source data"}, {"timestamp": [1027.32, 1028.78], "text": " and leverage those collection platforms,"}, {"timestamp": [1028.78, 1034.44], "text": " but also bring in the cloud data and mesh it all together"}, {"timestamp": [1034.44, 1035.72], "text": " and build something really, really rich"}, {"timestamp": [1035.72, 1038.6], "text": " and then provide actionable data based on that."}, {"timestamp": [1039.18, 1044.22], "text": " So what I'm going to do is I'm going to show you a demo of OpsCrew's."}, {"timestamp": [1044.4, 1046.02], "text": " Oh, sorry, Alok, did you want to add?"}, {"timestamp": [1046.18, 1048.42], "text": " Since I had the opportunity to look at the message,"}, {"timestamp": [1048.58, 1051.72], "text": " someone asked, what about FluentD or FluentBit?"}, {"timestamp": [1051.92, 1053.16], "text": " And, you know, everything for that."}, {"timestamp": [1053.26, 1054.6], "text": " So let's address that."}, {"timestamp": [1056.48, 1060.7], "text": " Yeah, no, so as mentioned, right, we can take logs, basically,"}, {"timestamp": [1061.1, 1063.98], "text": " from whether it's Loki, FluentBit is usually the thing,"}, {"timestamp": [1064.18, 1066.02], "text": " or FluentD, those are usually the pieces we"}, {"timestamp": [1066.02, 1068.3], "text": " run into, right? And absolutely"}, {"timestamp": [1068.3, 1070.18], "text": " you know, the whole point is to build a modular"}, {"timestamp": [1070.18, 1070.84], "text": " flexible"}, {"timestamp": [1070.84, 1074.28], "text": " platform where you can grab"}, {"timestamp": [1074.28, 1076.06], "text": " data from, you know,"}, {"timestamp": [1076.12, 1078.44], "text": " whatever your preferred variant"}, {"timestamp": [1078.44, 1080.18], "text": " of that is,"}, {"timestamp": [1080.38, 1082.1], "text": " right? So yeah, absolutely. OpsCrews particularly"}, {"timestamp": [1082.1, 1083.4], "text": " provides support for Fluent D,"}, {"timestamp": [1084.0, 1086.72], "text": " Loki, and a few others as well"}, {"timestamp": [1086.72, 1091.16], "text": " yeah so the takeaway message I want to do before we go to the demo so we can you know address them"}, {"timestamp": [1091.16, 1098.6], "text": " all across is as long as we have the source of getting the logs in the metrics in this approach"}, {"timestamp": [1098.6, 1103.58], "text": " will still work of course with OpenCNC we don't have to do proprietary agents proprietary"}, {"timestamp": [1103.58, 1107.32], "text": " instrumentation we can be sitting outside without being intrusive."}, {"timestamp": [1107.52, 1108.88], "text": " So think of it that way."}, {"timestamp": [1109.08, 1112.76], "text": " The real intelligence or observability is not how the metrics got to us"}, {"timestamp": [1112.96, 1115.32], "text": " and what it is, as long as we have coverage."}, {"timestamp": [1115.32, 1118.08], "text": " That's the key. The coverage is all of these is needed."}, {"timestamp": [1118.08, 1121.32], "text": " You can't just go on metrics and logs and traces independently."}, {"timestamp": [1121.52, 1122.72], "text": " It doesn't give you the whole picture."}, {"timestamp": [1122.92, 1125.3], "text": " Otherwise we are one of the six blind men looking"}, {"timestamp": [1125.3, 1128.14], "text": " at the elephant, which is in the room."}, {"timestamp": [1128.14, 1129.3], "text": " All right, go ahead, Cesar."}, {"timestamp": [1129.3, 1131.26], "text": " I had to say that because I think it's good."}, {"timestamp": [1131.26, 1133.02], "text": " CESAR GAVIDIA MARTÍNEZ- Thanks for that."}, {"timestamp": [1133.02, 1134.94], "text": " All right, so now I'll share."}, {"timestamp": [1134.94, 1137.1], "text": " So look, I think you might have to stop."}, {"timestamp": [1137.1, 1138.52], "text": " CESAR GAVIDIA MARTÍNEZ- I can stop sharing, right?"}, {"timestamp": [1138.52, 1140.76], "text": " Sure."}, {"timestamp": [1140.76, 1143.56], "text": " CESAR GAVIDIA MARTÍNEZ- All right."}, {"timestamp": [1143.56, 1155.36], "text": " So let me share hopefully you guys will be able to see my screen here"}, {"timestamp": [1156.82, 1162.32], "text": " so I have one proof pointed but there you go it's coming up excellent okay"}, {"timestamp": [1162.32, 1168.32], "text": " awesome so yeah so this is this is a uh this is our landing page for"}, {"timestamp": [1168.32, 1173.52], "text": " obscures and you can see there's there's uh quite a few pieces of data here um you might you know"}, {"timestamp": [1173.52, 1180.16], "text": " this screen might look familiar uh for any of you who have used apm tools uh before so this is a"}, {"timestamp": [1180.16, 1187.34], "text": " real-time service apology um map essentially you know you know, we're leveraging, as Alok mentioned,"}, {"timestamp": [1187.44, 1196.44], "text": " eBPF as well, right? So eBPF allows us to grab this network data and bring it in alongside not"}, {"timestamp": [1196.44, 1202.9], "text": " only the tracing, which in this case happens to be optional because we have eBPF, but it is the"}, {"timestamp": [1202.9, 1206.02], "text": " eBPF network data alongside the tracing data,"}, {"timestamp": [1206.14, 1208.66], "text": " alongside the metric data, alongside the logs,"}, {"timestamp": [1208.8, 1211.78], "text": " alongside the configuration data discovered from whether it's cloud"}, {"timestamp": [1211.78, 1214.04], "text": " or Kubernetes or the virtual machines themselves"}, {"timestamp": [1214.04, 1216.1], "text": " or the serverless or other PaaS components."}, {"timestamp": [1216.18, 1218.1], "text": " It's all brought together in a single place."}, {"timestamp": [1218.5, 1221.2], "text": " But we're giving you this real-time, excuse me,"}, {"timestamp": [1221.36, 1225.2], "text": " a flow of how your services are interacting with each other."}, {"timestamp": [1225.2, 1227.04], "text": " And I'm zooming in more."}, {"timestamp": [1227.04, 1230.72], "text": " Of course, now this is not even, this is nowhere close to some of the busiest environments,"}, {"timestamp": [1230.72, 1233.44], "text": " but you can see that it does get busy really, really quick."}, {"timestamp": [1233.6, 1237.96], "text": " And that's one of the cool things about, you know, having all the configuration data"}, {"timestamp": [1237.96, 1241.96], "text": " and the really rich data that the underlying tools like C-Advisor collect"}, {"timestamp": [1241.96, 1247.92], "text": " is that we get a lot of really rich object data along with the metric"}, {"timestamp": [1247.92, 1256.66], "text": " blocks. So things like being able to understand the configuration data of these pieces allows us"}, {"timestamp": [1256.66, 1263.48], "text": " to also extract things like labels and tags. So when you have a busy environment, you might only"}, {"timestamp": [1263.48, 1266.38], "text": " want to filter, for example, on a particular namespace, right?"}, {"timestamp": [1266.38, 1269.78], "text": " I might only want to look at maybe the Obscures namespace."}, {"timestamp": [1269.78, 1273.62], "text": " And so that really helps you cut down on some of that noise"}, {"timestamp": [1273.62, 1276.74], "text": " when you're trying to isolate an issue."}, {"timestamp": [1276.74, 1278.36], "text": " But going back to kind of our premise,"}, {"timestamp": [1278.36, 1280.46], "text": " what we're showing here is a mixture"}, {"timestamp": [1280.46, 1282.08], "text": " of quite a few different pieces of data."}, {"timestamp": [1282.08, 1284.36], "text": " You're showing the EVPF pieces."}, {"timestamp": [1284.36, 1290.46], "text": " Again, we talked about Cloud. So this demo happens to be running inside of AWS. But, you know, whatever cloud you're"}, {"timestamp": [1290.46, 1295.26], "text": " running on, you're going to have that PaaS layer very likely. So being able to collect that data"}, {"timestamp": [1295.26, 1299.94], "text": " and bring it all together to your Kubernetes environments, you know, and all monitor it in"}, {"timestamp": [1299.94, 1305.88], "text": " a single place is absolutely powerful. So if I click on, for example, that AWS RDS instance,"}, {"timestamp": [1306.3, 1309.04], "text": " you know, again, we're talking about the metrics. So if you look at this right side,"}, {"timestamp": [1309.36, 1314.22], "text": " we're collecting all those individual metrics, the read IOPS and the throughput, et cetera."}, {"timestamp": [1315.18, 1321.0], "text": " And this is a high level summary, but important is metrics, right? So I can go in here and look"}, {"timestamp": [1321.0, 1325.28], "text": " at all the individual metrics. That one of the pillars right over topability"}, {"timestamp": [1325.28, 1330.88], "text": " um and that's just that's just for one entity same thing for uh same thing for a pod right"}, {"timestamp": [1330.88, 1335.44], "text": " this is a pod in the container so if i click on a pod same thing i'm bringing back all this"}, {"timestamp": [1335.44, 1340.8], "text": " configuration data all these labels you know what time this was created uh what's what host it's"}, {"timestamp": [1340.8, 1344.0], "text": " running on it's important to understand all these things because when you're troubleshooting you"}, {"timestamp": [1344.0, 1345.02], "text": " know well when was i well you know, well, when was I,"}, {"timestamp": [1345.14, 1347.0], "text": " you know, what time was this pod running?"}, {"timestamp": [1347.08, 1348.92], "text": " It was supposed to have been restarted five minutes ago."}, {"timestamp": [1349.0, 1350.34], "text": " Did we actually perform the restart"}, {"timestamp": [1350.34, 1351.8], "text": " or was there an issue, you know,"}, {"timestamp": [1351.8, 1354.4], "text": " doing that rollout of the application?"}, {"timestamp": [1354.58, 1356.12], "text": " Well, look, it's been running for, you know,"}, {"timestamp": [1356.54, 1357.82], "text": " since a couple months ago."}, {"timestamp": [1358.18, 1361.46], "text": " So, hey, that rollout wasn't successful, right?"}, {"timestamp": [1362.18, 1365.06], "text": " Again, we've got metrics as well,"}, {"timestamp": [1365.28, 1368.5], "text": " and each entity has its own pieces of data,"}, {"timestamp": [1368.58, 1371.38], "text": " and it's important to be able to look at that data, again,"}, {"timestamp": [1371.48, 1375.54], "text": " in context for, you know, whatever problem you're troubleshooting."}, {"timestamp": [1376.0, 1378.06], "text": " In this scenario, I clicked on this container."}, {"timestamp": [1378.3, 1380.68], "text": " It happens to be the Jaeger agent, but I click on this container,"}, {"timestamp": [1380.82, 1382.88], "text": " and now I'm getting, you know, additional data that's contextual"}, {"timestamp": [1382.88, 1385.12], "text": " for that particular"}, {"timestamp": [1385.12, 1391.44], "text": " container, the ports that are being exposed. But on top of that, being able to see how the"}, {"timestamp": [1391.44, 1397.36], "text": " infrastructure is working, what things are related to what. So for example, we have these contextual"}, {"timestamp": [1397.36, 1402.4], "text": " access to these different pieces, right? So if I click on this three layer view, right, what it"}, {"timestamp": [1402.4, 1408.16], "text": " does is it shows me this particular container and this pod"}, {"timestamp": [1408.16, 1413.04], "text": " is running some details about it, the IP address, the image name that it's using, as well as some"}, {"timestamp": [1413.04, 1418.88], "text": " high-level metrics such as CPU and memory. But also, it shows me what Kubernetes node this"}, {"timestamp": [1418.88, 1423.36], "text": " particular container is running on, as well as some of the neighbors and those CPU and memory"}, {"timestamp": [1423.36, 1429.04], "text": " metrics for those neighbors. And then this Kubernetes node is running on top of what cloud instance, right? So when you're"}, {"timestamp": [1429.04, 1434.0], "text": " troubleshooting, I know I have some instances in, let's say you're running EKS and you have"}, {"timestamp": [1434.64, 1439.52], "text": " some nodes in one particular subnet or one particular availability zone that are having"}, {"timestamp": [1439.52, 1444.96], "text": " connectivity issues and you're trying to diagnose, right, in a single click, you can understand"}, {"timestamp": [1444.96, 1449.12], "text": " if your container happens to be running on one of those nodes and things"}, {"timestamp": [1449.12, 1452.26], "text": " like the region and how much storage is attached to it."}, {"timestamp": [1452.26, 1460.52], "text": " But not only that, again, as we mentioned, the ubiquitousness of all this data and the"}, {"timestamp": [1460.52, 1464.62], "text": " ease of collecting makes it really, really simple to bring it all together."}, {"timestamp": [1464.84, 1467.1], "text": " And now we can look at the infrastructure map that we call,"}, {"timestamp": [1467.1, 1468.72], "text": " which is essentially a cloud map."}, {"timestamp": [1468.72, 1470.12], "text": " And now we're looking in the context"}, {"timestamp": [1470.12, 1473.04], "text": " of this particular cloud instance,"}, {"timestamp": [1473.04, 1475.5], "text": " and we're looking at this EC2 virtual machine"}, {"timestamp": [1475.5, 1478.5], "text": " and looking at the configuration of that and the tags, right?"}, {"timestamp": [1478.5, 1481.76], "text": " And I'm just kind of showing behind the scenes,"}, {"timestamp": [1483.1, 1488.88], "text": " all the open source data that we're actually collecting and how even that open source"}, {"timestamp": [1488.88, 1490.84], "text": " data by itself makes it really powerful."}, {"timestamp": [1490.84, 1491.98], "text": " But once you combine the intelligence,"}, {"timestamp": [1491.98, 1493.38], "text": " which I'll talk about in a second,"}, {"timestamp": [1493.38, 1495.78], "text": " that's where things really start to take off."}, {"timestamp": [1495.78, 1497.62], "text": " But as we mentioned, we're collecting data"}, {"timestamp": [1497.62, 1501.64], "text": " from the Kubernetes API and from the container."}, {"timestamp": [1501.64, 1504.3], "text": " So that's where we're grabbing the individual container"}, {"timestamp": [1504.3, 1506.0], "text": " metrics and the node metrics."}, {"timestamp": [1507.52, 1510.56], "text": " We also have an understanding, for example, at a per node view, right?"}, {"timestamp": [1510.64, 1513.8], "text": " So instead of looking at it from a kind of application centered view,"}, {"timestamp": [1513.86, 1516.08], "text": " I can look at the node level."}, {"timestamp": [1516.5, 1517.72], "text": " Let's clear out some of these filters."}, {"timestamp": [1518.5, 1520.6], "text": " Now, so you see we have five nodes running,"}, {"timestamp": [1520.92, 1522.82], "text": " and now I'm looking at each individual node,"}, {"timestamp": [1523.1, 1528.4], "text": " and I can see the workloads that are running on top of that node I can click on metrics and get"}, {"timestamp": [1528.4, 1534.34], "text": " the metrics for that particular node so load in just a second but I'll go back"}, {"timestamp": [1534.34, 1547.04], "text": " and then we can actually look at the configuration for the particular node Yep. I have some filters on here by default. Real time internet issues."}, {"timestamp": [1548.64, 1554.48], "text": " Always fun. There we go. All right. So yeah, so again, we're collecting all the configuration"}, {"timestamp": [1554.48, 1559.28], "text": " and metadata, not only of the containers themselves, but even the nodes that you're"}, {"timestamp": [1559.28, 1565.36], "text": " running on. So things like the memory utilized, sorry, the memory capacity, whether the node is ready."}, {"timestamp": [1565.44, 1571.34], "text": " So you'll see here like max memory, max storage, what version of Kubernetes are they running?"}, {"timestamp": [1571.82, 1578.76], "text": " And so, you know, here we see that we're running version 117 of the Kubernetes node, which is actually probably a little bit outdated."}, {"timestamp": [1579.52, 1585.94], "text": " And the kernel version of the operating system that it's running on, et cetera. So we're bringing, again, all this data together,"}, {"timestamp": [1585.94, 1589.56], "text": " which is really, really empowered by all these open source layer tools."}, {"timestamp": [1589.72, 1590.78], "text": " We're not using custom agents."}, {"timestamp": [1590.9, 1592.66], "text": " We're not doing anything special."}, {"timestamp": [1592.8, 1594.48], "text": " It's just leveraging all this data,"}, {"timestamp": [1594.56, 1596.5], "text": " but bringing it all together in a single place."}, {"timestamp": [1597.94, 1600.62], "text": " On top of that, I mentioned it's important to cover things"}, {"timestamp": [1600.62, 1602.66], "text": " like PaaS services and serverless."}, {"timestamp": [1602.66, 1606.3], "text": " So, again, we also collect that kind of data."}, {"timestamp": [1606.3, 1609.66], "text": " So you'll notice here you saw an RDS instance."}, {"timestamp": [1609.66, 1611.84], "text": " I think I'm going to show a load balancer as well."}, {"timestamp": [1611.84, 1616.2], "text": " In this case, in this environment, I have an API gateway running with"}, {"timestamp": [1616.66, 1620.34], "text": " with an S3 call out actually via serverless."}, {"timestamp": [1620.8, 1622.92], "text": " So you'll see this API gateway."}, {"timestamp": [1622.92, 1626.0], "text": " And again, I'm grabbing the data from that particular API gateway,"}, {"timestamp": [1626.0, 1630.0], "text": " just like for the containers we saw that particular entities made it in for the nodes."}, {"timestamp": [1630.0, 1634.0], "text": " Now here's for the API gateway and some of the metrics as well."}, {"timestamp": [1634.0, 1636.0], "text": " And same thing for for the serverless functions. Right."}, {"timestamp": [1636.0, 1640.0], "text": " I can see the ARN of that particular serverless function, the region."}, {"timestamp": [1640.0, 1649.28], "text": " And I think we've got metrics down to that. So the whole point is to bring something that's all together. And finally, you know, actually, before I show that, I also did mention"}, {"timestamp": [1649.28, 1654.88], "text": " traces. And let me actually share this screen because I think I'm not sharing that."}, {"timestamp": [1657.44, 1663.52], "text": " I do want to show the traces before jumping on to something else. Here we go."}, {"timestamp": [1661.6, 1662.92], "text": " to something else."}, {"timestamp": [1663.64, 1665.42], "text": " There we go."}, {"timestamp": [1668.32, 1669.78], "text": " So again, we also have our trace map view that we just recently announced."}, {"timestamp": [1669.94, 1671.72], "text": " And so when you're leveraging, as we mentioned,"}, {"timestamp": [1671.84, 1672.66], "text": " distributed tracing,"}, {"timestamp": [1673.42, 1675.12], "text": " we can collect all that data, again,"}, {"timestamp": [1675.12, 1676.32], "text": " on a single space."}, {"timestamp": [1677.36, 1679.74], "text": " And now what we're doing is we're collecting"}, {"timestamp": [1679.74, 1681.24], "text": " the individual traces."}, {"timestamp": [1681.44, 1683.28], "text": " And actually, we're doing something pretty cool,"}, {"timestamp": [1683.36, 1685.56], "text": " which is what we call the trace map"}, {"timestamp": [1685.56, 1688.44], "text": " and identification of these trace paths."}, {"timestamp": [1689.42, 1689.98], "text": " Sorry, Alok."}, {"timestamp": [1690.42, 1691.76], "text": " I don't see your screen."}, {"timestamp": [1692.28, 1693.1], "text": " Oh, you don't see my screen?"}, {"timestamp": [1693.3, 1693.66], "text": " Sorry about that."}, {"timestamp": [1693.66, 1694.28], "text": " Can you share?"}, {"timestamp": [1694.42, 1696.62], "text": " Because I don't think they know what you mean by the trace."}, {"timestamp": [1696.9, 1697.84], "text": " Okay, great."}, {"timestamp": [1698.62, 1698.84], "text": " Okay."}, {"timestamp": [1699.46, 1700.84], "text": " And I did see one thing."}, {"timestamp": [1700.9, 1703.1], "text": " If you are able to bump up the text just a little bit,"}, {"timestamp": [1703.34, 1706.4], "text": " I saw a couple of comments about that as well."}, {"timestamp": [1707.4, 1711.12], "text": " Sure. Is this, hopefully this is a little bit better."}, {"timestamp": [1713.22, 1714.92], "text": " Yeah, I think, I think that should be good."}, {"timestamp": [1715.32, 1715.58], "text": " Okay."}, {"timestamp": [1715.58, 1716.42], "text": " Thank you for calling that up."}, {"timestamp": [1717.2, 1719.68], "text": " Gotcha. Let me know if there's still visibility issues,"}, {"timestamp": [1719.76, 1721.14], "text": " but I've bumped it up just a little bit."}, {"timestamp": [1722.26, 1723.4], "text": " Yeah. So again, we're just,"}, {"timestamp": [1723.52, 1725.08], "text": " I'm just showing off the tracing capabilities."}, {"timestamp": [1725.2, 1727.26], "text": " Again, just bringing everything all together in a single place."}, {"timestamp": [1727.58, 1730.72], "text": " You can see here this trace map showing the different interactions"}, {"timestamp": [1730.72, 1733.66], "text": " from the front end to the ad service to the product catalog service."}, {"timestamp": [1734.96, 1738.12], "text": " But one of the really cool things that is kind of unique"}, {"timestamp": [1738.12, 1742.9], "text": " that we've been able to develop is identifying, you know,"}, {"timestamp": [1743.0, 1746.04], "text": " a lot of times in tracing, you'll get transaction"}, {"timestamp": [1746.04, 1755.02], "text": " identifications. Oh, I'm seeing that. Hopefully, hopefully, this is a little bit better. I think"}, {"timestamp": [1755.02, 1763.12], "text": " I've hit the limit of my of my zooming in capabilities. Sorry, guys, I always was a"}, {"timestamp": [1763.12, 1766.4], "text": " little bit bigger. Hopefully, this is some sort of a mobile for you."}, {"timestamp": [1767.28, 1768.7], "text": " Okay, so we've got the traces."}, {"timestamp": [1768.86, 1770.34], "text": " We've got the trace maps."}, {"timestamp": [1772.62, 1777.94], "text": " And, oh, it looks like I'm getting some too much noise on my machine."}, {"timestamp": [1778.08, 1778.98], "text": " So, sorry about that."}, {"timestamp": [1779.0, 1779.94], "text": " I'm seeing that in the chat."}, {"timestamp": [1781.24, 1783.74], "text": " Hopefully, it turned off the notification sounds here."}, {"timestamp": [1783.84, 1785.68], "text": " Hopefully, that will stop interrupting. Okay, so we've got the notification sounds here. Hopefully that'll stop interrupting."}, {"timestamp": [1785.68, 1791.76], "text": " Okay, so we've got the trace map view, but we're also discovering what we call the trace"}, {"timestamp": [1791.76, 1799.38], "text": " paths. So these trace paths are not just sorry, guys, give me just one second. I'm trying"}, {"timestamp": [1799.38, 1800.38], "text": " to..."}, {"timestamp": [1800.38, 1802.38], "text": " We're still on Slack, guys. That's why."}, {"timestamp": [1802.38, 1803.38], "text": " Yeah, there's..."}, {"timestamp": [1803.38, 1805.28], "text": " Taylor, you know how this goes. on slack guys that's why yeah there's uh"}, {"timestamp": [1808.8, 1812.08], "text": " taylor you know how this goes oh absolutely absolutely"}, {"timestamp": [1816.56, 1820.48], "text": " i feel like as soon as anyone goes live that's there must be like a hidden button or something like that somewhere because that's that's what i started to get a lot of money we are being pinged"}, {"timestamp": [1820.48, 1825.04], "text": " on that so anyway i believe i've turned off not do not disturb successfully,"}, {"timestamp": [1825.04, 1831.0], "text": " which I thought I did before the call. I just shut down Slack. My apologies to everyone. Okay."}, {"timestamp": [1831.48, 1838.64], "text": " So let me head back here. Okay. You know, we have auto discovery, essentially, of not only the"}, {"timestamp": [1838.64, 1847.44], "text": " transactions themselves, which are used to seeing distributed tracing uh platforms but um we are also grabbing uh"}, {"timestamp": [1847.44, 1854.24], "text": " that identification of the paths themselves you might have a transaction you know for one of these"}, {"timestamp": [1854.24, 1859.68], "text": " uh products um that you know might be a slash checkout but you might have a different types"}, {"timestamp": [1859.68, 1866.3], "text": " of checkouts for maybe um uh a uh class right maybe you're selling a class on your e-commerce site"}, {"timestamp": [1866.3, 1867.38], "text": " versus a product, right?"}, {"timestamp": [1867.5, 1869.82], "text": " So even though, you know, they're both called checkout,"}, {"timestamp": [1869.94, 1871.02], "text": " one might go to AD service"}, {"timestamp": [1871.02, 1873.38], "text": " and another one might go to this checkout service"}, {"timestamp": [1873.38, 1874.58], "text": " and then product catalog service."}, {"timestamp": [1874.66, 1875.8], "text": " So even though they're both named the same,"}, {"timestamp": [1876.1, 1878.16], "text": " we identify those differences between them"}, {"timestamp": [1878.16, 1881.72], "text": " and then also perform automated anomaly detection"}, {"timestamp": [1881.72, 1884.6], "text": " and profile those transactions separately"}, {"timestamp": [1884.6, 1885.84], "text": " from each other"}, {"timestamp": [1885.84, 1891.36], "text": " right um so that is that you know that's some of the tracing we won't delve too too far into this"}, {"timestamp": [1891.36, 1896.8], "text": " because i want to show really some of the some of the magic behind um what we can do now that"}, {"timestamp": [1896.8, 1905.12], "text": " we have all that really rich open source data right so um let me stop sharing and reshare my other screen. Just give me a second here."}, {"timestamp": [1912.24, 1912.72], "text": " So,"}, {"timestamp": [1913.92, 1920.86], "text": " there we go. You guys should be seeing my screen pop up here in a second."}, {"timestamp": [1929.96, 1937.24], "text": " up here in a second. All right. All right. So, you know, some of the things that we can do now that we have all this open source data is that we can now start doing anomaly detection, detecting"}, {"timestamp": [1937.24, 1943.02], "text": " of misconfigurations, misbehaviors. You know, one of the things I actually did not show, if I go"}, {"timestamp": [1943.02, 1954.54], "text": " back here really quickly, is that we're also collecting configuration data, not only at this kind of high-level metadata kind of view, but we're also showing the entire manifest."}, {"timestamp": [1954.7, 1965.84], "text": " So if I click, and I'll just show what I did there, if I click on detailed view for this particular pod, right, now I'm looking at the actual manifest for this particular pod so I can look at the details of what exactly is going"}, {"timestamp": [1965.84, 1970.32], "text": " on throughout without having to go inside the command line and figure out you know get a cube"}, {"timestamp": [1970.32, 1976.56], "text": " ctl get pod dash oh yaml and this is uh this is way simpler and it also helps keep everything in"}, {"timestamp": [1976.56, 1982.8], "text": " context and keep you inside of a single place um but now with all this really rich data and on and"}, {"timestamp": [1982.8, 1995.36], "text": " knowing you know the other thing we do is we have what we call curated knowledge, because on top of all this, you do need to understand how these systems interoperate with each other and what kind of dependencies they have on each other."}, {"timestamp": [1995.46, 1999.58], "text": " That's why we do build that relationship view, leveraging all the data."}, {"timestamp": [2000.26, 2002.8], "text": " That's why we want to know what container is running on what pod."}, {"timestamp": [2003.22, 2006.94], "text": " I'm sorry, on what node and what node is running on top of what piece of infrastructure."}, {"timestamp": [2007.1, 2008.76], "text": " So we know when a piece of infrastructure is down,"}, {"timestamp": [2009.1, 2012.7], "text": " we know that it's affecting the container that's hosted on it."}, {"timestamp": [2013.72, 2017.88], "text": " And there's a lot of nuance and variance to the kind of problems that can arise."}, {"timestamp": [2018.3, 2023.08], "text": " But having, again, this richness of this open source data, it makes it all possible."}, {"timestamp": [2023.54, 2025.92], "text": " So I'll show a couple of things that we do."}, {"timestamp": [2026.38, 2031.32], "text": " Here, let me find an alert. I think I was looking at this alert a little bit earlier. So I'll explain"}, {"timestamp": [2031.32, 2035.26], "text": " a little bit what this is, right? So in this case, we have a deployment problem, right? On our"}, {"timestamp": [2035.26, 2040.02], "text": " particular web server deployment, we're supposed to have a total of three replicas. And in this"}, {"timestamp": [2040.02, 2043.14], "text": " case, and you know what, I'll bump up the text a little bit because I know that was asked before."}, {"timestamp": [2043.72, 2047.72], "text": " So we're supposed to have a total of three replicas. In this case, we've only got two"}, {"timestamp": [2047.72, 2052.34], "text": " available replicas, and this has been going on for a little bit. So down here, you know,"}, {"timestamp": [2052.34, 2056.2], "text": " we provide some details. It's part of the shopping cart namespace. It's the web server deployment."}, {"timestamp": [2056.58, 2061.4], "text": " And here's some, you know, additional kind of feel key value pair details, but we'll go to"}, {"timestamp": [2061.4, 2065.28], "text": " the fun view. I know some of you guys love reading JSON, but I kind of like"}, {"timestamp": [2066.16, 2073.12], "text": " the UI just a little bit more. So when I click on this analyze view, what it shows us is what we"}, {"timestamp": [2073.12, 2080.24], "text": " call the contextual RCA, which is our fishbone RCA, right? So in this case, what we're showing"}, {"timestamp": [2080.24, 2085.04], "text": " is we're showing failure categories across the top and bottom that are affecting"}, {"timestamp": [2085.04, 2089.28], "text": " this particular deployment so again all this is being collected just through the you know"}, {"timestamp": [2089.28, 2094.8], "text": " acquiring the kubernetes api and then the the relationship of the of collecting the events and"}, {"timestamp": [2094.8, 2099.92], "text": " the containers and linking those all those pieces together um so we have a replica set scaling issue"}, {"timestamp": [2099.92, 2106.34], "text": " right we're having a an issue scaling up an additional replica of that particular image."}, {"timestamp": [2106.82, 2109.7], "text": " And now we're getting actually a back off restart as well."}, {"timestamp": [2109.88, 2112.8], "text": " But this is all really associated to the startup failure."}, {"timestamp": [2113.06, 2116.6], "text": " And if I click on that, what it's going to tell me is that I have an invalid image name."}, {"timestamp": [2116.86, 2122.14], "text": " So obscures is spelled with one I, and it looks here like somebody spelled obscure with two I's."}, {"timestamp": [2122.54, 2123.98], "text": " And so that's a bad image name."}, {"timestamp": [2122.14, 2122.68], "text": " like somebody spelled obscrute with two I's."}, {"timestamp": [2123.98, 2125.24], "text": " And so that's a bad image name."}, {"timestamp": [2129.44, 2134.14], "text": " You know, it took us all of, what, you know, three, four seconds to figure out that one of our replicas isn't coming up because of a bad image name."}, {"timestamp": [2134.2, 2136.82], "text": " So it's those kinds of things, the richness of the data that allows us"}, {"timestamp": [2136.82, 2142.18], "text": " to build these really, really quick root cause analysis pieces"}, {"timestamp": [2142.18, 2144.66], "text": " into something like obscripts, right?"}, {"timestamp": [2144.74, 2146.82], "text": " So, yes, you can do this from the command line."}, {"timestamp": [2148.38, 2149.9], "text": " It's, you know, it's a little bit more work."}, {"timestamp": [2149.98, 2153.32], "text": " It'll probably take anywhere from, I don't know, 30 seconds to a couple of minutes."}, {"timestamp": [2153.32, 2156.8], "text": " But, you know, multiply this times 1,000 times 5,000."}, {"timestamp": [2156.98, 2157.92], "text": " That can happen in a month."}, {"timestamp": [2158.76, 2161.88], "text": " You know, that's a lot of time saved for operations teams, right?"}, {"timestamp": [2162.1, 2164.78], "text": " And you'll also notice other ones that some of these are more complex."}, {"timestamp": [2168.1, 2175.88], "text": " And, you know, these are just building blocks to what I'm going to show you in a sec of these individual kind of problem detections and anomaly detections. But you'll notice other"}, {"timestamp": [2175.88, 2180.76], "text": " categories. So things like a missing config map, right? If you reference a config map in your"}, {"timestamp": [2180.76, 2185.44], "text": " manifest that does not exist, you're going to have a failure of your pod."}, {"timestamp": [2185.54, 2190.02], "text": " So we'll highlight those things or failed volume mounts or even bad image tags. I think I might"}, {"timestamp": [2190.02, 2196.64], "text": " actually have a bad image tag in here that I was looking at. It's a very, very similar scenario,"}, {"timestamp": [2196.64, 2200.64], "text": " but for the cart server, if I click on analyze, yep, same kind of symptoms,"}, {"timestamp": [2201.1, 2205.12], "text": " replica set scaling issues, we're having back off restarts going to a crash loop."}, {"timestamp": [2205.12, 2210.48], "text": " But in this case, we have an invalid image tag. This particular image tag does not exist."}, {"timestamp": [2212.16, 2217.44], "text": " Now, the other thing that I didn't go too far into, but it really is absolutely key,"}, {"timestamp": [2218.08, 2227.92], "text": " is machine learning. So from all the individual services that you deploy onto your clusters,"}, {"timestamp": [2228.24, 2231.64], "text": " what happens is that with that data being collected from C-Advisor"}, {"timestamp": [2231.64, 2234.96], "text": " and from Nodex program, from the discovery pieces,"}, {"timestamp": [2234.96, 2242.34], "text": " what we do is we create a really rich behavior model, right?"}, {"timestamp": [2242.4, 2248.9], "text": " We detect what is normal behavior for your individual services, right? So"}, {"timestamp": [2248.9, 2255.5], "text": " if you are, you know, we don't just look at one or two metrics like error rates and response time,"}, {"timestamp": [2255.52, 2259.74], "text": " but we look actually each one of the entities that I've shown you have their own behavior"}, {"timestamp": [2259.74, 2262.28], "text": " models. And there's a bunch of others that I didn't show you as part of this demo, but things"}, {"timestamp": [2262.28, 2268.84], "text": " like if you're using database like MongoDB or a JVM or an Nginx container, right?"}, {"timestamp": [2269.1, 2273.74], "text": " And then the generic containers themselves, the nodes themselves, they all have their own behavior models."}, {"timestamp": [2274.4, 2278.76], "text": " And we pick up a mixture of a lot of different metrics to understand what is normal behavior."}, {"timestamp": [2279.3, 2285.12], "text": " And then when we find what is abnormal, we have these types of alerts that are prefixed by ML telling us"}, {"timestamp": [2285.12, 2290.56], "text": " that there is some sort of ML detected performance violation, right? So if I click on this,"}, {"timestamp": [2291.52, 2295.6], "text": " in this scenario, you know, again, I'm going to get some details as to what happened, right? I"}, {"timestamp": [2295.6, 2300.32], "text": " get, you know, I know when we zoom in a bit, network transmittal bytes increased by 540%"}, {"timestamp": [2300.96, 2307.4], "text": " and level four bytes for the outbound traffic increase and inbound transmittal bytes decreased,"}, {"timestamp": [2307.4, 2311.96], "text": " actually. So we don't only detect increases, but also abnormal decreases as well."}, {"timestamp": [2311.96, 2318.6], "text": " But just like in the other scenarios, if I click on analyze, I can get a fishbone representation"}, {"timestamp": [2318.6, 2326.62], "text": " of what exactly is going on with the metrics and why the ML in the first place triggered an anomaly. And so I'm going to"}, {"timestamp": [2326.62, 2333.24], "text": " zoom out just one piece. Just like you saw for the Kubernetes specific deployment scenarios,"}, {"timestamp": [2333.64, 2339.52], "text": " now in this phase one RCA, we're looking at a container view. This particularly the card cache"}, {"timestamp": [2339.52, 2346.62], "text": " had some deviation in its metrics. And actually before looking at this,"}, {"timestamp": [2346.62, 2349.26], "text": " I'm gonna go back just to the summary screen"}, {"timestamp": [2349.26, 2352.48], "text": " and show you down here, if I click more details,"}, {"timestamp": [2352.48, 2353.92], "text": " you know, speaking about the ML"}, {"timestamp": [2353.92, 2355.08], "text": " and all the metrics we take,"}, {"timestamp": [2355.08, 2356.32], "text": " these are all the different metrics"}, {"timestamp": [2356.32, 2358.38], "text": " for just the generic container model"}, {"timestamp": [2358.38, 2359.94], "text": " that we're looking at, right?"}, {"timestamp": [2359.94, 2362.16], "text": " So again, it's not just one or two or three metrics."}, {"timestamp": [2362.16, 2363.78], "text": " We're looking at transmittal bytes,"}, {"timestamp": [2363.78, 2366.98], "text": " packets in, packets out, memory failures, CPU utilization."}, {"timestamp": [2367.34, 2373.1], "text": " All this data, particularly for the container, is, again, provided by C-Advisor, an open source tool, right?"}, {"timestamp": [2373.48, 2382.04], "text": " So, again, going back to the analyze button, now we're seeing the actual pieces that actually triggered the ML."}, {"timestamp": [2382.46, 2385.64], "text": " And so now you'll notice that the fishbone has changed from our startup failure."}, {"timestamp": [2386.12, 2389.8], "text": " Now we're showing memory and file system and CPU."}, {"timestamp": [2390.32, 2392.08], "text": " And so right away, we'll show you in red."}, {"timestamp": [2392.12, 2394.16], "text": " You don't have to go and look at a chart for this specific thing."}, {"timestamp": [2394.28, 2395.54], "text": " It's here, right?"}, {"timestamp": [2395.58, 2399.04], "text": " So I'm seeing CPU utilization has increased by close to 50%."}, {"timestamp": [2399.04, 2403.12], "text": " I'm looking at demand, which is incoming requests."}, {"timestamp": [2403.12, 2406.08], "text": " The response time has increased by over 1,700%."}, {"timestamp": [2406.08, 2409.08], "text": " I'm looking at outbound supply side."}, {"timestamp": [2409.5, 2413.86], "text": " Response time has increased by 2,200% for outbound requests from PartCash."}, {"timestamp": [2414.4, 2420.24], "text": " And then not only that, our response size has increased from one to close to eight megs, right?"}, {"timestamp": [2420.3, 2424.5], "text": " And then bringing in the Kubernetes layer, it's this whole image change, right?"}, {"timestamp": [2424.5, 2431.22], "text": " So again, bringing in the data from the Kubernetes API, I can see that I've got a recent image change that's likely contributing to this failure."}, {"timestamp": [2432.12, 2440.32], "text": " Now, again, I'm glossing over a few details because in the interest of time, I want to show you guys how we bring, you know, a couple of these things together."}, {"timestamp": [2440.44, 2448.66], "text": " This is, you know, an ML alert. And by the way, you can see this automatically chart those important metrics down here. So you can see their behavior during the time of the anomaly."}, {"timestamp": [2449.04, 2455.14], "text": " And as mentioned, you know, you can drill down into any logs that might be coming in. Actually,"}, {"timestamp": [2455.22, 2460.38], "text": " I should probably show that. I don't think I did. So here's another example of an anomaly database"}, {"timestamp": [2460.38, 2470.04], "text": " server. And you'll notice here that you have different contextual access, right? Application state, I'll show that. We have a time travel capability where with all this data,"}, {"timestamp": [2470.22, 2475.44], "text": " the metric data from Prometheus, the log data from Fluent, from Loki, the trace data, all this,"}, {"timestamp": [2475.56, 2480.76], "text": " you know, we build that real-time map that you guys saw and all the configuration data."}, {"timestamp": [2481.16, 2484.26], "text": " We take snapshots every five minutes, and I'll show you guys that in a second, but you can go"}, {"timestamp": [2484.26, 2488.9], "text": " back in time at the time how your system was configured during the time of this particular"}, {"timestamp": [2488.9, 2494.18], "text": " anomaly. In this case, this goes back a day. So if I click that, it'll take me back into one day"}, {"timestamp": [2494.18, 2498.94], "text": " before and show me the entire config of my entire state at that time. But we'll go into that in a"}, {"timestamp": [2498.94, 2503.34], "text": " second. I can click on metrics to understand the metrics for that database server or any events"}, {"timestamp": [2503.34, 2506.34], "text": " that are related to that. In this case, I want to show logs. So if I just click on metrics to understand the metrics for that database server or any events that are related to that. In this case, I wanna show logs."}, {"timestamp": [2506.34, 2508.94], "text": " So if I just click on the pod or the container logs,"}, {"timestamp": [2509.9, 2511.54], "text": " oh, maybe it wasn't logging there,"}, {"timestamp": [2511.54, 2514.62], "text": " but I do wanna show that we have contextual access"}, {"timestamp": [2514.62, 2515.62], "text": " to the logs actually."}, {"timestamp": [2515.62, 2518.42], "text": " Let me find, I just wanna show"}, {"timestamp": [2518.42, 2520.46], "text": " because I think we did not actually show logs."}, {"timestamp": [2520.46, 2524.38], "text": " Let me see, I think maybe note exporter will be logged."}, {"timestamp": [2524.38, 2525.24], "text": " Sorry."}, {"timestamp": [2525.24, 2528.32], "text": " And Cesar, I saw a couple questions come in on that front."}, {"timestamp": [2528.32, 2532.7], "text": " One question was, does OpsCruise enable custom metrics?"}, {"timestamp": [2532.7, 2534.24], "text": " Does OpsCruise enable custom metrics?"}, {"timestamp": [2534.24, 2535.24], "text": " Yes."}, {"timestamp": [2535.24, 2540.8], "text": " Basically, it's any data that's being exposed to Prometheus."}, {"timestamp": [2540.8, 2545.76], "text": " So as long as data is being exposed to Prometheus, that data can be brought into OpsCrews."}, {"timestamp": [2545.76, 2549.24], "text": " We're just leveraging Prometheus as a metric ingestion point."}, {"timestamp": [2549.24, 2550.24], "text": " Awesome."}, {"timestamp": [2550.24, 2554.96], "text": " And then one other question was, do the ML alerts wait for a particular size of training"}, {"timestamp": [2554.96, 2557.62], "text": " data before alerting on that front?"}, {"timestamp": [2557.62, 2558.9], "text": " So I can answer that."}, {"timestamp": [2558.9, 2564.26], "text": " So typically it depends on, our default is about one day, but we can speed it up so we"}, {"timestamp": [2564.26, 2566.94], "text": " can learn. And the only reason I'll say that is"}, {"timestamp": [2566.94, 2569.46], "text": " because let's say there was hardly any activity"}, {"timestamp": [2569.46, 2570.76], "text": " on a weekend, he deployed it."}, {"timestamp": [2570.76, 2572.56], "text": " You're not gonna see a lot of activity to profile it,"}, {"timestamp": [2572.56, 2574.36], "text": " but the next day it starts increasing."}, {"timestamp": [2574.36, 2576.42], "text": " So over time, essentially we update that,"}, {"timestamp": [2576.42, 2579.98], "text": " but our default is 24 hours can be even less."}, {"timestamp": [2579.98, 2581.16], "text": " Gonna make it even a few hours."}, {"timestamp": [2581.16, 2583.44], "text": " Just have enough data to get an initial profile"}, {"timestamp": [2583.44, 2592.32], "text": " and then we continuously update that. cool thank you no the users don't have to do anything i wish i could learn that fast"}, {"timestamp": [2592.32, 2596.72], "text": " yeah accelerate like one day is good but yeah if i could learn like this you saw that generic"}, {"timestamp": [2596.72, 2601.92], "text": " container example that says there's about 30 metrics and you have no way of knowing because"}, {"timestamp": [2601.92, 2609.92], "text": " there's some calls being made as a problem versus the memory failure suddenly increases so there is no way a person can do that that's what"}, {"timestamp": [2609.92, 2616.96], "text": " the beauty of the using the ml to get unknown numbers keep going yeah absolutely um so the"}, {"timestamp": [2616.96, 2620.48], "text": " the thing i wanted to show is logs because i absolutely did not not show that even though"}, {"timestamp": [2620.48, 2625.06], "text": " it's super important but um any anything that logging, right, we pick that up from your"}, {"timestamp": [2625.06, 2631.84], "text": " standard out. But if you click on any, whether it's an anomaly, and this is showing the pod,"}, {"timestamp": [2631.96, 2636.44], "text": " right, I have a pod open. And from its, this is what we call the quick view, right? From its quick"}, {"timestamp": [2636.44, 2641.6], "text": " view, one of the links you have is for logs, right? So I can just click on logs. And that"}, {"timestamp": [2641.6, 2644.78], "text": " takes me straight into the logs for that particular service. Now, this is pretty,"}, {"timestamp": [2645.04, 2651.12], "text": " click on logs and that takes me straight into the logs for that particular service now this is pretty um you know uh static logs here but i can you know we it is searchable right so i can look for"}, {"timestamp": [2651.12, 2658.4], "text": " requests for example or conversion yeah the idea is to contextually link it to the problem correct"}, {"timestamp": [2658.4, 2664.16], "text": " yes uh so you know depending where we get it from yeah correct yeah i i think i don't have a problem"}, {"timestamp": [2664.16, 2669.44], "text": " here that has logs right now but um but we surface that as well. So if you're having an anomaly, you can go straight"}, {"timestamp": [2669.44, 2674.08], "text": " into the logs for when they're active and that'll show. Now, what I do want to show with all this"}, {"timestamp": [2674.08, 2681.36], "text": " really put together is I'm going to show you an alert, right? So we have, I showed you guys,"}, {"timestamp": [2681.36, 2690.4], "text": " you know, how we collect all the different data, the architecture, right, again, we're leveraging just purely open source tools here to collect the data from, you know,"}, {"timestamp": [2690.4, 2694.88], "text": " whether it's VMs, or whether it's Kubernetes, et cetera, from the application level, MongoDB"}, {"timestamp": [2694.88, 2699.52], "text": " exporters, or Nginx exporters, as well as the traces, whatever, open telemetry, compatible"}, {"timestamp": [2699.52, 2707.8], "text": " libraries, all basically built on open source. But now, right, what we have here is, again, I also showed"}, {"timestamp": [2707.8, 2711.88], "text": " you the anomalies on how we like specific Kubernetes detection, and then I showed you the ML,"}, {"timestamp": [2712.28, 2716.54], "text": " how we automatically detect performance deviation, and again, lots of different metrics."}, {"timestamp": [2716.98, 2721.68], "text": " So in this scenario, we're kind of tying everything together, right? So I have a response time SLO"}, {"timestamp": [2721.68, 2725.3], "text": " breach on my Nginx service. So I'm going to click on that."}, {"timestamp": [2726.4, 2728.02], "text": " And again, here's some details, right?"}, {"timestamp": [2728.06, 2729.5], "text": " I have an SLO of five seconds."}, {"timestamp": [2729.86, 2731.68], "text": " My response time is at over 15 seconds."}, {"timestamp": [2732.0, 2734.0], "text": " So I want to see what's going on, right?"}, {"timestamp": [2734.02, 2737.92], "text": " If I click on Analyze, what this is going to do, and I'm going to close this."}, {"timestamp": [2737.92, 2739.0], "text": " We'll come back to this summary in a second."}, {"timestamp": [2739.04, 2739.82], "text": " I'm going to close that piece."}, {"timestamp": [2740.54, 2748.14], "text": " What this is doing is now we're showing a slice of the actual app map that we were looking at earlier."}, {"timestamp": [2748.28, 2754.72], "text": " But now it's focused on the timeframe and in the context of this particular anomaly."}, {"timestamp": [2755.06, 2757.3], "text": " So your route is essentially here."}, {"timestamp": [2757.48, 2758.96], "text": " At NGINX, you're seeing a slowdown."}, {"timestamp": [2759.46, 2762.5], "text": " But we've also identified what downstream services are involved."}, {"timestamp": [2762.82, 2769.94], "text": " So we have NGINX itself, right? So this is the Kubernetes service, the pod, the container, and same thing, service, pod, container. For web"}, {"timestamp": [2769.94, 2775.1], "text": " server, Redis service, Redis pod, Redis container. They've got a cart server, service, pod, and"}, {"timestamp": [2775.1, 2778.8], "text": " container. You'll notice immediately in the red, we've highlighted. So we're doing fault domain"}, {"timestamp": [2778.8, 2783.3], "text": " isolation as well. You don't have to call the NGINX microservice team, whoever's managing that."}, {"timestamp": [2783.36, 2786.42], "text": " You don't have to call the web server microservice team, whoever's managing that."}, {"timestamp": [2786.58, 2788.52], "text": " Could be the same team, could be a couple of different teams."}, {"timestamp": [2788.76, 2789.7], "text": " You don't have to reach out to them."}, {"timestamp": [2789.74, 2792.64], "text": " You don't have to go inside your tools and look at the metrics for these parts."}, {"timestamp": [2793.02, 2795.34], "text": " We're showing you they're healthy, right?"}, {"timestamp": [2795.44, 2805.12], "text": " So what the data has shown us from the data we've collected from these containers, as well as from the network data and the configuration data and combined with RML, that intelligent layer of"}, {"timestamp": [2805.12, 2811.76], "text": " the operations is we've highlighted the red pieces, right? So our container for Redis is red,"}, {"timestamp": [2812.08, 2816.12], "text": " our card server service is red, and so are our podding containers. So we'll kind of take this"}, {"timestamp": [2816.12, 2820.14], "text": " in the chain and see what's going on. So we've identified we have an SLO failure up here."}, {"timestamp": [2820.42, 2824.66], "text": " We're responding really, really slow. Now, if I click on the next piece in the chain, I'm showing"}, {"timestamp": [2824.66, 2826.8], "text": " that Redis container is problematic."}, {"timestamp": [2826.8, 2828.92], "text": " If I click on that, what it's going to do,"}, {"timestamp": [2828.92, 2830.62], "text": " it's going to show us this is a separate,"}, {"timestamp": [2830.62, 2832.92], "text": " technically a separate anomaly from the Nginx one,"}, {"timestamp": [2832.92, 2836.16], "text": " but the ML has detected that this is very much related."}, {"timestamp": [2836.16, 2837.92], "text": " You'll see a few different failures."}, {"timestamp": [2837.92, 2840.0], "text": " You'll see that we're getting"}, {"timestamp": [2840.0, 2842.78], "text": " an increase in throttling on the CPU."}, {"timestamp": [2842.78, 2846.1], "text": " The user second solders spent on the CPU has increased by"}, {"timestamp": [2846.1, 2851.5], "text": " about 10%. But really interesting actually here is you'll notice the response time normally is at"}, {"timestamp": [2851.5, 2857.34], "text": " 2.94 milliseconds. Right now we're at over two seconds. This was automatically detected. And"}, {"timestamp": [2857.34, 2862.14], "text": " then also super important is our error rate, right? Basically you usually have zero errors."}, {"timestamp": [2862.34, 2865.34], "text": " Right now our error rate has jumped up sorry to 36"}, {"timestamp": [2865.34, 2869.12], "text": " out of every single basically every single request has essentially got into"}, {"timestamp": [2869.12, 2873.5], "text": " an error mode so something is wrong so we're gonna we're gonna go back here and"}, {"timestamp": [2873.5, 2878.0], "text": " and just see what our RCA is pointing at so Redis is calling card server now if I"}, {"timestamp": [2878.0, 2882.32], "text": " click on card server I'm gonna see the alert very very clear this service the"}, {"timestamp": [2882.32, 2890.4], "text": " card server doesn't have any pod to serve requests. That's a very, very clear indicator that obviously Redis is experiencing a bunch of"}, {"timestamp": [2890.4, 2894.74], "text": " response time failures and now error rate failures because there are no requests behind"}, {"timestamp": [2894.74, 2900.26], "text": " this Kubernetes pod, sorry, no pods behind this service to serve any requests. And to look into"}, {"timestamp": [2900.26, 2904.86], "text": " just a little bit more detail, if I click on this particular pod now on the cart server to see if I"}, {"timestamp": [2904.86, 2907.18], "text": " can glean what's going on, it looks like I'm having a back off restart. detail if I click on this particular pod now on the cart server to see if I can glean what's going on looks like I'm having a back"}, {"timestamp": [2907.18, 2911.68], "text": " off restart and if I look at the details of that alert it'll actually show me a"}, {"timestamp": [2911.68, 2914.52], "text": " little bit more detail again these are all separate but linked together"}, {"timestamp": [2914.52, 2919.28], "text": " problems if I click on the analyze tab now it's going to show me the real root"}, {"timestamp": [2919.28, 2923.28], "text": " cons right we've got an invalid image name as we talked about earlier so this"}, {"timestamp": [2923.28, 2931.12], "text": " broken image name with two eyes in India India, this is, I'll zoom in a little bit so you can see that."}, {"timestamp": [2932.08, 2933.02], "text": " It doesn't look much."}, {"timestamp": [2934.6, 2938.4], "text": " But you can see that Obscures India here showing two I's, right?"}, {"timestamp": [2938.4, 2943.96], "text": " So we have this invalid image name, and that's really, you know, the root cause of the issue."}, {"timestamp": [2944.12, 2946.72], "text": " And it's all shown right here in a"}, {"timestamp": [2946.72, 2953.22], "text": " matter of seconds. Our engine is experiencing a response time slowdown. Redis is saying that"}, {"timestamp": [2953.22, 2958.44], "text": " we've got an increase in response time and errors. Card server is saying, well, I don't have any pods"}, {"timestamp": [2958.44, 2963.1], "text": " to serve. And the pod itself is saying, well, I can't start because somebody gave me a bad image"}, {"timestamp": [2963.1, 2967.62], "text": " name. All this in a matter of, you know, about 20 seconds, right?"}, {"timestamp": [2967.66, 2970.46], "text": " It took me obviously close to a minute, minute and a half to explain."}, {"timestamp": [2970.92, 2979.42], "text": " But all this understanding the Kubernetes level up to the application level and how they are affecting each other is all powered by these open source tools."}, {"timestamp": [2980.0, 2983.96], "text": " Plus, you know, the intelligent layer on top, which is, in my opinion, pretty darn cool."}, {"timestamp": [2984.96, 2985.0], "text": " In the interest of time, there are things I wanted to show. I did want to show time travel, which is, in my opinion, pretty darn cool."}, {"timestamp": [2986.86, 2986.88], "text": " In the interest of time, there are things I wanted to show."}, {"timestamp": [2989.48, 2989.52], "text": " I did want to show time travel, but I think we're pretty close out of time."}, {"timestamp": [2990.66, 2991.26], "text": " I want to open it up for questions."}, {"timestamp": [2993.52, 2994.2], "text": " So with that, I'll turn it back to you, Alok and Tyler."}, {"timestamp": [2996.68, 3000.66], "text": " Yeah, so I think we should go back to Q&A. So if I were just to summarize, kind of back to the premise we started with, right?"}, {"timestamp": [3001.34, 3007.36], "text": " In order to really help ops in this new cloud native microservice environment and"}, {"timestamp": [3007.36, 3012.48], "text": " kubernetes you know we have no longer to worry about where the data is coming from all the"}, {"timestamp": [3012.48, 3017.44], "text": " telemetry is there the idea is to build it but really what observer already has to do to have"}, {"timestamp": [3017.44, 3022.72], "text": " this intelligence is able to understand the full context of the application of course everything"}, {"timestamp": [3022.72, 3025.68], "text": " across all dependency track that users should"}, {"timestamp": [3025.68, 3029.84], "text": " not have to do that so that's what we need to build in and then understand the application"}, {"timestamp": [3029.84, 3034.24], "text": " profile the behavior so they don't have to worry about how to detect problem setting thresholds"}, {"timestamp": [3034.24, 3039.28], "text": " we want to take that off the table and then contextually analyze everything because now"}, {"timestamp": [3039.84, 3044.64], "text": " we have rich data in this whole distributed systems that whether it's an infrastructure"}, {"timestamp": [3044.64, 3045.94], "text": " or kubernetes related or down to the application they all link together and don't you don't need We have rich data in this whole distributed systems, and whether it's in infrastructure or Kubernetes related"}, {"timestamp": [3045.94, 3048.76], "text": " or down to the application, they all link together."}, {"timestamp": [3048.76, 3051.76], "text": " And you don't need six different folks"}, {"timestamp": [3051.76, 3055.12], "text": " looking at traces, logs, events, alerts to do that."}, {"timestamp": [3055.12, 3058.78], "text": " That's the role of observability in this new world."}, {"timestamp": [3058.78, 3062.26], "text": " And thanks to open telemetry and open source,"}, {"timestamp": [3062.26, 3063.76], "text": " it is possible to do that."}, {"timestamp": [3063.76, 3065.62], "text": " So think of us as the proof point."}, {"timestamp": [3066.06, 3069.96], "text": " You don't need to worry about running multiple siloed tools"}, {"timestamp": [3069.96, 3073.38], "text": " to really build that intelligence and reduce the amount of effort needed."}, {"timestamp": [3073.88, 3074.62], "text": " I'll pause there."}, {"timestamp": [3074.94, 3076.94], "text": " That was the whole point of, you know, guys,"}, {"timestamp": [3076.94, 3081.92], "text": " take advantage of the open telemetry and the open source tooling"}, {"timestamp": [3081.92, 3083.42], "text": " that CNC has been helping."}, {"timestamp": [3083.76, 3085.28], "text": " We are firm believers in that,"}, {"timestamp": [3085.4, 3087.18], "text": " and we hope you can leverage it too."}, {"timestamp": [3088.44, 3090.44], "text": " Awesome. Awesome. Thank you so much."}, {"timestamp": [3090.7, 3092.36], "text": " I did see quite a few questions come in."}, {"timestamp": [3092.4, 3093.24], "text": " I see at least three."}, {"timestamp": [3093.8, 3095.86], "text": " One question, the first one from Ishmael,"}, {"timestamp": [3095.86, 3098.3], "text": " was how easy is the installation?"}, {"timestamp": [3100.52, 3101.7], "text": " That's a great question."}, {"timestamp": [3102.3, 3104.76], "text": " Let me see if I have it in this environment here."}, {"timestamp": [3108.18, 3114.28], "text": " That's a great question. Let me see if I have it in this environment here. So for a typical deployment into maybe an on-prem cluster, we leverage Helm. Right? Again, another open"}, {"timestamp": [3114.28, 3119.8], "text": " source tool. So we leverage Helm. It's these commands. It's about three or four, well,"}, {"timestamp": [3119.8, 3120.8], "text": " it's actually five commands."}, {"timestamp": [3120.8, 3123.24], "text": " And Sarah, could you bump that up just a little bit?"}, {"timestamp": [3123.24, 3130.96], "text": " Yeah, I absolutely will. Thank you. So essentially, you know, if you don't have these, if you have these existing tools,"}, {"timestamp": [3130.96, 3135.0], "text": " because a lot of people, as I mentioned, you know, these are the essentially the de facto"}, {"timestamp": [3135.0, 3138.96], "text": " standard for open source monitoring and all these modern environments. Most of the people we run"}, {"timestamp": [3138.96, 3143.2], "text": " into already have these tools. So it's actually a little bit simpler. But if you don't have these"}, {"timestamp": [3143.2, 3153.0], "text": " tools, we absolutely, you know, this, I think it's this last command that will deploy, you know, all the open source tools if you don't have them underneath already."}, {"timestamp": [3153.0, 3155.0], "text": " Essentially, it's through Helm, right?"}, {"timestamp": [3155.0, 3161.0], "text": " These commands, you know, these five commands, and that gets you from a greenfield cluster to up and running."}, {"timestamp": [3161.0, 3168.62], "text": " Literally, I mean, copy and paste, I mean, you're up and running in about, I don't know, three minutes, four minutes, and you have the entire environment that I showed."}, {"timestamp": [3168.72, 3172.98], "text": " The only thing that's not available right off the bat is the ML because, you know, again,"}, {"timestamp": [3173.02, 3177.16], "text": " it takes a couple of, anywhere from a, I've seen ML alerts come back in a couple of hours"}, {"timestamp": [3177.16, 3183.06], "text": " to, you know, 24 hours. This is usually that sweet spot. But everything else that you saw"}, {"timestamp": [3183.06, 3186.24], "text": " within, you know, three to five minutes of deploying, you're getting"}, {"timestamp": [3186.24, 3187.24], "text": " all that data."}, {"timestamp": [3187.24, 3189.24], "text": " So this is how simple it is."}, {"timestamp": [3189.24, 3190.24], "text": " Awesome."}, {"timestamp": [3190.24, 3191.24], "text": " Awesome."}, {"timestamp": [3191.24, 3192.24], "text": " Thank you so much."}, {"timestamp": [3192.24, 3196.8], "text": " The next question I saw was, is it free or what levels?"}, {"timestamp": [3196.8, 3197.8], "text": " How does it work?"}, {"timestamp": [3197.8, 3200.1], "text": " Is it a SaaS or is it something you can host on your own?"}, {"timestamp": [3200.1, 3202.1], "text": " It is a SaaS service, yes."}, {"timestamp": [3202.1, 3203.1], "text": " Cool."}, {"timestamp": [3203.1, 3204.1], "text": " Cool."}, {"timestamp": [3204.1, 3207.22], "text": " Absolutely. There is a freemium offering for people who want to try it out."}, {"timestamp": [3208.18, 3211.54], "text": " So you can go to our website, opsfeeds.com, and check it out."}, {"timestamp": [3212.94, 3214.66], "text": " Cool, cool. Thank you."}, {"timestamp": [3215.24, 3217.2], "text": " Next question. We've got lots of questions coming in."}, {"timestamp": [3217.28, 3218.94], "text": " Thank you, everybody, so much for submitting those."}, {"timestamp": [3219.42, 3221.82], "text": " Please keep them coming. I think we have about seven minutes,"}, {"timestamp": [3221.92, 3224.98], "text": " so I'm happy to field those as much as possible."}, {"timestamp": [3225.76, 3226.84], "text": " The next question is,"}, {"timestamp": [3226.84, 3229.22], "text": " is it a good idea to export this data"}, {"timestamp": [3229.22, 3232.4], "text": " to do offline troubleshooting by importing data collected?"}, {"timestamp": [3232.4, 3234.94], "text": " I was wondering about troubleshooting edge deployment cases"}, {"timestamp": [3234.94, 3237.04], "text": " where we don't have access to the cluster."}, {"timestamp": [3238.18, 3239.02], "text": " Interesting."}, {"timestamp": [3239.92, 3243.12], "text": " So you're talking about when you don't have,"}, {"timestamp": [3243.12, 3245.52], "text": " if you can collect those metrics, you're saying,"}, {"timestamp": [3245.52, 3249.28], "text": " and push it to us, it's a little trickier with this,"}, {"timestamp": [3249.28, 3250.52], "text": " and depending on the context,"}, {"timestamp": [3250.52, 3252.64], "text": " maybe you have to dig into a little more specifics,"}, {"timestamp": [3252.64, 3254.76], "text": " you know, what data, because remember,"}, {"timestamp": [3254.76, 3256.42], "text": " not understand the application context,"}, {"timestamp": [3256.42, 3259.42], "text": " we pull everything, we may be able to see the dependencies."}, {"timestamp": [3259.42, 3262.0], "text": " So seeing it in isolation doesn't tell you that."}, {"timestamp": [3262.0, 3264.4], "text": " So it would probably be specific,"}, {"timestamp": [3264.4, 3265.04], "text": " so we can take this"}, {"timestamp": [3265.04, 3270.72], "text": " offline and just if this uh you know attendee has something specific that we can follow up"}, {"timestamp": [3270.72, 3276.16], "text": " and ping us yeah i will mention you know it you know it depends on how your edges it just"}, {"timestamp": [3276.16, 3282.8], "text": " compared like if you flat out don't have like um access to to like export metrics right i mean"}, {"timestamp": [3282.8, 3286.06], "text": " again you know obscures itself isn't really doing much"}, {"timestamp": [3286.06, 3287.36], "text": " on the collection side."}, {"timestamp": [3287.44, 3288.56], "text": " It's really around"}, {"timestamp": [3288.56, 3291.66], "text": " having Prometheus on the cluster"}, {"timestamp": [3291.66, 3293.8], "text": " and having Loki or FluentD"}, {"timestamp": [3293.8, 3295.4], "text": " on the cluster to collect that data."}, {"timestamp": [3296.08, 3298.02], "text": " Really, if you don't have access to there,"}, {"timestamp": [3298.3, 3300.02], "text": " that's something to be explored."}, {"timestamp": [3300.38, 3302.06], "text": " But speaking about Edge itself,"}, {"timestamp": [3302.58, 3304.6], "text": " we have recently published"}, {"timestamp": [3304.6, 3307.44], "text": " a joint blog with Verizon where we're talking about."}, {"timestamp": [3307.58, 3309.52], "text": " And again, I don't know what's going on with my D&D button."}, {"timestamp": [3309.98, 3310.76], "text": " I know somebody mentioned it."}, {"timestamp": [3310.86, 3312.28], "text": " I don't know what's going on with my Do Not Disturb."}, {"timestamp": [3312.32, 3313.46], "text": " I promise I turned it on."}, {"timestamp": [3314.12, 3330.88], "text": " But anyway, you know, when you're running Kubernetes clusters, for example, at the Edge or running workloads at the Edge, it absolutely is a supported model right again that joint blog where i mentioned is is uh is launching a kubernetes cluster on aws wavelength and um"}, {"timestamp": [3330.88, 3336.32], "text": " you know with kubernetes and uh observability built in with you know with obstacles and you"}, {"timestamp": [3336.32, 3339.92], "text": " know it functions perfectly fine but again if you have like a really really locked down edge and"}, {"timestamp": [3339.92, 3344.0], "text": " that might be something we can you know talk offline feel free to reach out we can talk about"}, {"timestamp": [3344.0, 3345.18], "text": " that scenario."}, {"timestamp": [3346.34, 3348.22], "text": " It's a definitely use case because a lot of the edge applications"}, {"timestamp": [3348.22, 3349.44], "text": " are not deploying in Kubernetes,"}, {"timestamp": [3349.64, 3351.14], "text": " so we are playing into that space."}, {"timestamp": [3352.78, 3354.9], "text": " Awesome. Awesome. Thank you."}, {"timestamp": [3355.48, 3357.16], "text": " Next question is,"}, {"timestamp": [3357.26, 3359.14], "text": " is AWS BottleRocket supported?"}, {"timestamp": [3361.48, 3362.5], "text": " Can you read the second part?"}, {"timestamp": [3362.7, 3363.74], "text": " AWS what? Sorry."}, {"timestamp": [3364.38, 3365.5], "text": " Yeah, Amazon's operating system, BottleRocket, Can you read the second part? AWS? What? Sorry."}, {"timestamp": [3365.5, 3366.5], "text": " Yeah, so that's..."}, {"timestamp": [3366.5, 3371.36], "text": " Yeah, Amazon's operating system, BottleRocket, I believe that it's kind of built for containers"}, {"timestamp": [3371.36, 3373.6], "text": " and running things on that front."}, {"timestamp": [3373.6, 3378.0], "text": " My initial thought would be yes, because of the interfaces that you've chosen to bind"}, {"timestamp": [3378.0, 3380.62], "text": " to, you know, CNI, CSI, all of those things."}, {"timestamp": [3380.62, 3383.04], "text": " But so long as those are supported, that should be good."}, {"timestamp": [3383.04, 3390.88], "text": " But not sure if that might correlate to the system metrics might be the the specific question yeah correct so it should be"}, {"timestamp": [3390.88, 3394.96], "text": " you know i don't i don't remember if there's actually somebody that is using aws bottle"}, {"timestamp": [3394.96, 3399.52], "text": " rocket but again we actually don't go in necessarily too much into the into the os"}, {"timestamp": [3399.52, 3411.2], "text": " because as long as they're running like the minimum required kernel like on the on the on the nodes right which is uh i believe kernel 415 uh of linux and up um yeah i mean we we shouldn't have any"}, {"timestamp": [3411.2, 3415.52], "text": " any issues supporting that um if you want to explore i i highly suggest signing up for the"}, {"timestamp": [3415.52, 3422.0], "text": " uh for the free version and it should work i don't see why it wouldn't um so yeah that that's primarily"}, {"timestamp": [3422.0, 3426.26], "text": " we look at os's that will enable collecting flow data like eBPF."}, {"timestamp": [3426.26, 3428.26], "text": " That's probably our primary requirement."}, {"timestamp": [3428.26, 3429.64], "text": " That's it."}, {"timestamp": [3429.64, 3430.48], "text": " Cool."}, {"timestamp": [3430.48, 3433.54], "text": " And all those nodes run Node Explorer, of course."}, {"timestamp": [3433.54, 3434.44], "text": " Cool, cool."}, {"timestamp": [3434.44, 3435.46], "text": " Interesting, interesting."}, {"timestamp": [3435.46, 3437.26], "text": " It's fun to kind of see all these different computes"}, {"timestamp": [3437.26, 3439.38], "text": " and then being able to surface that information."}, {"timestamp": [3439.38, 3440.9], "text": " And you know, it's not just AWS."}, {"timestamp": [3440.9, 3442.86], "text": " We work with any cloud vendor."}, {"timestamp": [3442.86, 3445.92], "text": " That's the advantage of working without looking at provider"}, {"timestamp": [3445.92, 3454.56], "text": " agents. Awesome. Awesome. Awesome. Next question is, does the SaaS support SSO and SAML login?"}, {"timestamp": [3456.08, 3462.24], "text": " Yes, absolutely. Yes. So I believe the free version doesn't have that quality of life."}, {"timestamp": [3462.24, 3465.26], "text": " There's some of those things that are you know, are like more enterprise features."}, {"timestamp": [3465.68, 3466.62], "text": " But yes, absolutely."}, {"timestamp": [3467.04, 3470.72], "text": " Many of our customers are using like Azure AD"}, {"timestamp": [3470.72, 3473.02], "text": " or they might be using Okta, et cetera."}, {"timestamp": [3473.14, 3474.06], "text": " We absolutely support that."}, {"timestamp": [3475.56, 3476.0], "text": " Excellent."}, {"timestamp": [3477.04, 3478.88], "text": " Next question is, I promise,"}, {"timestamp": [3479.0, 3480.8], "text": " keep peppering you until we're out of time."}, {"timestamp": [3480.8, 3480.94], "text": " Keep going."}, {"timestamp": [3482.88, 3486.44], "text": " What resources does OpsCruise require?"}, {"timestamp": [3486.56, 3488.18], "text": " I'm guessing that might be pertinent"}, {"timestamp": [3488.18, 3491.0], "text": " to like Kubernetes primitives,"}, {"timestamp": [3491.2, 3493.56], "text": " like nodes, storage, deployments, config maps."}, {"timestamp": [3493.58, 3494.84], "text": " You're talking about what is required"}, {"timestamp": [3494.84, 3496.6], "text": " for just the collectors that we have, right?"}, {"timestamp": [3496.8, 3497.72], "text": " That's an easy one."}, {"timestamp": [3498.12, 3500.9], "text": " I think it might be more the platform"}, {"timestamp": [3500.9, 3502.66], "text": " to like install that integration"}, {"timestamp": [3502.66, 3504.8], "text": " to report the data to the SaaS."}, {"timestamp": [3505.18, 3505.3], "text": " Yeah, so for, you know, when we showed the architecture, really, more the platform to install that integration to report the data to the SaaS. Yeah."}, {"timestamp": [3505.66, 3514.06], "text": " So when we showed the architecture, really, it doesn't look like a cluster, right?"}, {"timestamp": [3514.18, 3519.7], "text": " But typically, for the actual open source collection tools, I mean, each one has their"}, {"timestamp": [3519.7, 3521.06], "text": " own requirements, but they're really small."}, {"timestamp": [3521.06, 3526.48], "text": " I mean, we're talking about hundreds of millicores to run the open"}, {"timestamp": [3526.48, 3531.84], "text": " source collectors like C-Advisor and Notixware. Those are all really, really lightweight."}, {"timestamp": [3531.84, 3538.4], "text": " The only piece that utilizes more resources is really Prometheus, right? And that's just"}, {"timestamp": [3538.4, 3545.2], "text": " depending on how many objects you have in the cluster. We typically for a small size cluster recommend maybe like in maybe like a"}, {"timestamp": [3545.84, 3552.96], "text": " two CPU 8 or 12 gig machine. But you know, as you scale up in the amount of objects that you know,"}, {"timestamp": [3552.96, 3558.56], "text": " I think I think I've seen just and I might be misremembering. So if you have if you want to"}, {"timestamp": [3558.56, 3564.8], "text": " more details, like really hard numbers, please reach out. But I think we've seen like five,"}, {"timestamp": [3568.6, 3577.9], "text": " hard numbers, please reach out. But I think we've seen like five, close to 5000 containers being monitored by like, at this point, maybe 64 gig for CPU machine node to power Prometheus. And it's"}, {"timestamp": [3577.9, 3582.52], "text": " not fully used. But sometimes when it you know, when you scale out, and you get just all those"}, {"timestamp": [3582.52, 3585.0], "text": " some tons of spike spike in objects."}, {"timestamp": [3585.94, 3587.32], "text": " That's really when it uses that."}, {"timestamp": [3587.32, 3589.36], "text": " But Prometheus is really the biggest one"}, {"timestamp": [3589.36, 3591.32], "text": " and you'll find this, it's not an option,"}, {"timestamp": [3591.32, 3593.58], "text": " it's a Prometheus piece."}, {"timestamp": [3593.58, 3595.78], "text": " But other than that, all those components,"}, {"timestamp": [3595.78, 3597.18], "text": " I mean, you're talking extremely,"}, {"timestamp": [3597.18, 3599.12], "text": " extremely small resource requirements,"}, {"timestamp": [3599.12, 3601.4], "text": " really inexorable on your clusters."}, {"timestamp": [3602.48, 3604.34], "text": " Absolutely, absolutely."}, {"timestamp": [3604.34, 3607.32], "text": " Well, with that, unfortunately, we are at time."}, {"timestamp": [3607.42, 3609.12], "text": " I really do appreciate everybody reaching out"}, {"timestamp": [3609.12, 3610.42], "text": " and asking those questions."}, {"timestamp": [3610.94, 3614.3], "text": " Like all good things, the streams have to come to an end."}, {"timestamp": [3614.56, 3616.26], "text": " So we are at that point."}, {"timestamp": [3616.38, 3617.6], "text": " But thank you so much."}, {"timestamp": [3617.6, 3620.56], "text": " If there is anyone looking to kind of reach out to either of you,"}, {"timestamp": [3620.66, 3623.14], "text": " is there a good place to open up those questions?"}, {"timestamp": [3623.7, 3624.04], "text": " Sure."}, {"timestamp": [3624.12, 3629.24], "text": " You can reach us to info at obscurus.com to be generic enough."}, {"timestamp": [3629.24, 3633.84], "text": " And you can also ping us on the website, obscurus.com itself."}, {"timestamp": [3633.84, 3635.88], "text": " You know, it should be easy to find us."}, {"timestamp": [3635.88, 3639.44], "text": " We're also on LinkedIn if you want to look us up on the Obscurus page."}, {"timestamp": [3639.44, 3641.76], "text": " Love to chat with you guys, get your feedback."}, {"timestamp": [3641.76, 3642.76], "text": " Excellent."}, {"timestamp": [3642.76, 3643.76], "text": " Excellent."}, {"timestamp": [3643.76, 3646.64], "text": " Hopefully this was interesting and exciting,"}, {"timestamp": [3646.64, 3649.16], "text": " given where things are going with OpenTelemetry,"}, {"timestamp": [3649.16, 3651.64], "text": " open source and observability."}, {"timestamp": [3651.64, 3653.4], "text": " Well, thank you both so much."}, {"timestamp": [3653.4, 3655.74], "text": " Thank you, everyone, for joining the latest episode"}, {"timestamp": [3655.74, 3656.6], "text": " of Cloud Native Live."}, {"timestamp": [3656.6, 3659.0], "text": " It was great to hear from Eloke and Cesar."}, {"timestamp": [3659.0, 3661.78], "text": " We really, again, love the interaction and questions"}, {"timestamp": [3661.78, 3663.54], "text": " from all of the audience."}, {"timestamp": [3663.54, 3666.96], "text": " Join us next week to hear about how we're going to be building stability"}, {"timestamp": [3666.96, 3670.3], "text": " in Kubernetes with Andy Suderman of Fairwinds."}, {"timestamp": [3670.78, 3673.64], "text": " Thank you all for joining us today, and we will see you soon."}, {"timestamp": [3674.1, 3674.42], "text": " Have a good one."}, {"timestamp": [3674.44, 3674.94], "text": " Thanks, everyone."}, {"timestamp": [3675.54, 3675.92], "text": " Thank you."}], "text": " Hello, everyone. Welcome to Cloud Native Live, where we dive into the code behind Cloud Native. I'm Taylor Dolezal, a senior developer advocate at HashiCorp, where I focus on all things infrastructure, application delivery, and developer experience. Every week, we bring a new set of presenters to showcase how to work with cloud native technologies. They will build things, they will break things, and they will answer your questions. In today's session, Alok and Cesar have joined us to talk about leveraging the CNCF observability tools for Kubernetes troubleshooting. This is an official live stream of the CNCF and as such is subject to the CNCF Code of Conduct. Please don't add anything to the chat or questions that would be in violation of the Code of Conduct. Basically, please be respectful to all of your fellow participants and presenters. In short, please be respectful to all of your fellow participants and presenters. In short, please be excellent to one another. With that, I'd love to hand it over to Alok and Cesar to kick off today's presentation. With that, I'll turn it over to you. Thank you, Taylor. So I'm Alok. I'm the Founder and CTO of Upscruise, an observability company built on open source and CNCF telemetry and I will also introduce Cesar Quintana my colleague who was the principal solutions architect at Opscores thank you so the way we thought we would do this before we set up the demo itself and go through that the fun part I want to do a little bit of setup so you know have the context of what we do so with that in mind let me share my screen and bring up uh you know kind of set the stage if you will let me find the right one and let me know if this is coming up And let me know if this is coming up. Cool. That looks good here. Okay, great. So as mentioned by Taylor, we are talking about how to add intelligence and observability now that we have open source monitoring. Going through the standard confidentiality and legal notice. We'll skip over that. I'll let the stage by, you know, revisiting what really has become the challenges of cloud native application observability, right? And fundamentally this has been happening now for a few years as applications move to microservice architectures. You know, there are three things that we know have started and really added a lot more challenges to the ops teams that are managing them. Number one was just complexity, scale, the tiering. There's so many dependencies. And the dependencies, as we've shown on red, you've got, of course, the application pieces. Could be PaaS services, could be SaaS services, could be Kubernetes container could be SAS services could be a kubernetes kubernetes uh container running an application code it could be serverless all of them and then of course even dependents on kubernetes itself orchestrating all of these and then the underlying infrastructure wherever it might be so these create tier dependencies you know if you will from top down kind of what we call vertical, as well as across. And this is happening all the time. The third complexity is dynamism. Great. We want to be agile, right? We want to add services, change any one component, scale up, scale in, you know, some things drop, some things brought up. That, together with all of this, is like a highly complex distributed system. And just looking at a couple of metrics is no longer sufficient. You know, things are changing, things are coming up. You have a zillion dashboards. So the good news and the sort of not so good news is the following. The good news is, thanks to CNCF and open source, pretty much every possible telemetry from real-time metrics logs events configuration information traces flows are all available now directly from an open source environment meaning open telemetry is an example open source that has existed now for a while so that means we can move to the the key pieces to solve that problem. What's the not so good news? That complexity. The role of observability is changing. It's no longer about dashboards and just alerting. It's how can we help the ops teams get to understand what's happening in real time so they can detect quickly, find the real issues, and get up and running you know it's the same things that you've heard time to mean time to test should be fast don't waste time with false alerts and get to the root cost mean time to resolution right so what we what we've learned having been born cloud native ourselves, is what does Ops really do, if you think about it? What Ops does, beyond getting all the telemetry, is they understand the dependency. And they look at an application with multiple services talking to each other. They understand what the interactions are. They know how applications, services are being monitored by the orchestration. They know the dependence of infrastructure. So really savvy ops teams, SRE teams know that. They also know what's changing and they apply curated knowledge. They know a published, subscribing, or producer consumer model versus a database, what they're supposed to do. They know what to look for, what metrics to look for. They are aware of the app and the dependencies. That knowledge is what they're supposed to do they know what to look for what metrics to look for they are aware of the app and the dependencies that knowledge is what they use when they look through the data and sift through whether it's the metrics the laws of the traces right so fundamentally when they look at something that's happening they're looking at the every component the inbound and outbound who's talking to whom what resources and services depending on? All of this. And so they essentially built context to understand what is happening so they can actually detect the problem, isolate it and analyze and figure out what the resolution should be. So if you think about, if observability has to be really intelligent, they have to establish this context, this understanding, and surface that from all that effectively called noise that's coming in, all the data that's sitting in. If you can't do that, then we've actually made the life of a typical DevOps and SRE very difficult. So that's what we want to do. So our thesis is help leverage this data, right? If you look at open source and open telemetry, clearly we know that things like Prometheus, getting information from Fluidly for the logs and pulling into something like Grafana Loki, using Jaeger or open tracing standards even to get the traces, looking at flows from things like eBPF and Istio, using configuration information from Kubernetes, changes, even the cloud infrastructure data as well as pass information, all that that's available, pull that together to build that context across all of them and then help reduce the amount of information and focus on the right information that ops needs, right? So this is probably the one key slide as we get into the demo to tell you. On the left-hand side, what you're seeing is all the open telemetry, all the open source that is available today. You don't have to put proprietary agents and do proprietary information code thanks to cncf thanks to open source monitoring that's available so the first thing as we said for context is understand the structure of the application what we call the application graph right so imagine able to automatically build out that structure in real time so the ops guy doesn't know even the app developer doesn't have to go around trying to figure out what's talking to or do something exotic to get there. And that graph has to change dynamically. That graph tells you who's talking to whom, what are they dependent on, how is Kubernetes, you know, managing it, allocating resources appropriately or not, and how does Kubernetes have access to the type of cloud resources it needs to allocate to the services and then to really understand what's going on that context pull the data and what we do is something called a behavior model profile every component that comprises the application to see what is expected profiling is kind of like building a very simple emulator model and you can look at it Profiling is kind of like building a very simple emulator model. And you can look at it, collect the data to figure out, for example, across all the metrics that you collect, all the flows and events, and say, hey, this one, for example, is IO dependent, this one is CPU dependent, or mix, or this one does a lot of calls. So that as data is coming in, as you learn that, this is where ML comes in, you know what to expect, because you have the realtime metrics, you have the application graph. So once you have started learning that, over time, you know, within, actually in our case, within 24 hours, you start getting a baseline behavior, which gets better over time, you can start looking at deviations. So you don't have to worry about setting thresholds. You don't have to try and guess what the thresholds are and start tuning them. First of all, you don't know which metric and what the level is let the ml model learn expose that so that we can analyze it again in context we know what what are the what are the drivers for every service uh because we know the application route so once we have these deviations whether it's coming from explicit alerts like a failure in infrastructure, a failure that Kubernetes detects, or application starts slowing down or having a degradation, you put it all in context and then analyze that. We essentially do what we call local detection across the application, and then we call an analysis of what we call dynamic decision, which looks at everything in context, why would this happen given what I have seen so essentially think of it it's almost like an anthropomorphic what an ops would do and they understand if we can put all of this in place and automate this pipeline we have reduced amount of work that off spends today trying to understand what is it what does the application look like who's talking to whom when is there a problem there instead of setting thresholds, and if I do, how do I analyze it? If we can collapse that and reduce that, we have really done the right service to get the right level of intelligence and observability. So this flow, if you think about it, is what we will demo today using what you're seeing on the left that essentially build context understand the application graph understand the behavior to surface problems detected analyze it in context using all the telemetry we have you know including changes logs events and help isolate the cause and so that's our purpose of our demo today. And I'm going to hand this over to Cesar because we want to get to the demo and he'll tell you exactly how we leverage open source monitoring and use CNCF, OpenTelemetry to do this. So I'm going to stop sharing. Cesar, please take it away. Actually, look, if you could go back to that, we'll talk really briefly about those open source platforms that we're leveraging. If you could share that. I'll go back. I shouldn't have done that. I was a little too hasty there. All right. Is that coming up? There it is. All right. Yeah. So again, everybody, my name is Cesar Quintana. I'm a principal solutions architect at OpsCrews here. And, yeah, so to add on to what Alok was mentioning, right, the whole premise of leveraging these open source platforms that, you know, essentially the whole data collection layer has been commoditized, right? Observing data is now easier than ever to access thanks to these, you know, powerful open source, particularly around the CNCF platforms, right? So what we've set out in mind, right, is to build something and leverage these amazing tools to make everybody's life easier, right? So things like this is an example of our architecture of how we're leveraging all this open source data and all these open source platforms. So as you'll notice here, if you focus on that Kubernetes cluster square on the right side, right? What you'll see is across the top in the green you'll see your workloads right you know pod one two three four these are eventually your own applications running whatever you're doing whether you're running an e-commerce site a financial trading platform etc this is what you're running inside your actual workloads but underneath in the in that light and dark blue are the open source tools that are now so common throughout the IT landscape and in the modern application environments. So towards the bottom with the dark blue, you'll see here in this reference architecture, we're showing Jaeger, Prometheus, Loki. It could be something, this is just an example. We can leverage logs from other sources like Fluent. I think somebody asked about Fluent. It could be Loki. It could be Fluent. And then we take metrics in from Prometheus. And then Traces. We're leveraging Jaeger as a backend for our particular architecture. But we are supporting open telemetry libraries for the client side. So the important that's one of the really, really cool things about the new standards is that they're now well defined, which means that you could be using a mixture in your environment of open zipkin and Jaeger, the open telemetry libraries themselves and still have a unified back end where you're able to collect all that data and leverage it and use it, even though you're technically using disparate libraries throughout your enterprise. Right. So so what you'll see here, you know how we've architected ourselves to be built is again around these open source platforms again whether it's fluency whether it's loki and prometheus and jaeger etc they serve as you know now your your your data collection and data data store you don't have to go out and pay another vendor you know uh 10 15 x for storing just metrics right when you can store them in your own infrastructure we're all doing the same thing right just putting them inside of uh uh inside of a long-term, you know, bucket, right? And so now that's under your control. And so we, for example, Promptail, right? If you start looking upward towards the stack in the light blue, Promptail will run as a daemon set, collect logs from all your nodes and from all your containers, right? And then you have on top of that node exporter, right, functioning as an exporter for Prometheus to grab the metrics from the nodes themselves. And going above that, you'll see C-Advisor collecting data from the containers themselves running on each node. And then we also leverage KSM exporter, pretty awesome, grabbing Kubernetes object status data. And all those are going to be fed out into Prometheus or to if you're using traces again to Jaeger. And really, you know, now, even just with that, you've got a pretty darn functional observability layer, right? Now you have metrics and they have traces. Now you can go into different places and look at your logs. But what we're, you know, what Alok was mentioning earlier is that smart layer, right? Now you want to leverage all those pieces of data, bring them in together and do something really, really powerful with having all that context, all that configuration data that we can grab from the Kubernetes API, and then just bring it all together. On top of that, you have metric data, configuration data, performance data, and event data from your cloud environments, right? So bringing in things like, you know, more and more applications are hybrid, right? They're using, you know, whether it's VM and kubernetes or serverless and uh and pass you know you have all these really really hybrid environments that again it's the whole um uh extreme uh production of data and having one place and easy ways to collect them and that's really what these open source platforms have allowed us to do right but going back to what i was mentioning about cloud you also want a place where you can grab your data and bring it in. Talking about, again, serverless or function as a service, the PaaS layers, which are only constantly growing, right? You have these cloud caches and messaging services, cloud databases, et cetera. So what OpsCrew sets out to do is not only grab that open source data and leverage those collection platforms, but also bring in the cloud data and mesh it all together and build something really, really rich and then provide actionable data based on that. So what I'm going to do is I'm going to show you a demo of OpsCrew's. Oh, sorry, Alok, did you want to add? Since I had the opportunity to look at the message, someone asked, what about FluentD or FluentBit? And, you know, everything for that. So let's address that. Yeah, no, so as mentioned, right, we can take logs, basically, from whether it's Loki, FluentBit is usually the thing, or FluentD, those are usually the pieces we run into, right? And absolutely you know, the whole point is to build a modular flexible platform where you can grab data from, you know, whatever your preferred variant of that is, right? So yeah, absolutely. OpsCrews particularly provides support for Fluent D, Loki, and a few others as well yeah so the takeaway message I want to do before we go to the demo so we can you know address them all across is as long as we have the source of getting the logs in the metrics in this approach will still work of course with OpenCNC we don't have to do proprietary agents proprietary instrumentation we can be sitting outside without being intrusive. So think of it that way. The real intelligence or observability is not how the metrics got to us and what it is, as long as we have coverage. That's the key. The coverage is all of these is needed. You can't just go on metrics and logs and traces independently. It doesn't give you the whole picture. Otherwise we are one of the six blind men looking at the elephant, which is in the room. All right, go ahead, Cesar. I had to say that because I think it's good. CESAR GAVIDIA MARTÍNEZ- Thanks for that. All right, so now I'll share. So look, I think you might have to stop. CESAR GAVIDIA MARTÍNEZ- I can stop sharing, right? Sure. CESAR GAVIDIA MARTÍNEZ- All right. So let me share hopefully you guys will be able to see my screen here so I have one proof pointed but there you go it's coming up excellent okay awesome so yeah so this is this is a uh this is our landing page for obscures and you can see there's there's uh quite a few pieces of data here um you might you know this screen might look familiar uh for any of you who have used apm tools uh before so this is a real-time service apology um map essentially you know you know, we're leveraging, as Alok mentioned, eBPF as well, right? So eBPF allows us to grab this network data and bring it in alongside not only the tracing, which in this case happens to be optional because we have eBPF, but it is the eBPF network data alongside the tracing data, alongside the metric data, alongside the logs, alongside the configuration data discovered from whether it's cloud or Kubernetes or the virtual machines themselves or the serverless or other PaaS components. It's all brought together in a single place. But we're giving you this real-time, excuse me, a flow of how your services are interacting with each other. And I'm zooming in more. Of course, now this is not even, this is nowhere close to some of the busiest environments, but you can see that it does get busy really, really quick. And that's one of the cool things about, you know, having all the configuration data and the really rich data that the underlying tools like C-Advisor collect is that we get a lot of really rich object data along with the metric blocks. So things like being able to understand the configuration data of these pieces allows us to also extract things like labels and tags. So when you have a busy environment, you might only want to filter, for example, on a particular namespace, right? I might only want to look at maybe the Obscures namespace. And so that really helps you cut down on some of that noise when you're trying to isolate an issue. But going back to kind of our premise, what we're showing here is a mixture of quite a few different pieces of data. You're showing the EVPF pieces. Again, we talked about Cloud. So this demo happens to be running inside of AWS. But, you know, whatever cloud you're running on, you're going to have that PaaS layer very likely. So being able to collect that data and bring it all together to your Kubernetes environments, you know, and all monitor it in a single place is absolutely powerful. So if I click on, for example, that AWS RDS instance, you know, again, we're talking about the metrics. So if you look at this right side, we're collecting all those individual metrics, the read IOPS and the throughput, et cetera. And this is a high level summary, but important is metrics, right? So I can go in here and look at all the individual metrics. That one of the pillars right over topability um and that's just that's just for one entity same thing for uh same thing for a pod right this is a pod in the container so if i click on a pod same thing i'm bringing back all this configuration data all these labels you know what time this was created uh what's what host it's running on it's important to understand all these things because when you're troubleshooting you know well when was i well you know, well, when was I, you know, what time was this pod running? It was supposed to have been restarted five minutes ago. Did we actually perform the restart or was there an issue, you know, doing that rollout of the application? Well, look, it's been running for, you know, since a couple months ago. So, hey, that rollout wasn't successful, right? Again, we've got metrics as well, and each entity has its own pieces of data, and it's important to be able to look at that data, again, in context for, you know, whatever problem you're troubleshooting. In this scenario, I clicked on this container. It happens to be the Jaeger agent, but I click on this container, and now I'm getting, you know, additional data that's contextual for that particular container, the ports that are being exposed. But on top of that, being able to see how the infrastructure is working, what things are related to what. So for example, we have these contextual access to these different pieces, right? So if I click on this three layer view, right, what it does is it shows me this particular container and this pod is running some details about it, the IP address, the image name that it's using, as well as some high-level metrics such as CPU and memory. But also, it shows me what Kubernetes node this particular container is running on, as well as some of the neighbors and those CPU and memory metrics for those neighbors. And then this Kubernetes node is running on top of what cloud instance, right? So when you're troubleshooting, I know I have some instances in, let's say you're running EKS and you have some nodes in one particular subnet or one particular availability zone that are having connectivity issues and you're trying to diagnose, right, in a single click, you can understand if your container happens to be running on one of those nodes and things like the region and how much storage is attached to it. But not only that, again, as we mentioned, the ubiquitousness of all this data and the ease of collecting makes it really, really simple to bring it all together. And now we can look at the infrastructure map that we call, which is essentially a cloud map. And now we're looking in the context of this particular cloud instance, and we're looking at this EC2 virtual machine and looking at the configuration of that and the tags, right? And I'm just kind of showing behind the scenes, all the open source data that we're actually collecting and how even that open source data by itself makes it really powerful. But once you combine the intelligence, which I'll talk about in a second, that's where things really start to take off. But as we mentioned, we're collecting data from the Kubernetes API and from the container. So that's where we're grabbing the individual container metrics and the node metrics. We also have an understanding, for example, at a per node view, right? So instead of looking at it from a kind of application centered view, I can look at the node level. Let's clear out some of these filters. Now, so you see we have five nodes running, and now I'm looking at each individual node, and I can see the workloads that are running on top of that node I can click on metrics and get the metrics for that particular node so load in just a second but I'll go back and then we can actually look at the configuration for the particular node Yep. I have some filters on here by default. Real time internet issues. Always fun. There we go. All right. So yeah, so again, we're collecting all the configuration and metadata, not only of the containers themselves, but even the nodes that you're running on. So things like the memory utilized, sorry, the memory capacity, whether the node is ready. So you'll see here like max memory, max storage, what version of Kubernetes are they running? And so, you know, here we see that we're running version 117 of the Kubernetes node, which is actually probably a little bit outdated. And the kernel version of the operating system that it's running on, et cetera. So we're bringing, again, all this data together, which is really, really empowered by all these open source layer tools. We're not using custom agents. We're not doing anything special. It's just leveraging all this data, but bringing it all together in a single place. On top of that, I mentioned it's important to cover things like PaaS services and serverless. So, again, we also collect that kind of data. So you'll notice here you saw an RDS instance. I think I'm going to show a load balancer as well. In this case, in this environment, I have an API gateway running with with an S3 call out actually via serverless. So you'll see this API gateway. And again, I'm grabbing the data from that particular API gateway, just like for the containers we saw that particular entities made it in for the nodes. Now here's for the API gateway and some of the metrics as well. And same thing for for the serverless functions. Right. I can see the ARN of that particular serverless function, the region. And I think we've got metrics down to that. So the whole point is to bring something that's all together. And finally, you know, actually, before I show that, I also did mention traces. And let me actually share this screen because I think I'm not sharing that. I do want to show the traces before jumping on to something else. Here we go. to something else. There we go. So again, we also have our trace map view that we just recently announced. And so when you're leveraging, as we mentioned, distributed tracing, we can collect all that data, again, on a single space. And now what we're doing is we're collecting the individual traces. And actually, we're doing something pretty cool, which is what we call the trace map and identification of these trace paths. Sorry, Alok. I don't see your screen. Oh, you don't see my screen? Sorry about that. Can you share? Because I don't think they know what you mean by the trace. Okay, great. Okay. And I did see one thing. If you are able to bump up the text just a little bit, I saw a couple of comments about that as well. Sure. Is this, hopefully this is a little bit better. Yeah, I think, I think that should be good. Okay. Thank you for calling that up. Gotcha. Let me know if there's still visibility issues, but I've bumped it up just a little bit. Yeah. So again, we're just, I'm just showing off the tracing capabilities. Again, just bringing everything all together in a single place. You can see here this trace map showing the different interactions from the front end to the ad service to the product catalog service. But one of the really cool things that is kind of unique that we've been able to develop is identifying, you know, a lot of times in tracing, you'll get transaction identifications. Oh, I'm seeing that. Hopefully, hopefully, this is a little bit better. I think I've hit the limit of my of my zooming in capabilities. Sorry, guys, I always was a little bit bigger. Hopefully, this is some sort of a mobile for you. Okay, so we've got the traces. We've got the trace maps. And, oh, it looks like I'm getting some too much noise on my machine. So, sorry about that. I'm seeing that in the chat. Hopefully, it turned off the notification sounds here. Hopefully, that will stop interrupting. Okay, so we've got the notification sounds here. Hopefully that'll stop interrupting. Okay, so we've got the trace map view, but we're also discovering what we call the trace paths. So these trace paths are not just sorry, guys, give me just one second. I'm trying to... We're still on Slack, guys. That's why. Yeah, there's... Taylor, you know how this goes. on slack guys that's why yeah there's uh taylor you know how this goes oh absolutely absolutely i feel like as soon as anyone goes live that's there must be like a hidden button or something like that somewhere because that's that's what i started to get a lot of money we are being pinged on that so anyway i believe i've turned off not do not disturb successfully, which I thought I did before the call. I just shut down Slack. My apologies to everyone. Okay. So let me head back here. Okay. You know, we have auto discovery, essentially, of not only the transactions themselves, which are used to seeing distributed tracing uh platforms but um we are also grabbing uh that identification of the paths themselves you might have a transaction you know for one of these uh products um that you know might be a slash checkout but you might have a different types of checkouts for maybe um uh a uh class right maybe you're selling a class on your e-commerce site versus a product, right? So even though, you know, they're both called checkout, one might go to AD service and another one might go to this checkout service and then product catalog service. So even though they're both named the same, we identify those differences between them and then also perform automated anomaly detection and profile those transactions separately from each other right um so that is that you know that's some of the tracing we won't delve too too far into this because i want to show really some of the some of the magic behind um what we can do now that we have all that really rich open source data right so um let me stop sharing and reshare my other screen. Just give me a second here. So, there we go. You guys should be seeing my screen pop up here in a second. up here in a second. All right. All right. So, you know, some of the things that we can do now that we have all this open source data is that we can now start doing anomaly detection, detecting of misconfigurations, misbehaviors. You know, one of the things I actually did not show, if I go back here really quickly, is that we're also collecting configuration data, not only at this kind of high-level metadata kind of view, but we're also showing the entire manifest. So if I click, and I'll just show what I did there, if I click on detailed view for this particular pod, right, now I'm looking at the actual manifest for this particular pod so I can look at the details of what exactly is going on throughout without having to go inside the command line and figure out you know get a cube ctl get pod dash oh yaml and this is uh this is way simpler and it also helps keep everything in context and keep you inside of a single place um but now with all this really rich data and on and knowing you know the other thing we do is we have what we call curated knowledge, because on top of all this, you do need to understand how these systems interoperate with each other and what kind of dependencies they have on each other. That's why we do build that relationship view, leveraging all the data. That's why we want to know what container is running on what pod. I'm sorry, on what node and what node is running on top of what piece of infrastructure. So we know when a piece of infrastructure is down, we know that it's affecting the container that's hosted on it. And there's a lot of nuance and variance to the kind of problems that can arise. But having, again, this richness of this open source data, it makes it all possible. So I'll show a couple of things that we do. Here, let me find an alert. I think I was looking at this alert a little bit earlier. So I'll explain a little bit what this is, right? So in this case, we have a deployment problem, right? On our particular web server deployment, we're supposed to have a total of three replicas. And in this case, and you know what, I'll bump up the text a little bit because I know that was asked before. So we're supposed to have a total of three replicas. In this case, we've only got two available replicas, and this has been going on for a little bit. So down here, you know, we provide some details. It's part of the shopping cart namespace. It's the web server deployment. And here's some, you know, additional kind of feel key value pair details, but we'll go to the fun view. I know some of you guys love reading JSON, but I kind of like the UI just a little bit more. So when I click on this analyze view, what it shows us is what we call the contextual RCA, which is our fishbone RCA, right? So in this case, what we're showing is we're showing failure categories across the top and bottom that are affecting this particular deployment so again all this is being collected just through the you know acquiring the kubernetes api and then the the relationship of the of collecting the events and the containers and linking those all those pieces together um so we have a replica set scaling issue right we're having a an issue scaling up an additional replica of that particular image. And now we're getting actually a back off restart as well. But this is all really associated to the startup failure. And if I click on that, what it's going to tell me is that I have an invalid image name. So obscures is spelled with one I, and it looks here like somebody spelled obscure with two I's. And so that's a bad image name. like somebody spelled obscrute with two I's. And so that's a bad image name. You know, it took us all of, what, you know, three, four seconds to figure out that one of our replicas isn't coming up because of a bad image name. So it's those kinds of things, the richness of the data that allows us to build these really, really quick root cause analysis pieces into something like obscripts, right? So, yes, you can do this from the command line. It's, you know, it's a little bit more work. It'll probably take anywhere from, I don't know, 30 seconds to a couple of minutes. But, you know, multiply this times 1,000 times 5,000. That can happen in a month. You know, that's a lot of time saved for operations teams, right? And you'll also notice other ones that some of these are more complex. And, you know, these are just building blocks to what I'm going to show you in a sec of these individual kind of problem detections and anomaly detections. But you'll notice other categories. So things like a missing config map, right? If you reference a config map in your manifest that does not exist, you're going to have a failure of your pod. So we'll highlight those things or failed volume mounts or even bad image tags. I think I might actually have a bad image tag in here that I was looking at. It's a very, very similar scenario, but for the cart server, if I click on analyze, yep, same kind of symptoms, replica set scaling issues, we're having back off restarts going to a crash loop. But in this case, we have an invalid image tag. This particular image tag does not exist. Now, the other thing that I didn't go too far into, but it really is absolutely key, is machine learning. So from all the individual services that you deploy onto your clusters, what happens is that with that data being collected from C-Advisor and from Nodex program, from the discovery pieces, what we do is we create a really rich behavior model, right? We detect what is normal behavior for your individual services, right? So if you are, you know, we don't just look at one or two metrics like error rates and response time, but we look actually each one of the entities that I've shown you have their own behavior models. And there's a bunch of others that I didn't show you as part of this demo, but things like if you're using database like MongoDB or a JVM or an Nginx container, right? And then the generic containers themselves, the nodes themselves, they all have their own behavior models. And we pick up a mixture of a lot of different metrics to understand what is normal behavior. And then when we find what is abnormal, we have these types of alerts that are prefixed by ML telling us that there is some sort of ML detected performance violation, right? So if I click on this, in this scenario, you know, again, I'm going to get some details as to what happened, right? I get, you know, I know when we zoom in a bit, network transmittal bytes increased by 540% and level four bytes for the outbound traffic increase and inbound transmittal bytes decreased, actually. So we don't only detect increases, but also abnormal decreases as well. But just like in the other scenarios, if I click on analyze, I can get a fishbone representation of what exactly is going on with the metrics and why the ML in the first place triggered an anomaly. And so I'm going to zoom out just one piece. Just like you saw for the Kubernetes specific deployment scenarios, now in this phase one RCA, we're looking at a container view. This particularly the card cache had some deviation in its metrics. And actually before looking at this, I'm gonna go back just to the summary screen and show you down here, if I click more details, you know, speaking about the ML and all the metrics we take, these are all the different metrics for just the generic container model that we're looking at, right? So again, it's not just one or two or three metrics. We're looking at transmittal bytes, packets in, packets out, memory failures, CPU utilization. All this data, particularly for the container, is, again, provided by C-Advisor, an open source tool, right? So, again, going back to the analyze button, now we're seeing the actual pieces that actually triggered the ML. And so now you'll notice that the fishbone has changed from our startup failure. Now we're showing memory and file system and CPU. And so right away, we'll show you in red. You don't have to go and look at a chart for this specific thing. It's here, right? So I'm seeing CPU utilization has increased by close to 50%. I'm looking at demand, which is incoming requests. The response time has increased by over 1,700%. I'm looking at outbound supply side. Response time has increased by 2,200% for outbound requests from PartCash. And then not only that, our response size has increased from one to close to eight megs, right? And then bringing in the Kubernetes layer, it's this whole image change, right? So again, bringing in the data from the Kubernetes API, I can see that I've got a recent image change that's likely contributing to this failure. Now, again, I'm glossing over a few details because in the interest of time, I want to show you guys how we bring, you know, a couple of these things together. This is, you know, an ML alert. And by the way, you can see this automatically chart those important metrics down here. So you can see their behavior during the time of the anomaly. And as mentioned, you know, you can drill down into any logs that might be coming in. Actually, I should probably show that. I don't think I did. So here's another example of an anomaly database server. And you'll notice here that you have different contextual access, right? Application state, I'll show that. We have a time travel capability where with all this data, the metric data from Prometheus, the log data from Fluent, from Loki, the trace data, all this, you know, we build that real-time map that you guys saw and all the configuration data. We take snapshots every five minutes, and I'll show you guys that in a second, but you can go back in time at the time how your system was configured during the time of this particular anomaly. In this case, this goes back a day. So if I click that, it'll take me back into one day before and show me the entire config of my entire state at that time. But we'll go into that in a second. I can click on metrics to understand the metrics for that database server or any events that are related to that. In this case, I want to show logs. So if I just click on metrics to understand the metrics for that database server or any events that are related to that. In this case, I wanna show logs. So if I just click on the pod or the container logs, oh, maybe it wasn't logging there, but I do wanna show that we have contextual access to the logs actually. Let me find, I just wanna show because I think we did not actually show logs. Let me see, I think maybe note exporter will be logged. Sorry. And Cesar, I saw a couple questions come in on that front. One question was, does OpsCruise enable custom metrics? Does OpsCruise enable custom metrics? Yes. Basically, it's any data that's being exposed to Prometheus. So as long as data is being exposed to Prometheus, that data can be brought into OpsCrews. We're just leveraging Prometheus as a metric ingestion point. Awesome. And then one other question was, do the ML alerts wait for a particular size of training data before alerting on that front? So I can answer that. So typically it depends on, our default is about one day, but we can speed it up so we can learn. And the only reason I'll say that is because let's say there was hardly any activity on a weekend, he deployed it. You're not gonna see a lot of activity to profile it, but the next day it starts increasing. So over time, essentially we update that, but our default is 24 hours can be even less. Gonna make it even a few hours. Just have enough data to get an initial profile and then we continuously update that. cool thank you no the users don't have to do anything i wish i could learn that fast yeah accelerate like one day is good but yeah if i could learn like this you saw that generic container example that says there's about 30 metrics and you have no way of knowing because there's some calls being made as a problem versus the memory failure suddenly increases so there is no way a person can do that that's what the beauty of the using the ml to get unknown numbers keep going yeah absolutely um so the the thing i wanted to show is logs because i absolutely did not not show that even though it's super important but um any anything that logging, right, we pick that up from your standard out. But if you click on any, whether it's an anomaly, and this is showing the pod, right, I have a pod open. And from its, this is what we call the quick view, right? From its quick view, one of the links you have is for logs, right? So I can just click on logs. And that takes me straight into the logs for that particular service. Now, this is pretty, click on logs and that takes me straight into the logs for that particular service now this is pretty um you know uh static logs here but i can you know we it is searchable right so i can look for requests for example or conversion yeah the idea is to contextually link it to the problem correct yes uh so you know depending where we get it from yeah correct yeah i i think i don't have a problem here that has logs right now but um but we surface that as well. So if you're having an anomaly, you can go straight into the logs for when they're active and that'll show. Now, what I do want to show with all this really put together is I'm going to show you an alert, right? So we have, I showed you guys, you know, how we collect all the different data, the architecture, right, again, we're leveraging just purely open source tools here to collect the data from, you know, whether it's VMs, or whether it's Kubernetes, et cetera, from the application level, MongoDB exporters, or Nginx exporters, as well as the traces, whatever, open telemetry, compatible libraries, all basically built on open source. But now, right, what we have here is, again, I also showed you the anomalies on how we like specific Kubernetes detection, and then I showed you the ML, how we automatically detect performance deviation, and again, lots of different metrics. So in this scenario, we're kind of tying everything together, right? So I have a response time SLO breach on my Nginx service. So I'm going to click on that. And again, here's some details, right? I have an SLO of five seconds. My response time is at over 15 seconds. So I want to see what's going on, right? If I click on Analyze, what this is going to do, and I'm going to close this. We'll come back to this summary in a second. I'm going to close that piece. What this is doing is now we're showing a slice of the actual app map that we were looking at earlier. But now it's focused on the timeframe and in the context of this particular anomaly. So your route is essentially here. At NGINX, you're seeing a slowdown. But we've also identified what downstream services are involved. So we have NGINX itself, right? So this is the Kubernetes service, the pod, the container, and same thing, service, pod, container. For web server, Redis service, Redis pod, Redis container. They've got a cart server, service, pod, and container. You'll notice immediately in the red, we've highlighted. So we're doing fault domain isolation as well. You don't have to call the NGINX microservice team, whoever's managing that. You don't have to call the web server microservice team, whoever's managing that. Could be the same team, could be a couple of different teams. You don't have to reach out to them. You don't have to go inside your tools and look at the metrics for these parts. We're showing you they're healthy, right? So what the data has shown us from the data we've collected from these containers, as well as from the network data and the configuration data and combined with RML, that intelligent layer of the operations is we've highlighted the red pieces, right? So our container for Redis is red, our card server service is red, and so are our podding containers. So we'll kind of take this in the chain and see what's going on. So we've identified we have an SLO failure up here. We're responding really, really slow. Now, if I click on the next piece in the chain, I'm showing that Redis container is problematic. If I click on that, what it's going to do, it's going to show us this is a separate, technically a separate anomaly from the Nginx one, but the ML has detected that this is very much related. You'll see a few different failures. You'll see that we're getting an increase in throttling on the CPU. The user second solders spent on the CPU has increased by about 10%. But really interesting actually here is you'll notice the response time normally is at 2.94 milliseconds. Right now we're at over two seconds. This was automatically detected. And then also super important is our error rate, right? Basically you usually have zero errors. Right now our error rate has jumped up sorry to 36 out of every single basically every single request has essentially got into an error mode so something is wrong so we're gonna we're gonna go back here and and just see what our RCA is pointing at so Redis is calling card server now if I click on card server I'm gonna see the alert very very clear this service the card server doesn't have any pod to serve requests. That's a very, very clear indicator that obviously Redis is experiencing a bunch of response time failures and now error rate failures because there are no requests behind this Kubernetes pod, sorry, no pods behind this service to serve any requests. And to look into just a little bit more detail, if I click on this particular pod now on the cart server to see if I can glean what's going on, it looks like I'm having a back off restart. detail if I click on this particular pod now on the cart server to see if I can glean what's going on looks like I'm having a back off restart and if I look at the details of that alert it'll actually show me a little bit more detail again these are all separate but linked together problems if I click on the analyze tab now it's going to show me the real root cons right we've got an invalid image name as we talked about earlier so this broken image name with two eyes in India India, this is, I'll zoom in a little bit so you can see that. It doesn't look much. But you can see that Obscures India here showing two I's, right? So we have this invalid image name, and that's really, you know, the root cause of the issue. And it's all shown right here in a matter of seconds. Our engine is experiencing a response time slowdown. Redis is saying that we've got an increase in response time and errors. Card server is saying, well, I don't have any pods to serve. And the pod itself is saying, well, I can't start because somebody gave me a bad image name. All this in a matter of, you know, about 20 seconds, right? It took me obviously close to a minute, minute and a half to explain. But all this understanding the Kubernetes level up to the application level and how they are affecting each other is all powered by these open source tools. Plus, you know, the intelligent layer on top, which is, in my opinion, pretty darn cool. In the interest of time, there are things I wanted to show. I did want to show time travel, which is, in my opinion, pretty darn cool. In the interest of time, there are things I wanted to show. I did want to show time travel, but I think we're pretty close out of time. I want to open it up for questions. So with that, I'll turn it back to you, Alok and Tyler. Yeah, so I think we should go back to Q&A. So if I were just to summarize, kind of back to the premise we started with, right? In order to really help ops in this new cloud native microservice environment and kubernetes you know we have no longer to worry about where the data is coming from all the telemetry is there the idea is to build it but really what observer already has to do to have this intelligence is able to understand the full context of the application of course everything across all dependency track that users should not have to do that so that's what we need to build in and then understand the application profile the behavior so they don't have to worry about how to detect problem setting thresholds we want to take that off the table and then contextually analyze everything because now we have rich data in this whole distributed systems that whether it's an infrastructure or kubernetes related or down to the application they all link together and don't you don't need We have rich data in this whole distributed systems, and whether it's in infrastructure or Kubernetes related or down to the application, they all link together. And you don't need six different folks looking at traces, logs, events, alerts to do that. That's the role of observability in this new world. And thanks to open telemetry and open source, it is possible to do that. So think of us as the proof point. You don't need to worry about running multiple siloed tools to really build that intelligence and reduce the amount of effort needed. I'll pause there. That was the whole point of, you know, guys, take advantage of the open telemetry and the open source tooling that CNC has been helping. We are firm believers in that, and we hope you can leverage it too. Awesome. Awesome. Thank you so much. I did see quite a few questions come in. I see at least three. One question, the first one from Ishmael, was how easy is the installation? That's a great question. Let me see if I have it in this environment here. That's a great question. Let me see if I have it in this environment here. So for a typical deployment into maybe an on-prem cluster, we leverage Helm. Right? Again, another open source tool. So we leverage Helm. It's these commands. It's about three or four, well, it's actually five commands. And Sarah, could you bump that up just a little bit? Yeah, I absolutely will. Thank you. So essentially, you know, if you don't have these, if you have these existing tools, because a lot of people, as I mentioned, you know, these are the essentially the de facto standard for open source monitoring and all these modern environments. Most of the people we run into already have these tools. So it's actually a little bit simpler. But if you don't have these tools, we absolutely, you know, this, I think it's this last command that will deploy, you know, all the open source tools if you don't have them underneath already. Essentially, it's through Helm, right? These commands, you know, these five commands, and that gets you from a greenfield cluster to up and running. Literally, I mean, copy and paste, I mean, you're up and running in about, I don't know, three minutes, four minutes, and you have the entire environment that I showed. The only thing that's not available right off the bat is the ML because, you know, again, it takes a couple of, anywhere from a, I've seen ML alerts come back in a couple of hours to, you know, 24 hours. This is usually that sweet spot. But everything else that you saw within, you know, three to five minutes of deploying, you're getting all that data. So this is how simple it is. Awesome. Awesome. Thank you so much. The next question I saw was, is it free or what levels? How does it work? Is it a SaaS or is it something you can host on your own? It is a SaaS service, yes. Cool. Cool. Absolutely. There is a freemium offering for people who want to try it out. So you can go to our website, opsfeeds.com, and check it out. Cool, cool. Thank you. Next question. We've got lots of questions coming in. Thank you, everybody, so much for submitting those. Please keep them coming. I think we have about seven minutes, so I'm happy to field those as much as possible. The next question is, is it a good idea to export this data to do offline troubleshooting by importing data collected? I was wondering about troubleshooting edge deployment cases where we don't have access to the cluster. Interesting. So you're talking about when you don't have, if you can collect those metrics, you're saying, and push it to us, it's a little trickier with this, and depending on the context, maybe you have to dig into a little more specifics, you know, what data, because remember, not understand the application context, we pull everything, we may be able to see the dependencies. So seeing it in isolation doesn't tell you that. So it would probably be specific, so we can take this offline and just if this uh you know attendee has something specific that we can follow up and ping us yeah i will mention you know it you know it depends on how your edges it just compared like if you flat out don't have like um access to to like export metrics right i mean again you know obscures itself isn't really doing much on the collection side. It's really around having Prometheus on the cluster and having Loki or FluentD on the cluster to collect that data. Really, if you don't have access to there, that's something to be explored. But speaking about Edge itself, we have recently published a joint blog with Verizon where we're talking about. And again, I don't know what's going on with my D&D button. I know somebody mentioned it. I don't know what's going on with my Do Not Disturb. I promise I turned it on. But anyway, you know, when you're running Kubernetes clusters, for example, at the Edge or running workloads at the Edge, it absolutely is a supported model right again that joint blog where i mentioned is is uh is launching a kubernetes cluster on aws wavelength and um you know with kubernetes and uh observability built in with you know with obstacles and you know it functions perfectly fine but again if you have like a really really locked down edge and that might be something we can you know talk offline feel free to reach out we can talk about that scenario. It's a definitely use case because a lot of the edge applications are not deploying in Kubernetes, so we are playing into that space. Awesome. Awesome. Thank you. Next question is, is AWS BottleRocket supported? Can you read the second part? AWS what? Sorry. Yeah, Amazon's operating system, BottleRocket, Can you read the second part? AWS? What? Sorry. Yeah, so that's... Yeah, Amazon's operating system, BottleRocket, I believe that it's kind of built for containers and running things on that front. My initial thought would be yes, because of the interfaces that you've chosen to bind to, you know, CNI, CSI, all of those things. But so long as those are supported, that should be good. But not sure if that might correlate to the system metrics might be the the specific question yeah correct so it should be you know i don't i don't remember if there's actually somebody that is using aws bottle rocket but again we actually don't go in necessarily too much into the into the os because as long as they're running like the minimum required kernel like on the on the on the nodes right which is uh i believe kernel 415 uh of linux and up um yeah i mean we we shouldn't have any any issues supporting that um if you want to explore i i highly suggest signing up for the uh for the free version and it should work i don't see why it wouldn't um so yeah that that's primarily we look at os's that will enable collecting flow data like eBPF. That's probably our primary requirement. That's it. Cool. And all those nodes run Node Explorer, of course. Cool, cool. Interesting, interesting. It's fun to kind of see all these different computes and then being able to surface that information. And you know, it's not just AWS. We work with any cloud vendor. That's the advantage of working without looking at provider agents. Awesome. Awesome. Awesome. Next question is, does the SaaS support SSO and SAML login? Yes, absolutely. Yes. So I believe the free version doesn't have that quality of life. There's some of those things that are you know, are like more enterprise features. But yes, absolutely. Many of our customers are using like Azure AD or they might be using Okta, et cetera. We absolutely support that. Excellent. Next question is, I promise, keep peppering you until we're out of time. Keep going. What resources does OpsCruise require? I'm guessing that might be pertinent to like Kubernetes primitives, like nodes, storage, deployments, config maps. You're talking about what is required for just the collectors that we have, right? That's an easy one. I think it might be more the platform to like install that integration to report the data to the SaaS. Yeah, so for, you know, when we showed the architecture, really, more the platform to install that integration to report the data to the SaaS. Yeah. So when we showed the architecture, really, it doesn't look like a cluster, right? But typically, for the actual open source collection tools, I mean, each one has their own requirements, but they're really small. I mean, we're talking about hundreds of millicores to run the open source collectors like C-Advisor and Notixware. Those are all really, really lightweight. The only piece that utilizes more resources is really Prometheus, right? And that's just depending on how many objects you have in the cluster. We typically for a small size cluster recommend maybe like in maybe like a two CPU 8 or 12 gig machine. But you know, as you scale up in the amount of objects that you know, I think I think I've seen just and I might be misremembering. So if you have if you want to more details, like really hard numbers, please reach out. But I think we've seen like five, hard numbers, please reach out. But I think we've seen like five, close to 5000 containers being monitored by like, at this point, maybe 64 gig for CPU machine node to power Prometheus. And it's not fully used. But sometimes when it you know, when you scale out, and you get just all those some tons of spike spike in objects. That's really when it uses that. But Prometheus is really the biggest one and you'll find this, it's not an option, it's a Prometheus piece. But other than that, all those components, I mean, you're talking extremely, extremely small resource requirements, really inexorable on your clusters. Absolutely, absolutely. Well, with that, unfortunately, we are at time. I really do appreciate everybody reaching out and asking those questions. Like all good things, the streams have to come to an end. So we are at that point. But thank you so much. If there is anyone looking to kind of reach out to either of you, is there a good place to open up those questions? Sure. You can reach us to info at obscurus.com to be generic enough. And you can also ping us on the website, obscurus.com itself. You know, it should be easy to find us. We're also on LinkedIn if you want to look us up on the Obscurus page. Love to chat with you guys, get your feedback. Excellent. Excellent. Hopefully this was interesting and exciting, given where things are going with OpenTelemetry, open source and observability. Well, thank you both so much. Thank you, everyone, for joining the latest episode of Cloud Native Live. It was great to hear from Eloke and Cesar. We really, again, love the interaction and questions from all of the audience. Join us next week to hear about how we're going to be building stability in Kubernetes with Andy Suderman of Fairwinds. Thank you all for joining us today, and we will see you soon. Have a good one. Thanks, everyone. Thank you."}